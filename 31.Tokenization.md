### Tokenization（分词技术）的发展历程与技术方案全景

#### 一、发展历程与核心技术演进
Tokenization 是将连续文本或多模态数据拆解为模型可理解的最小单元（Token）的过程，其技术演进可分为以下阶段：

1. **词级分词（Word-Level）**
   - **技术特点**：以完整单词或中文词汇为单位，如 Word2Vec、GloVe 模型。
   - **局限**：存在未登录词（OOV）问题，无法处理新词、多语言混合或形态变化（如“dogs”与“dog”被视为无关词）。

2. **子词级分词（Subword-Level）**
   - **BPE（字节对编码）**：通过迭代合并高频字符对构建词表，适用于 GPT 系列、RoBERTa 等模型。
   - **WordPiece**：基于语言模型似然性合并子词，用于 BERT、DistilBERT 等模型，减少 OOV 并提升语义准确性。
   - **SentencePiece**：语言无关的无监督分词框架，支持 BPE 和 Unigram 算法，适用于多语言（如 T5、XLNet）及低资源语言场景。
   - **Tiktoken**：OpenAI 针对 GPT-3.5/4 优化的 BPE 变体，兼顾速度与分词质量。

3. **字符/字节级分词**
   - **Byte-level BPE**：直接以 UTF-8 字节为单位（如 GPT-2），支持 emoji、多语言字符的无差别处理。
   - **CANINE/ByT5**：纯字符或字节输入，增强对拼写错误、噪声的鲁棒性，但序列长度会显著增加。

4. **多模态 Tokenization**
   - **图像**：Patch Embedding（ViT 切分图像块）或离散编码（VQ-VAE 将图像映射为视觉 Token，如 DALL·E）。
   - **音频/视频**：频谱分帧（Whisper）或时序 Token 序列（AudioLM、VideoGPT）。


#### 二、场景与业务适配的技术方案
| **场景/业务**       | **推荐技术**                  | **典型应用**                          |
|---------------------|-----------------------------|-------------------------------------|
| **大语言模型（LLM）** | Tiktoken（GPT 系列）、BPE（LLaMA） | 对话生成、长文本理解                  |
| **多语言 NLP**       | SentencePiece（Unigram）| 跨语言翻译、多语言问答                |
| **金融文本处理**     | WordPiece（BERT）| 金融舆情分析、财报实体识别            |
| **多模态生成（图文）** | SentencePiece + 视觉 VQ Token | DALL·E 图文生成、Flamingo 多模态问答  |
| **低资源语言**       | SentencePiece（BPE）| 非洲语言、小众方言的 NLP 任务         |


#### 三、算法与模型的技术映射
| **算法**       | **核心原理**                          | **代表模型**               |
|----------------|---------------------------------------|----------------------------|
| BPE            | 频率驱动的贪心字符对合并              | GPT-4、RoBERTa、LLaMA      |
| WordPiece      | 最大化语言模型似然的子词合并          | BERT、DistilBERT           |
| SentencePiece  | 无监督学习子词，支持多算法（BPE/Unigram） | T5、XLNet、mBART           |
| Tiktoken       | 优化后的 BPE，针对 GPT 模型效率调优   | GPT-3.5、GPT-4             |
| Unigram        | 从大词表剪枝，保留最优子词分布        | ALBERT、XLM-R              |


#### 四、技术选择的关键考量
- **效率优先**：选择 Tiktoken（GPT 生态）或 BPE（如 LLaMA）。
- **多语言支持**：优先 SentencePiece（Unigram 模式）。
- **语义精度**：WordPiece（BERT 系列）更适合需要强语义对齐的任务。
- **多模态融合**：需结合视觉 Token 技术（如 VQ-VAE）与文本 Tokenizer 协同设计。

通过以上技术演进与场景适配，Tokenization 已从单一的文本切分工具，发展为支撑大模型、多模态 AI 的核心基础设施。

#
以下是补充了BBPE后的完整表格：

| 缩写               | 全称                                  | 中文翻译                  |
|--------------------|---------------------------------------|---------------------------|
| OOV                | Out-Of-Vocabulary                     | 未登录词（不在词表中的词）|
| BPE                | Byte Pair Encoding                    | 字节对编码                |
| **BBPE**           | **Byte-level Byte Pair Encoding**     | **字节级字节对编码**      |
| GPT                | Generative Pre-trained Transformer    | 生成式预训练Transformer  |
| RoBERTa            | Robustly Optimized BERT Pretraining Approach | 鲁棒优化的BERT预训练方法 |
| BERT               | Bidirectional Encoder Representations from Transformers | 基于Transformer的双向编码器表示 |
| DistilBERT         | Distilled BERT                        | 蒸馏版BERT（轻量版BERT）  |
| SentencePiece      | 无全称（独立工具名）                  | 句子片段（分词工具）      |
| T5                 | Text-to-Text Transfer Transformer     | 文本到文本迁移Transformer |
| XLNet              | eXtreme Language Net                  | 极致语言网络              |
| Tiktoken           | 无全称（OpenAI专属分词工具）          | （无固定译法，可直称Tiktoken） |
| ViT                | Vision Transformer                    | 视觉Transformer           |
| VQ-VAE             | Vector Quantized Variational Autoencoder | 向量量化变分自编码器      |
| DALL·E             | 无全称（结合艺术家Dali和机器人WALL·E命名） | （无固定译法，可直称DALL·E） |
| Whisper            | 无全称（OpenAI语音识别模型）          | （无固定译法，可直称Whisper） |
| AudioLM            | Audio Language Model                  | 音频语言模型              |
| VideoGPT           | Video Generative Pre-trained Transformer | 视频生成式预训练Transformer |
| CANINE             | Character-Aware Neural Information Extractor | 字符感知神经信息提取器    |
| ByT5               | Byte-level Text-to-Text Transfer Transformer | 字节级文本到文本迁移Transformer |
| LLM                | Large Language Model                  | 大语言模型                |
| LLaMA              | Large Language Model Meta AI          | Meta AI大语言模型（元宇宙AI大语言模型） |
| mBART              | Multilingual BART                     | 多语言BART（BART为Bidirectional and Auto-Regressive Transformers的缩写，即双向自回归Transformer） |
| ALBERT             | A Lite BERT                           | 轻量级BERT                |
| XLM-R              | XLM-RoBERTa                           | 多语言RoBERTa（XLM为Cross-Lingual Language Model的缩写，即跨语言语言模型） |


**说明**：BBPE（字节级字节对编码）是BPE的变体，其核心区别在于：  
- 传统BPE通常以字符（Character）为初始单元，而BBPE直接以字节（Byte，如UTF-8编码的字节）为初始单元，更适合处理多语言、特殊符号（如emoji）和低资源语言场景，能彻底避免字符集依赖问题。GPT-2及后续部分模型采用了BBPE思路。