---
created: 2025-10-28T11:49:07 (UTC +08:00)
tags: []
source: https://zhuanlan.zhihu.com/p/1928126905481138307
author: 关于作者TableauAI从业者，坚持探索并分享AI技术应用，让我们共启AI之旅。南京大学 地图学与地理信息系统硕士回答125文章114关注者3,881关注发私信
---

# (70 封私信 / 83 条消息) RAG中Reranker（重排序）——BM25、Cross Encoder Reranker、FlashRank、RankLLM - 知乎

> ## Excerpt
> 检索增强生成（Retrieval Augmented Generation，简称RAG）与AI智能体（AI Agent）已迅速成为现代AI开发的核心，使模型能够利用外部数据源以获得更为丰富和准确的响应。尽管基础的RAG流程至关重要，但要构建真正健…

---
检索增强生成（Retrieval Augmented Generation，简称RAG）与[AI智能体](https://zhida.zhihu.com/search?content_id=260327269&content_type=Article&match_order=1&q=AI%E6%99%BA%E8%83%BD%E4%BD%93&zhida_source=entity)（AI Agent）已迅速成为现代AI开发的核心，使模型能够利用外部数据源以获得更为丰富和准确的响应。尽管基础的RAG流程至关重要，但要构建真正健壮的RAG系统，则需要深入探讨高级检索与重排序（Reranking）技术。本文将分析为何Reranker如此重要，并介绍多种提升RAG性能的重排序策略。

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20RAG%E4%B8%ADReranker%EF%BC%88%E9%87%8D%E6%8E%92%E5%BA%8F%EF%BC%89%E2%80%94%E2%80%94BM25%E3%80%81Cross%20Encoder%20Reranker%E3%80%81FlashRank%E3%80%81RankLLM%20-%20%E7%9F%A5%E4%B9%8E/v2-e25339c80fd3e9864480e3d6fab868c5_1440w.jpg)

**RAG流程简述**

典型的RAG系统工作流程如下：

**数据摄取（Data Ingestion）**：外部数据（如文档、PDF、网页内容）首先被处理为更小的片段（chunk）。这些片段随后被转换为数值型“嵌入”（embedding，向量表示），并存储在[向量数据库](https://zhida.zhihu.com/search?content_id=260327269&content_type=Article&match_order=1&q=%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93&zhida_source=entity)中。

**检索（Retrieval）**：当用户提出查询时，查询同样会被转换为嵌入。该查询嵌入用于在向量数据库中执行相似度检索，以找到最“相关”（或初步“排序”）的文档或片段。这一过程旨在为[大语言模型](https://zhida.zhihu.com/search?content_id=260327269&content_type=Article&match_order=1&q=%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B&zhida_source=entity)（LLM, Large Language Model）提供上下文。

**生成（Generation）**：检索到的文档（上下文）与原始查询及提示词（prompt）一同输入LLM，模型据此生成有意义且有据可依的答案。

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20RAG%E4%B8%ADReranker%EF%BC%88%E9%87%8D%E6%8E%92%E5%BA%8F%EF%BC%89%E2%80%94%E2%80%94BM25%E3%80%81Cross%20Encoder%20Reranker%E3%80%81FlashRank%E3%80%81RankLLM%20-%20%E7%9F%A5%E4%B9%8E/v2-8a2358b43631d1f71d433e34437c054b_1440w.jpg)

RAG的核心组成部分

## **基础检索的挑战**

基础检索通常依赖于关键词或向量相似度搜索，虽然是良好的起点，但在实际应用中往往难以满足需求。向量搜索并不理解查询的_意图_，它只关注_统计相似性_。因此，检索结果常常：

-   过于宽泛
-   稍有偏离主题
-   存在冗余
-   缺乏关键细节

## **为什么Reranking（重排序）至关重要**

Reranker通过以下方式弥补初步检索的不足：

**提供更相关的信息**：过滤掉不太相关的数据，确保LLM获得最聚焦、最有用的上下文。

**减少数据噪声**：通过重新评估和精炼初步检索到的文档，显著减少传递给LLM的无关信息。

**提升数据效率**：更精炼的上下文提升了LLM的表现，模型不再被无关细节拖累。

**优化输入**：Reranker将初步“排序结果”精炼为“重排序结果”，为LLM生成阶段提供更优质的输入。

Reranker的核心工作很简单：对初步检索结果再次排序——但这一次更智能。它试图回答：“这些文档中，哪些真正有助于解答用户问题？”结果是：传递给LLM的片段更少但更优，输出质量显著提升。

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20RAG%E4%B8%ADReranker%EF%BC%88%E9%87%8D%E6%8E%92%E5%BA%8F%EF%BC%89%E2%80%94%E2%80%94BM25%E3%80%81Cross%20Encoder%20Reranker%E3%80%81FlashRank%E3%80%81RankLLM%20-%20%E7%9F%A5%E4%B9%8E/v2-7079ca528df26417cd711b645a960425_1440w.jpg)

Reranking

目前已有多种技术和模型可用于实现关键的重排序步骤：

## **BM25（Best Match 25）**

BM25是[TF-IDF](https://zhida.zhihu.com/search?content_id=260327269&content_type=Article&match_order=1&q=TF-IDF&zhida_source=entity)（词频-逆文档频率）的升级版，在关键词或token层面计算相似度。

**词频（Term Frequency）**——某词在单个文档中出现的次数。BM25认为多次出现是“更强的提示”，但在多次重复后会递减加分。

**逆文档频率（Inverse Document Frequency）**——该词在所有文档中出现的稀有程度。常见词（如“the”）得分极低，罕见词得分高。

BM25非常适合小型、术语密集型语料库或对成本敏感的场景。

```python
from langchain.retrievers import BM25Retriever retriever = BM25Retriever.from_documents(docs) result = retriever.invoke(query)
```

## **Cross-Encoder Reranker（交叉编码器重排序器）**

Cross-encoder reranker（交叉编码器重排序器）是一类强大的开源模型，通常基于BERT等[Transformer架构](https://zhida.zhihu.com/search?content_id=260327269&content_type=Article&match_order=1&q=Transformer%E6%9E%B6%E6%9E%84&zhida_source=entity)。其工作方式是将查询与文档片段一同输入同一个模型，生成一个相似度分数（通常为0到1），表示查询与文档片段的相关性。

在典型检索流程中的作用如下：

1.  **初步检索（Bi-encoder）**：首先，bi-encoder（双编码器，使用稠密嵌入）快速选出与查询最相关的前K个文档片段。
2.  **交叉编码器**：随后，cross-encoder对这K个查询-文档片段对进行逐一评估。通过在统一上下文中考虑查询与文档片段，交叉编码器能够以更高精度重新打分和排序。

交叉编码器的优势在于相关性判断更为准确，但代价是需要更多GPU算力，并比单用bi-encoder多消耗数十毫秒。

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_community.vectorstores import FAISS

# 假设texts为文档列表，embeddingsModel为嵌入模型
retriever = FAISS.from_documents(
    texts,
    embeddingsModel
).as_retriever(search_kwargs={"k": 20})

model = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-base")
compressor = CrossEncoderReranker(model=model, top_n=3)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=retriever
)

compressed_docs = compression_retriever.invoke(query)
```

## **[Cohere Rerank API](https://zhida.zhihu.com/search?content_id=260327269&content_type=Article&match_order=1&q=Cohere+Rerank+API&zhida_source=entity)**

Cohere等公司提供专门的API重排序服务，模型针对该任务专门训练。通过Cohere的rerank模型，可以对向量数据库或其他检索器的结果重新排序，确保最相关的文档优先传递给LLM。

```python
import os
from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain_cohere import CohereRerank, CohereEmbeddings
from langchain_community.vectorstores import FAISS

os.environ["COHERE_API_KEY"] = "your-cohere-api-key"

retriever = FAISS.from_documents(
    texts,
    CohereEmbeddings(model="embed-english-v3.0")
).as_retriever(search_kwargs={"k": 20})

compressor = CohereRerank(model="rerank-english-v3.0")

compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=retriever
)

compressed_docs = compression_retriever.invoke(query)
```

## **[FlashRank Reranker](https://zhida.zhihu.com/search?content_id=260327269&content_type=Article&match_order=1&q=FlashRank+Reranker&zhida_source=entity)**

FlashRank是专为高效文档重排序设计的Python库。其核心优势在于提供最前沿的重排序能力，同时依赖极少（最小模型无需PyTorch/Transformers），推理速度极快，非常适合CPU环境及对成本敏感、低延迟的应用。支持以下排序模型：

-   点式（Pointwise）重排序器：每次考虑一个查询和一个文档，判断其相关性。
-   对式（Pairwise）重排序器：每次考虑查询和两个文档，判断哪一个更相关。
-   列表式（Listwise）LLM重排序器：一次性考虑整个文档列表（或大部分），在查询上下文下寻找最优排序。

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain_community.document_compressors import FlashrankRerank
from langchain_openai import ChatOpenAI
from flashrank import Ranker

llm = ChatOpenAI(temperature=0)

compressor = FlashrankRerank(model="ms-marco-MiniLM-L-12-v2")

compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=retriever,
)

compressed_docs = compression_retriever.invoke(query)
```

## **[RankLLM Reranker](https://zhida.zhihu.com/search?content_id=260327269&content_type=Article&match_order=1&q=RankLLM+Reranker&zhida_source=entity)**

RankLLM是一个开源Python包，利用大语言模型（LLM, Large Language Model）进行文档重排序。RankLLM针对检索与排序任务进行了优化，支持开源LLM及RankGPT、RankGemini等专有重排序器，涵盖点式、对式、列表式等主流重排序范式。

RankLLM的独特之处在于“prompt-decoder”重排序，即利用LLM的生成能力，通过精心设计的提示词（prompt）生成排序列表或相关性分数。

```python
import torch
from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain_community.document_compressors.rankllm_rerank import RankLLMRerank

torch.cuda.empty_cache()

compressor = RankLLMRerank(top_n=3, model="rank_zephyr")
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=retriever,
)

del compressor
compressed_docs = compression_retriever.invoke(query)
```

## **结论**

首先，RAG流程中的初步检索虽然能高效拓宽信息范围，但常常陷入“差不多就行”的困境，仅检索到相关性一般的文档，而非真正关键的内容。正因如此，Reranker（重排序器）变得不可或缺。通过引入复杂、具备上下文感知能力的“第二遍”筛选，基于用户查询与文档之间的深层语义关系重新评估和优先排序，Reranker能够将一个普通的RAG系统提升为卓越系统。
