### 提取的文字
第一部分：Transformer结构
与LLM相关的面试都会问到transformer，比如手撕多头注意力，自注意力缩放，参数计算等等
1.分词器tokenizer & Embedding层
BPE，BBPE，WordPiece等算法，了解一下各类模型的分词方法，感兴趣的同学可以看一下tokenizer在预训练过程具体如何处理
2.注意力模块
self-attention，cross-attention的原理，MHA、MQA、GQA、MLA、DCA等多种注意力机制优化策略（可能会考手撕），线性注意力，稀疏注意力，kvcache等等，这部分推荐看苏神的科学空间，原理推导写的很清楚
3.前馈神经网络FFN&残差连接&归一化
这几个模块的作用是什么，LN和BN的区别，pre-norm和post-norm，SwiGLU等激活函数，RMSNorm等归一化


### 问题解答（基于提取文字中的知识点）
#### 问题1：分词器相关算法（BPE、BBPE、WordPiece）及tokenizer在预训练的处理
- **BPE（字节对编码）**：从字符开始，迭代地合并最频繁出现的字节对，生成新的token，以此构建词汇表。例如在处理英文时，可将“low”“er”合并为“lower”。
- **BBPE（字节级BPE）**：以字节为基础单位进行BPE操作，能有效处理OOV（未登录词）问题，常见于GPT系列模型的分词。
- **WordPiece**：与BPE类似但合并时基于语言模型概率，如BERT使用的分词方法，将“playing”拆分为“play”和“##ing”。
- **tokenizer在预训练的处理**：首先对语料进行初步分词（如按空格），然后通过上述算法学习合并规则，构建词汇表；预训练时，将输入文本用该词汇表对应的token id表示，为模型输入做准备。


#### 问题2：self-attention、cross-attention原理及各类注意力优化策略
- **self-attention原理**：通过计算query、key、value之间的相似度，得到每个token对其他token的注意力权重，再加权求和value得到输出。公式为：$\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$，其中$\frac{1}{\sqrt{d_k}}$是缩放因子，避免点积过大导致softmax饱和。
- **cross-attention原理**：query来自一个模态（如文本），key和value来自另一个模态（如图像），用于跨模态信息交互，例如Vision-Transformer中图像特征与文本query的交互。
- **注意力优化策略**：
  - **MHA（多头注意力）**：将Q、K、V分成多个头，并行计算注意力后拼接，增强模型对不同语义信息的捕捉能力。
  - **MQA（多查询注意力）**：多个query共享同一组key和value，减少计算量和显存占用，适合推理加速。
  - **GQA（分组查询注意力）**：介于MHA和MQA之间，将query分组，每组共享key和value，平衡性能与效率。
  - **MLA（多尺度注意力）**：在不同尺度（如不同长度的窗口）上计算注意力，捕捉多粒度信息。
  - **DCA（动态上下文注意力）**：根据输入动态调整注意力的上下文范围，提升效率。
  - **线性注意力**：将注意力计算从二次复杂度降低到线性，通过核函数转换（如$\text{softmax}(QK^T/\sqrt{d_k})V \approx \frac{Q(\text{softmax}(K^T)V)}{\sqrt{d_k}}$）实现，适合长序列。
  - **稀疏注意力**：仅计算部分token之间的注意力（如局部窗口、随机稀疏等），降低计算量。
  - **kvcache**：在推理时缓存key和value，避免重复计算，加速自回归生成过程。


#### 问题3：前馈神经网络FFN、残差连接、归一化的作用，LN与BN区别，pre-norm与post-norm，SwiGLU、RMSNorm
- **FFN作用**：对注意力输出进行非线性变换，增强模型的表达能力，通常由两个线性层和激活函数组成（如$\text{FFN}(x)=\text{ReLU}(xW_1 + b_1)W_2 + b_2$，SwiGLU则是$\text{SwiGLU}(x) = \text{Swish}(xW_1 + b_1) \otimes (xW_2 + b_2)$，其中$\otimes$是逐元素相乘）。
- **残差连接作用**：解决深层网络梯度消失和训练困难问题，公式为$x + \text{TransformerBlock}(x)$，使模型能直接学习残差映射。
- **归一化作用**：加速训练、提升稳定性，通过将特征归一化到特定分布（如均值为0、方差为1）实现。
- **LN（层归一化）与BN（批归一化）区别**：
  | 维度       | 归一化对象       | 场景适配       |
  |------------|------------------|----------------|
  | LN         | 单个样本的所有特征 | NLP（序列长度可变） |
  | BN         | 一批样本的同一特征 | 计算机视觉（批次稳定） |
- **pre-norm与post-norm**：
  - **pre-norm**：在残差连接前对输入进行归一化，使训练更稳定，但可能导致模型表达能力下降，常见于GPT系列。
  - **post-norm**：在残差连接后对输出进行归一化，表达能力强但训练难度高，早期BERT采用这种方式。
- **SwiGLU激活函数**：是Swish函数与GLU的结合，公式为$\text{SwiGLU}(x) = \text{Swish}(\alpha x + b_1) \times (\beta x + b_2)$，相比ReLU有更强的非线性表达能力，且能减少参数，被LLaMA等模型采用。
- **RMSNorm**：是LN的变体，计算时只除以元素平方的均值的平方根，公式为$\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{d}\sum_{i=1}^d x_i^2 + \epsilon}}$，计算更高效，稳定性好，被GPT-4等模型采用。


#### 问题4：Transformer参数计算（以典型结构为例）
以一个包含$L$层、隐层维度$d$、多头注意力头数$h$、FFN中间层维度$d_{ff}$的Transformer为例：
- **Embedding层参数**：词汇表大小$V \times d$。
- **注意力模块参数**：每个头的$Q/K/V$矩阵维度为$d \times (d/h)$，共$3h \times (d \times d/h) = 3d^2$；输出投影矩阵$d \times d$，所以单头注意力总参数$4d^2$，多头则为$4d^2$（因为$h$个头的参数合并后还是$4d^2$）。
- **FFN参数**：$d \times d_{ff} + d_{ff} \times d = 2dd_{ff}$（若用SwiGLU，中间层维度为$d_{ff}/2$，则参数为$d \times (d_{ff}/2) + (d_{ff}/2) \times d = dd_{ff}$）。
- **残差连接与归一化**：无额外参数（LN是对每个元素进行归一化，仅需$\gamma$和$\beta$两个可学习参数，每层LN参数为$2d$，$L$层则为$2Ld$）。
- **总参数**：$Vd + L(4d^2 + 2dd_{ff} + 2d)$（若考虑LN）。例如GPT-3 175B模型，就是通过超大的$V$、$L$、$d$、$d_{ff}$实现的参数规模。
