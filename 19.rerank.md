
## Embedding vs Rerank
| 维度       | Embedding 相似度查询                                                                 | Rerank 重排                                                                 |
|------------|--------------------------------------------------------------------------------------|-----------------------------------------------------------------------------|
| 模型结构   | 双塔/bi-encoder：查询塔和文档塔各自独立编码 → 得到两个向量                             | 交叉编码器 / cross-encoder：将“查询 + 文档”拼接后一次性输入同一模型        |
| 相似度度量 | 余弦、点积或欧氏距离等 向量空间度量                                                   | 模型输出一个相关性分数（0-1 或实数），本身即是排序依据                     |
| 速度 & 规模| 一次编码、一次 ANN 搜索 → O(log N) 近似检索，可在百万-亿级文档上实时运行                | 两次前向：Q×k，只能在少量候选(k<50-200)上跑，延迟高几个数量级              |
| 表示能力   | 向量化后失去部分细粒度语义（尤其对长文档、数字、否定词等）                             | 交叉注意力逐 token 交互 → 细粒度捕捉语义、句法、上下文                      |
| 典型用途   | 召回 / 初排                                                                          | 精排 / rerank / reroute                                                     |
| 可解释性   | 低（仅相似度）                                                                       | 中等，可查看注意力或对齐得分                                               |
| 部署形态   | GPU / CPU 皆可，常用 FAISS、ScaNN、Milvus、Chroma                                     | GPU 更佳，需要 BERT/RoBERTa-size 模型；量化或蒸馏后可上 CPU                |


