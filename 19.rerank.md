
## Embedding vs Rerank
| 维度       | Embedding 相似度查询                                                                 | Rerank 重排                                                                 |
|------------|--------------------------------------------------------------------------------------|-----------------------------------------------------------------------------|
| 模型结构   | 双塔/bi-encoder：查询塔和文档塔各自独立编码 → 得到两个向量                             | 交叉编码器 / cross-encoder：将“查询 + 文档”拼接后一次性输入同一模型        |
| 相似度度量 | 余弦、点积或欧氏距离等 向量空间度量                                                   | 模型输出一个相关性分数（0-1 或实数），本身即是排序依据                     |
| 速度 & 规模| 一次编码、一次 ANN 搜索 → O(log N) 近似检索，可在百万-亿级文档上实时运行                | 两次前向：Q×k，只能在少量候选(k<50-200)上跑，延迟高几个数量级              |
| 表示能力   | 向量化后失去部分细粒度语义（尤其对长文档、数字、否定词等）                             | 交叉注意力逐 token 交互 → 细粒度捕捉语义、句法、上下文                      |
| 典型用途   | 召回 / 初排                                                                          | 精排 / rerank / reroute                                                     |
| 可解释性   | 低（仅相似度）                                                                       | 中等，可查看注意力或对齐得分                                               |
| 部署形态   | GPU / CPU 皆可，常用 FAISS、ScaNN、Milvus、Chroma                                     | GPU 更佳，需要 BERT/RoBERTa-size 模型；量化或蒸馏后可上 CPU                |


---
created: 2025-10-28T11:36:41 (UTC +08:00)
tags: [cross encoder]
source: https://blog.csdn.net/keeppractice/article/details/148949577
author: keeppractice
---

# Bi-Encoder 与 Cross-Encoder 全解析：原理、对比与实战模型推荐_cross encoder-CSDN博客

> ## Excerpt
> 文章浏览阅读2k次，点赞18次，收藏18次。Bi-Encoder 是一种将 Query 和 Document 分别编码为向量的架构，通常用于大规模语义检索任务。Cross-Encoder 将 Query 和 Document 拼接后，一起输入到一个 Transformer 模型中，进行整体编码与匹配打分。应用场景推荐使用方式大规模语义检索Bi-Encoder + 向量数据库小规模高质量排序高性能企业RAG系统。_cross encoder

---
近年来，随着语义搜索、问答系统和大语言模型（LLM）相关技术的快速发展，**向量检索 + 精排（Ranking）机制**成为构建高质量自然语言处理系统的关键。而在这一过程中，\*\*Bi-Encoder（双塔模型）**和**Cross-Encoder（交叉编码器）\*\*是两种核心架构，常用于向量生成与相似度匹配。

本文将从原理、对比、适用场景出发，并结合实际经验推荐一些常用开源模型。

### 一、什么是 Bi-Encoder？

#### ✅ 定义：

Bi-Encoder 是一种**将 Query 和 Document 分别编码为向量**的架构，通常用于大规模语义检索任务。

#### ✅ 工作流程：

-   Query 和 Document 各自通过一个 Transformer（如 BERT）模型，独立编码。
    
-   得到两个向量后，通过**余弦相似度或向量距离**计算它们的语义相关性。
    

```scss
Embedding(Query) ≈ Embedding(Document)

→ 相似度高则匹配
```

#### ✅ 特点：

-   **可离线计算文档向量**，存入向量数据库，检索速度极快。
    
-   可扩展到**千万、亿级文档**。
    
-   缺点是 Query 和 Document 无法“交叉交互”，对复杂语义捕捉能力稍弱。
    

### 二、什么是 Cross-Encoder？

#### ✅ 定义：

Cross-Encoder 将 Query 和 Document 拼接后，**一起输入到一个 Transformer 模型中**，进行整体编码与匹配打分。

#### ✅ 工作流程：

```css
Input: [CLS] Query [SEP] Document [SEP]

→ Transformer → 分类头或打分回归头 → 得分
```

#### ✅ 特点：

-   语义交互更充分，**精度通常远高于 Bi-Encoder**。
    
-   每对 Query-Doc 都需要重新计算，**推理速度慢，不适合大规模检索**。
    
-   适合用作**排序器（Re-ranker）**，用于精排 Top-K 检索结果。
    

### 三、Bi-Encoder vs Cross-Encoder 对比表

| 特性 | Bi-Encoder（双塔） | Cross-Encoder（交叉编码） |
| --- | --- | --- |
| 编码方式 | Query 和 Document 独立编码 | Query 和 Document 联合编码 |
| 检索速度 | 🚀 快（适合大规模检索） | 🐢 慢（适合精排Top-K） |
| 精度 | 中等 | 高 |
| 可否预先编码文档 | ✅ 可以 | ❌ 不可以 |
| 用途 | 向量检索（RAG、语义搜索） | 精排（排序，问答匹配） |

### 四、实战常用模型推荐

#### ✅ Bi-Encoder 推荐模型

| 模型名称 | 说明 | 中文支持 |
| --- | --- | --- |
| `sentence-transformers/all-MiniLM-L6-v2` | 精度+速度平衡，常用于英文语义检索 | ❌ |
| `BAAI/bge-base-en` / `bge-large-en` | 中文团队 BAAI 发布，效果强，适合多语言检索 | ✅ |
| `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` | 多语言支持，适合国际化应用 | ✅ |
| `intfloat/multilingual-e5-base` | E5 系列模型，支持多语言，表现优秀 | ✅ |
| `shibing624/text2vec-base-chinese` | 专为中文语义搜索优化 | ✅ |

#### ✅ Cross-Encoder 推荐模型

| 模型名称 | 说明 | 中文支持 |
| --- | --- | --- |
| `cross-encoder/ms-marco-MiniLM-L-6-v2` | 微调于 MS MARCO 的排序任务，效果好，速度快 | ❌ |
| `BAAI/bge-reranker-base` / `large` | 中文团队 BGE 的 Cross 模型，RAG 精排效果佳 | ✅ |
| `cross-encoder/qnli-electra-base` | Electra 架构，适合英文匹配任务 | ❌ |
| 自行微调的 BERT 分类器 | 自定义语料可使用 BERT + \[CLS\] → 分类头进行训练 | ✅ |

### 五、实战搭配策略（Hybrid 检索架构）

在实际中，我们常将两者**联合使用**以兼顾性能与精度：

#### 💡 二阶段检索流程：

1.  **阶段一（召回）：**
    
    -   使用 Bi-Encoder 编码文档，存入向量数据库（如 Milvus、FAISS）。
        
    -   用户 Query 向量化后进行 Top-K 检索。
        
2.  **阶段二（精排）：**
    
    -   使用 Cross-Encoder 对召回的 Top-K 文档逐一打分。
        
    -   根据得分进行排序，返回最相关结果。
        

这种方式广泛用于：

-   企业级 RAG 系统
    
-   智能问答机器人
    
-   法律、医疗、金融等高精准检索系统
    

### 🏁 总结

| 应用场景 | 推荐使用方式 |
| --- | --- |
| 大规模语义检索 | Bi-Encoder + 向量数据库 |
| 小规模高质量排序 | Cross-Encoder |
| 高性能企业RAG系统 | Bi-Encoder + Cross-Encoder |

### 

[https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2")

[https://huggingface.co/cross-encoder/ms-marco-MiniLM-L12-v2](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L12-v2 "https://huggingface.co/cross-encoder/ms-marco-MiniLM-L12-v2")
