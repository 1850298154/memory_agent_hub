---
created: 2025-10-30T06:43:14 (UTC +08:00)
tags: [OCR（光学字符识别）]
source: https://zhuanlan.zhihu.com/p/547313179
author: 关于作者达观数据智能知识管理专家，Agent，RPA，文档处理，曹植大模型回答156文章1,536关注者2,082已关注发私信
---

# (70 封私信 / 83 条消息) 技术干货：如何训练高性能语义表示模型——交叉编码器VS双编码器 - 知乎

> ## Excerpt
> 目前，在语句编码器（sentence encoders）领域的学术研究相当多，刚开始涉足的话，可能有点令人生畏。如果你没有正确掌握语句编码器背后的理念和其中的细微差别，就很容易感到困惑。 例如，你可能知道存在sentence…

---
目前，在语句编码器（sentence encoders）领域的学术研究相当多，刚开始涉足的话，可能有点令人生畏。如果你没有正确掌握语句编码器背后的理念和其中的细微差别，就很容易感到困惑。

例如，你可能知道存在sentence transformer（bi-encoders），它们的目的是为句子级别的任务产生高质量的嵌入（qualitative embeddings）。但话说回来，普通的 transformer编码器也能产生较高质量的嵌入，并且完全适合句子级任务。transformer和与之对应的sentence transformer背后的架构似乎也是相同的。然后，还有[交叉编码器](https://zhida.zhihu.com/search?content_id=209758491&content_type=Article&match_order=1&q=%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8&zhida_source=entity)（cross-encoders）——它也被用于执行句子级的文本分类/回归任务。

这些疑问正是我们在笔者要在本文中所要阐明清楚的问题。

读完本文，你应该会对什么是 sentence transformer、[双编码器](https://zhida.zhihu.com/search?content_id=209758491&content_type=Article&match_order=1&q=%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8&zhida_source=entity)（bi-encoders）和交叉编码器（cross-encoders）、何时使用它们以及它们与普通的transformer（regular transformers）的关系有一个非常清楚的了解。

**01**

**为什么需要sentence transformers?**

一言以蔽之：

> “sentence transformer能产生比普通transformers更好的语句语义表示，以支持某些特定的句子级的下游任务，如语义检索、文本聚类、论据挖掘等。”

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20%E6%8A%80%E6%9C%AF%E5%B9%B2%E8%B4%A7%EF%BC%9A%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E9%AB%98%E6%80%A7%E8%83%BD%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8VS%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8%20-%20%E7%9F%A5%E4%B9%8E/v2-c55ffb14872062838c57aa6768fb6d61_1440w.jpg)

详情见 知乎.主题模型(topic model)到底还有没有用，该怎么用？,苏格兰折耳喵，[https://www.zhihu.com/question/34801598](https://www.zhihu.com/question/34801598)

语句的表示学习模型从较早期的词袋模型到静态预训练模型，再到如今的动态的语境预训练模型，对于句子语义信息的编码由“低保真”逐渐过渡到“高保真”状态。

在实际应用场景中，低保真还是高保真并没有优劣之分，选型完全取决于使用场景和具体需求。某个语义表示模型很可能非常适用于一个特定的使用场景，但在另一个使用场景下就没啥作用了。我将通过一个示例进行说明。

请考虑以下说辞："从此边城多战伐，不须笳鼓更悲凉"和 "投身革命即为家，血雨腥风应有涯"。那么，问题来了，这两条诗句是否应该被认为是相似的，也就是说，上述2个语句嵌入是否应该在语义空间中相互靠拢呢？

嗯，这取决于你的选择。如果你谈论的是主题，那么肯定是的---这两个语句都是关于“战争残酷”的内容，在这个意义上，它们是非常相似的。然而，如果你谈论的是情感，那么答案是不言而喻：前一句明显是悲伤的消极的情感，后一句是革命乐观主义的积极情感，二者的情感倾向截然不同。

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20%E6%8A%80%E6%9C%AF%E5%B9%B2%E8%B4%A7%EF%BC%9A%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E9%AB%98%E6%80%A7%E8%83%BD%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8VS%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8%20-%20%E7%9F%A5%E4%B9%8E/v2-e771ec898d81f177f2fb8006ba941e56_1440w.jpg)

所有嵌入都是相同的，但有些嵌入比其他嵌入更相同

> _**注意**：这就是为什么大型的预训练语言模型需要在非常通用的、数据规模庞大的任务上训练（如遮蔽词语言建模）的原因，其内在逻辑是，经大规模通用语料训练得到的语句嵌入表示反映了模型对语言非常广泛的理解，以后可以根据具体的使用情况进行调整。_  

言归正传：" sentence transformers是否能产生更好的语句嵌入？"。答案也是不言而喻的，---需要视场景而定，做"具体问题，具体分析"!

常规的transformer模型通过执行一系列操作来产生句子嵌入，如对其token-level的嵌入的元素算术平均。BERT的一个很好的池化选择是CLS池化。BERT有一个特殊的<CLS>标记，它应该捕获所有的序列信息。在预训练期间，它被调整为对下一句的预测（NSP）。

> **_注意_** _：RoBERTa在预训练试没有执行NSP任务，也没有<CLS>标记。它仅有一个句首<s>的标记，但嵌入没有经过训练，故无法描绘出有意义的句子表征，没法开箱即用~_  

由此产生的句子嵌入很适合于一些分类或回归任务，如果这是你接下来要做的任务，请直接使用transformers中的BertForSequenceClassification或其他模型类似的方法，它们的训练过程和使用方法都还不错。

然而，对于语义相似性任务来说，由常规的transformers获得的句子嵌入并不是很好。这就是sentence transformer大显身手的地方。********sentence transformers********的训练过程是专门针对******语义相似性任务******而设计的，笔者稍后会有更多关于训练过程的内容。

因此，综上所述：如果你想执行一些句子分类任务，普通的**********transformers**********就能胜任了。但是，如果你想要执行诸如语义检索、话题聚类等依赖于语义相似性的任务，你就应该将目光投向******sentence transformers******。

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20%E6%8A%80%E6%9C%AF%E5%B9%B2%E8%B4%A7%EF%BC%9A%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E9%AB%98%E6%80%A7%E8%83%BD%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8VS%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8%20-%20%E7%9F%A5%E4%B9%8E/v2-cdbbe04291aaf31ed0922c44d1529957_1440w.jpg)

从实际出发选择合适的模型架构和训练方法是NLPer的必备技能

**02**

**交叉编码器（Cross-encoders）**

现在，好奇的你可能会问自己这样一个问题：

**"难道不能把语义相似性问题转化为一个句子分类问题吗？也就是将两个句子分类拼接后，模型对其打上 "相关 "或 "不相关"的标签，而不是将所有这些句子进行单独编码。"**

这样想是非常正确的，因为****拼接后的句子之间语义还有交互，能最大限度的利用语句之间的语义信息，效果比单独编码句子后再进行相似度比较要好得多****。事实上，这正是交叉编码器（cross-encoder）所做的。

从本质上讲，交叉编码器所做的是将两个句子通过分隔符<SEP>拼接起来，并将其“喂进”一个语言模型。在语言模型的顶部有一个分类头，用以训练来预测一个目标 "相似度 "数值。

因此，例如，交叉编码器会将 "我的狗很快 "和 "他有一个V8柴油发动机 "这两个句子连接起来，并预测一个低的相似性分数，尽管它们都包含与速度有关的词汇。这当然是正确的。每个人都知道，Snuffles的引擎盖下摇晃着一台V12 TDI，它确实是非常省油的。

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20%E6%8A%80%E6%9C%AF%E5%B9%B2%E8%B4%A7%EF%BC%9A%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E9%AB%98%E6%80%A7%E8%83%BD%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8VS%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8%20-%20%E7%9F%A5%E4%B9%8E/v2-46476afc4b955bc914cc0dc6b0df0775_1440w.jpg)

交叉编码器的架构

因此，交叉编码器通过句子对及表征其语义相似程度的基本事实标签（可能是离散的类别标签，或者是连续性的相似度数值）来进行有监督训练。

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20%E6%8A%80%E6%9C%AF%E5%B9%B2%E8%B4%A7%EF%BC%9A%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E9%AB%98%E6%80%A7%E8%83%BD%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8VS%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8%20-%20%E7%9F%A5%E4%B9%8E/v2-0586c23fc4401478f2ccf4ffd47072fa_1440w.jpg)

交叉编码器的训练数据样例

虽然交叉编码器在句子层面的任务上表现非常好，但它存在一个“致命”缺点：**交****交叉编码器不产生句子嵌入******。

在信息检索的使用场景下，这意味着我们不能预先计算文档嵌入并将其与查询嵌入进行有效的比较。我们也不能对文档嵌入进行索引，以进行有效的搜索。

在句子聚类的背景下，这意味着我们必须将每一对可能的文档都传递给交叉编码器，并计算出预测的相似度。

可想而知，这使得交叉编码器在实际应用中的速度慢到几乎无法使用的地步，也就是说，交叉编码器难以落地！

**03**

**双****编码器****（Bi-encoders）**

你可能已经注意到，我已经把 “sentence transformer”和 "双编码器（bi-encoder） "这两个术语几乎交替起来使用。这是有原因的：它们在许多场景下是可以互换的。Sentence transformer只是指Python包中的sentence-transformers和原始论文SBERT，而双编码器更多的是指实际的模型架构。  

交叉编码器的低效率弱点正是双编码器大放异彩的强项。虽然交叉编码器往往更准确，但双编码器的优势在于它可以产生实际的句子嵌入，这使得它们在现实世界中更快、更实用，因为它们允许索引、预先计算嵌入等。

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20%E6%8A%80%E6%9C%AF%E5%B9%B2%E8%B4%A7%EF%BC%9A%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E9%AB%98%E6%80%A7%E8%83%BD%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8VS%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8%20-%20%E7%9F%A5%E4%B9%8E/v2-2c0e64c70fa54f6a9be460143344f3e4_1440w.jpg)

双编码器的模型架构

举例来说，用交叉编码器对1万个句子进行聚类，需要计算大约5000万个句子对的相似性分数，这在BERT架构下需要大约65小时。在这段时间里，你可以把所有13部黄飞鸿系列电影看1遍，听完张学友历年专辑里的每一首歌，而且你还会有几个小时的时间。

相比之下，用双编码器完成同样的任务需要大约5秒钟。这大约仅是****" 以小民之见，我们不只要练武强身，以抗外敌，更重要的是广开言路，治武合一，那才是国富民强之道..."****一幕说话的长度。

然而，应该注意的是，知识蒸馏（knowledge distillation）的训练程序中，双编码器的学生模型试图模仿交叉编码器的教师模型，既能让模型精简缩小，也能保留原始模型九成以上甚至反超原始模型的效果，这是非常有实用价值的一个方向。但这本身是一个话题了，笔者在本篇中就不赘言了。

**04**

**关于Sentence transformer的训练**

sentence transformer的真正威力在于其训练程序，它是专门为语义相似性而设计的。Sentence transformer的训练常使用所谓的********孪生网络（ Siamese networks**********)**。

在孪生网络中，单条训练数据是由一个 "锚数据点（anchor data point）"、一个 "正例数据点（positive data point） "和一个 "负例数据点（negative data point） "组成的三元组。这里的训练目标是"三元损失函数（ triplet loss function）"---它是在最小化锚数据点嵌入到正例数据点嵌入之间的距离的同时，还最大化锚数据点嵌入到负例数据点嵌入之间的距离。

孪生训练（Siamese training）是一个从计算机视觉中借用的想法。假设你想设计一个人脸相似度判别系统。你将数据结构化为一个 "锚 "图像、一个 "正例 "图像（即同一个人的不同图像）和一个 "负例"图像（即另一个人的图像）的三元组。然后对模型进行训练，使锚和正例图像的嵌入非常接近，而锚和负例图像的嵌入则拉远。

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20%E6%8A%80%E6%9C%AF%E5%B9%B2%E8%B4%A7%EF%BC%9A%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E9%AB%98%E6%80%A7%E8%83%BD%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8VS%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8%20-%20%E7%9F%A5%E4%B9%8E/v2-31b12b261cc1ebd1c46c41ad838f4d99_1440w.jpg)

孪生网络最初是用来进行人脸识别的

以此类推，在sentence transformer的背景下，每条训练数据为一个锚定语句与一个在语义上与锚定语句相似的正例语句，以及一个在语义上与锚定语句不相似的负例语句的三元组。

这种学习范式实际上是“偷师”于CV领域的对比学习中的一种形式，它是显性的规定“负例”。

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20%E6%8A%80%E6%9C%AF%E5%B9%B2%E8%B4%A7%EF%BC%9A%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E9%AB%98%E6%80%A7%E8%83%BD%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8VS%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8%20-%20%E7%9F%A5%E4%B9%8E/v2-3c0720eec08100155c8ae40dd206dc1d_1440w.jpg)

通过对比学习学到通用且优良的文本表示

那么，现在句表示学习领域的“扛把子”其实是隐性的规定负例（只有锚定语句和一个正例，同一批次中的其他语句则为负例，或者指定一组锚定语句、正例、困难负例，同一批次的其他语句皆为负例），使用的是Multiple Negative Ranking Loss，数学上的表达式为：  

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20%E6%8A%80%E6%9C%AF%E5%B9%B2%E8%B4%A7%EF%BC%9A%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E9%AB%98%E6%80%A7%E8%83%BD%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8VS%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8%20-%20%E7%9F%A5%E4%B9%8E/v2-e89b4fdbe5957099324f60d23914e058_1440w.jpg)

Multiple Negative Ranking Loss公式

其直观解释为：

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20%E6%8A%80%E6%9C%AF%E5%B9%B2%E8%B4%A7%EF%BC%9A%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E9%AB%98%E6%80%A7%E8%83%BD%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8VS%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8%20-%20%E7%9F%A5%E4%B9%8E/v2-9a36cd4e9a67211f147220aa4c325f47_1440w.jpg)

Multiple Negative Ranking Loss的直观解释

同样地，训练过程会不断动态调整语句的嵌入，使锚定语句的嵌入在语义空间上，不断接近正例语句的嵌入，同时远离负例语句的嵌入。

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20%E6%8A%80%E6%9C%AF%E5%B9%B2%E8%B4%A7%EF%BC%9A%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E9%AB%98%E6%80%A7%E8%83%BD%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8VS%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8%20-%20%E7%9F%A5%E4%B9%8E/v2-615415a305029ac4ac40dbe9709fdd2c_1440w.jpg)

语义表示模型的核心要素

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20%E6%8A%80%E6%9C%AF%E5%B9%B2%E8%B4%A7%EF%BC%9A%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E9%AB%98%E6%80%A7%E8%83%BD%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8VS%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8%20-%20%E7%9F%A5%E4%B9%8E/v2-53e029041e19b8b369f6bc087fa26d3a_1440w.jpg)

双编码器的三元组训练数据样例

> _注意：这个训练过程迫使_sentence transformer_产生有语义的语句嵌入，即语义相似的语句的语句嵌入接近，而语义不相似的句子的语句嵌入不接近。_  

## ******在实际训练过程中，负例的选择是关键，一般而言，要得到较为通用、质量较高的语句表示，不要指望无监督训练，不********要指望无监督训练，不要指望无监督训练，重要的事情说三遍，训练过程中对模型注入关于现实世界的先验知识非常关键！**

## ****简而言之，要善于构造训练数据中的困难负例，也就是和正例字面义或者主题相近、容易搞混淆，但跟锚定语句没多大关系的语句，这种信息对于模型来说很难学习，但是这些信息更能反映语义的本质，学会后，模型的理解能力非常强悍！****

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20%E6%8A%80%E6%9C%AF%E5%B9%B2%E8%B4%A7%EF%BC%9A%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E9%AB%98%E6%80%A7%E8%83%BD%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8VS%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8%20-%20%E7%9F%A5%E4%B9%8E/v2-ed731ace97ba5f41dad56c5754f1fe2a_1440w.jpg)

要善于构造困难负例

构造困难负例的常规方法如下图所示，其中的关键点在于：

-   用Cross-Encoder对所有挖掘出的段落进行分类
    
-   仅将Cross-Encoder得分低于阈值的段落作为Bi-Encoder的困难负例，这是降噪环节，有助于提升负例质量
    

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20%E6%8A%80%E6%9C%AF%E5%B9%B2%E8%B4%A7%EF%BC%9A%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E9%AB%98%E6%80%A7%E8%83%BD%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8VS%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8%20-%20%E7%9F%A5%E4%B9%8E/v2-20dc4ed54c0578bf6bd7e0a0d24a6103_1440w.jpg)

如何寻找困难负例?

看看基于困难负例学习的实际效果，绝对是值！  

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20%E6%8A%80%E6%9C%AF%E5%B9%B2%E8%B4%A7%EF%BC%9A%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E9%AB%98%E6%80%A7%E8%83%BD%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8VS%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8%20-%20%E7%9F%A5%E4%B9%8E/v2-26bf328a1ff2868717a106e0cec52462_1440w.jpg)

少整那些花里胡哨的无监督玩法，要真正用起来，高质量三元组（困难负例版本）搞起来！  

**05**

**结语**

综上所述，我们对sentence transformer进行了非常详细的技术分析。然而，我们不要忽视大局，回顾一下我们所讨论的内容。

-   比较了sentence transformer和普通transformer，发现其中一个并不普遍地比另一个好。sentence transformer只是针对语义相似性进行了调整。我们讨论了交叉编码器，它将语义相似性作为一个分类问题。我们的结论是，虽然交叉编码器通常非常准确，但它们在实际应用中因效率问题往往不太实用，难以落地。
    
-   看到了双编码器是如何被设计来为交叉编码器的低效率提供解决方案的。
    
-   最后讨论了双编码器的训练过程，这是真正使其脱颖而出的原因，因为它们的基本架构与普通transformer的相同。
    
-   构建基于困难负例的有监督三元组数据集和基于对比学习的表示学习范式所得的模型具有良好的泛化能力
    

最后附上一个思维导图，包含笔者的一些总结：

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20%E6%8A%80%E6%9C%AF%E5%B9%B2%E8%B4%A7%EF%BC%9A%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E9%AB%98%E6%80%A7%E8%83%BD%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8VS%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8%20-%20%E7%9F%A5%E4%B9%8E/v2-6010381f24e588904a9fc3626ef1e8fc_1440w.jpg)

**作者介绍**

**高长宽****：达观数据解决方案副总监**

擅长数据分析和可视化表达，热衷于用数据发现洞察，指导实践。


# sentence transformer、双编码器（bi-encoders）和交叉编码器（cross-encoders）、何时使用它们以及它们与普通的transformer（regular transformers）的关系？用具体例子推理讲解算法

要理解Sentence Transformer、双编码器（Bi-Encoders）、交叉编码器（Cross-Encoders）与普通Transformer的关系及适用场景，我们可以从“文本匹配任务”的核心需求切入——**如何衡量两段文本的相关性**，并结合具体算法逻辑和例子展开分析。


### 一、普通Transformer（Regular Transformers）：通用文本理解的“基础模型”
普通Transformer（如BERT、GPT的底层结构）是一种基于自注意力机制的通用模型，核心能力是**理解文本的上下文语义**。它的输入是单段文本（或多段文本拼接），输出是每个token的上下文嵌入（Contextual Embedding），可用于下游任务（如分类、翻译、问答等）。

**在文本匹配中的局限性**：  
若直接用普通Transformer处理“判断两段文本是否相关”（如“查询-文档匹配”），需要将两段文本拼接成一个序列（如`[CLS] 文本A [SEP] 文本B [SEP]`），通过`[CLS]` token的输出做分类。这种方式的问题是：**每次匹配都需要重新计算整个序列的注意力，无法预先缓存中间结果**，导致在需要频繁匹配的场景（如检索系统）中效率极低。


### 二、双编码器（Bi-Encoders）：高效检索的“向量匹配器”
双编码器是为解决“高效文本匹配”设计的模型，本质是**两个共享参数（或不共享）的编码器**，分别处理两段文本，输出各自的固定维度向量，再通过向量相似度（如余弦相似度）衡量相关性。

#### 算法逻辑：
1. 用编码器1（如BERT的编码器部分）处理文本A，输出向量`v_A`；  
2. 用编码器2（与编码器1参数相同）处理文本B，输出向量`v_B`；  
3. 计算`v_A`与`v_B`的相似度（如`cos(v_A, v_B)`），值越高则相关性越强。  

#### 与普通Transformer的关系：
双编码器的“编码器”本质是普通Transformer的**编码器部分**（仅保留特征提取能力，去掉下游分类层）。可以理解为：**用两个Transformer编码器分别独立处理文本，将文本转化为固定向量**。

#### 适用场景：
需要**高效批量匹配**的场景，如信息检索（IR）、推荐系统。因为向量可以预先计算并存储在向量数据库中，新文本只需生成向量后与库中向量快速比对（时间复杂度`O(n)`，`n`为库中文本数量）。

#### 例子：问答系统的候选答案检索
- 有1000个预设问题和答案（如“什么是人工智能？”→“AI是...”），预先用双编码器将所有问题转化为向量，存入向量库；  
- 用户输入新问题“AI的定义是什么？”，用同一编码器生成向量`v_q`；  
- 计算`v_q`与向量库中所有问题向量的余弦相似度，取Top5最相似的问题，返回其答案。  
- 优势：无需对每个新问题与1000个旧问题逐一拼接计算，效率极高。


### 三、交叉编码器（Cross-Encoders）：高精度匹配的“上下文推理器”
交叉编码器直接将两段文本拼接成一个序列输入模型，通过模型的**交叉注意力机制**（同时关注两段文本的交互）输出相关性分数。它不生成独立向量，而是直接建模两段文本的关联。

#### 算法逻辑：
1. 将文本A和文本B拼接为`[CLS] A [SEP] B [SEP]`，输入普通Transformer；  
2. 模型通过自注意力（同时关注A和B的每个token）捕捉两段文本的交叉信息；  
3. 用`[CLS]` token的输出通过全连接层预测相关性分数（如0-1之间的概率）。  

#### 与普通Transformer的关系：
交叉编码器本质是**普通Transformer在文本匹配任务上的直接应用**（保留完整的自注意力机制，增加相关性预测层）。它充分利用了Transformer的交叉注意力能力，但牺牲了效率。

#### 适用场景：
需要**高精度匹配**的场景，如排序（Reranking）、语义相似度判断。因为它能捕捉细粒度的交互（如“苹果手机”和“iPhone”的关联），但每次匹配都需重新计算，效率低（时间复杂度`O(n^2)`，`n`为文本长度）。

#### 例子：检索结果的重排序
- 用双编码器从1000个文档中检索出Top10与查询相关的文档（高效但可能有误差）；  
- 用交叉编码器对“查询+每个候选文档”的组合逐一计算精确相关性分数；  
- 根据分数重新排序，得到更准确的Top5结果。  
- 优势：双编码器筛选出少量候选，交叉编码器精排，兼顾效率和精度。


### 四、Sentence Transformer：双编码器的“工程实现”
Sentence Transformer是一个**开源工具库**，核心功能是将文本（句子、段落）转化为固定维度的向量，其底层默认采用**双编码器结构**（基于BERT、RoBERTa等Transformer模型改造）。

#### 与双编码器的关系：
Sentence Transformer是双编码器的**工程化实现**，它解决了普通Transformer输出“变长token向量”的问题——通过均值池化、最大池化或`[CLS]` token提取等方式，将Transformer输出的多个token向量压缩为**单个句子向量**，方便直接计算相似度。

#### 例子：句子相似度计算
- 输入两个句子：“猫坐在垫子上”和“一只猫咪趴在地毯上”；  
- Sentence Transformer的双编码器分别生成两个向量`v1`和`v2`；  
- 计算`cos(v1, v2) = 0.89`（高分，说明语义相似）。  


### 五、总结：关系与适用场景对比
| 模型/工具       | 核心机制                          | 与普通Transformer的关系               | 优势                     | 劣势                     | 适用场景                     |
|-----------------|-----------------------------------|---------------------------------------|--------------------------|--------------------------|------------------------------|
| 普通Transformer | 自注意力理解单段文本上下文        | 基础模型，提供编码器/解码器能力       | 通用，能捕捉复杂语义     | 文本匹配时效率极低       | 文本分类、翻译等单文本任务   |
| 双编码器        | 独立编码文本为向量，向量相似度匹配 | 基于Transformer编码器，输出固定向量   | 效率极高，支持批量检索   | 精度较低（丢失交叉信息） | 信息检索、推荐系统（粗筛）   |
| 交叉编码器      | 拼接文本，交叉注意力直接预测相关性 | 基于Transformer完整结构，建模交叉信息 | 精度极高，捕捉细粒度关联 | 效率极低，无法批量计算   | 检索重排序、高精度相似度判断 |
| Sentence Transformer | 双编码器的工程实现，生成句子向量 | 封装了Transformer，提供易用的向量接口 | 开箱即用，支持多语言     | 本质是双编码器，精度受限 | 快速计算句子/段落相似度     |


**一句话总结**：普通Transformer是基础，双编码器（Sentence Transformer）用“向量匹配”换效率，交叉编码器用“交叉注意力”换精度，实际应用中常结合两者（如“双编码器粗筛+交叉编码器精排”）。

# 二、双编码器（Bi-Encoders）：高效检索的 “向量匹配器”。 是不是就是推荐系统中的双塔模型，也就是召回/初排？。三、交叉编码器（Cross-Encoders）：高精度匹配的 “上下文推理器”。 是不是就是推荐系统中的reranker，也就是rerank/精排？

你的理解非常准确！双编码器（Bi-Encoders）本质上就是推荐系统中常用的**双塔模型**，主要用于召回（Recall）或初排（First-stage Ranking）；而交叉编码器（Cross-Encoders）则对应推荐系统中的**重排序模型（Reranker）**，用于精排（Second-stage Ranking）。两者的设计逻辑与推荐系统的“两阶段排序”思路完全吻合，我们可以结合推荐场景具体展开：


### 一、双编码器 = 双塔模型 = 召回/初排
推荐系统的核心目标是从海量候选物品（如商品、视频、文章）中找到用户可能感兴趣的内容。但“海量”意味着候选集可能有百万甚至亿级规模，直接对所有候选计算精确相关性不现实——因此需要先通过“召回”阶段快速筛选出少量（如几百到几千）潜在相关的候选，这正是双塔模型（双编码器）的用武之地。

#### 双塔模型的“双编码器”对应关系：
- 一个塔（编码器A）处理**用户侧特征**：如用户历史行为、 demographics（年龄/性别）、兴趣标签等，输出用户向量`u`；  
- 另一个塔（编码器B）处理**物品侧特征**：如物品标题、描述、类别、属性等，输出物品向量`v`；  
- 用向量相似度（如内积、余弦相似度）`score(u, v)`衡量用户对物品的兴趣，保留分数最高的Top N物品作为召回结果。  

#### 为什么是双编码器？
- **高效性**：物品向量可以预先计算并存储在向量数据库（如FAISS、Milvus）中，当新用户请求到来时，只需计算用户向量，再与库中物品向量批量比对（类似“向量检索”），时间复杂度极低，能支撑亿级物品的快速召回。  
- **双塔结构的独立性**：用户塔和物品塔的特征可以独立更新（如物品上新时只需更新物品向量，无需重新计算所有用户向量），工程上易于维护。  

#### 例子：短视频推荐的召回阶段
- 物品是千万级短视频，每个视频的标题、标签、画面特征等通过物品塔编码为向量，存入向量库；  
- 当用户打开APP时，用户的历史观看记录、关注列表等通过用户塔编码为用户向量`u`；  
- 用`u`在向量库中检索出最相似的500个视频向量，作为“可能感兴趣”的候选集，进入下一轮排序。  


### 二、交叉编码器 = 重排序模型（Reranker）= 精排
召回阶段得到的候选集（如500个物品）虽然相关性较高，但可能存在噪声（例如向量相似但实际语义不匹配），且未考虑用户与物品的细粒度交互（如用户历史是否点击过同类物品、物品描述中的某个关键词是否与用户近期兴趣强相关）。此时需要用精排模型进一步优化排序，交叉编码器正是为此设计。

#### 交叉编码器的“精排”逻辑：
- 将**用户特征与物品特征拼接**（类似文本匹配中“文本A+文本B”的拼接），输入模型；  
- 模型通过自注意力机制同时关注用户与物品的所有特征交互（如“用户最近搜索过‘悬疑片’”与“物品标签是‘悬疑’”的强关联，“用户年龄20岁”与“物品是儿童玩具”的弱关联）；  
- 直接输出用户对物品的精确兴趣分数（如点击率CTR、转化率CVR的预测值），按分数排序后返回Top N（如20个）物品给用户。  

#### 为什么是交叉编码器？
- **高精度**：交叉注意力能捕捉用户与物品的细粒度交互，避免双塔模型“独立编码”导致的信息丢失（例如双塔可能无法区分“用户喜欢科幻电影”和“用户喜欢科幻小说”，但交叉编码器通过拼接特征可以精准建模这种差异）。  
- **计算成本可控**：由于候选集已被召回阶段缩减到几百个，交叉编码器只需对这几百个“用户-物品”对逐一计算，虽然单对计算成本高于双塔，但整体效率可接受。  

#### 例子：短视频推荐的精排阶段
- 召回阶段得到500个候选视频；  
- 对每个视频，将“用户特征（如最近30天观看的视频标签、停留时长）+ 视频特征（如标题关键词、发布时间、创作者粉丝量）”拼接成输入，通过交叉编码器计算兴趣分数；  
- 按分数从高到低排序，最终返回20个视频展示给用户，此时的排序更符合用户真实兴趣。  


### 三、总结：推荐系统中的“双编码器+交叉编码器”流水线
推荐系统的典型流程是“**召回（双塔/双编码器）→ 精排（Reranker/交叉编码器）**”，两者缺一不可：  
- 双编码器（双塔）解决“从海量物品中快速缩小范围”的问题，靠效率取胜；  
- 交叉编码器（Reranker）解决“从少量候选中精准排序”的问题，靠精度取胜。  

这种组合既保证了系统能处理亿级物品的规模，又能提供高质量的推荐结果，是工业界推荐系统的主流架构（例如抖音的推荐、淘宝的商品推荐等均采用类似思路）。

---
created: 2025-10-30T07:20:38 (UTC +08:00)
tags: [人工智能,RAG搜索增强内容生成,深度学习（Deep Learning）]
source: https://zhuanlan.zhihu.com/p/7896055873
author: 关于作者deephubAI方向文章，看头像就知道，这里都是"干"货回答463文章2,032关注者49,402关注发私信
---

# (70 封私信 / 83 条消息) 解读双编码器和交叉编码器：信息检索中的向量表示与语义匹配 - 知乎

> ## Excerpt
> 在信息检索领域（即从海量数据中查找相关信息），双编码器和交叉编码器是两种至关重要的工具。它们各自拥有独特的工作机制、优势和局限性。作为检索增强生成 (RAG) 系统系列文章的第二篇，本文将深入探讨这两种核…

---
在信息检索领域（即从海量数据中查找相关信息），[双编码器](https://zhida.zhihu.com/search?content_id=250601909&content_type=Article&match_order=1&q=%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8&zhida_source=entity)和[交叉编码器](https://zhida.zhihu.com/search?content_id=250601909&content_type=Article&match_order=1&q=%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8&zhida_source=entity)是两种至关重要的工具。它们各自拥有独特的工作机制、优势和局限性。作为[检索增强生成](https://zhida.zhihu.com/search?content_id=250601909&content_type=Article&match_order=1&q=%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90&zhida_source=entity) (RAG) 系统系列文章的第二篇，本文将深入探讨这两种核心技术。

### 双编码器：高效的大规模检索

双编码器分别处理文档和搜索查询。可以将其类比为两个人独立工作：一人负责概括文档，另一人则专注于搜索查询，两者之间互不交流。“双”字体现了查询和文档的独立编码过程。

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20%E8%A7%A3%E8%AF%BB%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8%E5%92%8C%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E4%B8%AD%E7%9A%84%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E4%B8%8E%E8%AF%AD%E4%B9%89%E5%8C%B9%E9%85%8D%20-%20%E7%9F%A5%E4%B9%8E/v2-9e84efa36f4b25d2a40b74987e420eac_1440w.jpg)

用户查询和文档向量嵌入使用相同的嵌入模型计算，但两者完全隔离。

双编码器尤其适用于需要大规模实时检索的场景，例如搜索引擎或大型知识库。

### 双编码器的关键特性

**独立处理**

-   为查询和文档分别生成向量表示
-   允许预先计算文档嵌入
-   可将输入查询与数百万个预编码文档快速比较  
    **与近似最近邻 (ANN) 算法的兼容性**
-   固定维度向量表示非常适合 [ANN 算法](https://zhida.zhihu.com/search?content_id=250601909&content_type=Article&match_order=1&q=ANN+%E7%AE%97%E6%B3%95&zhida_source=entity)
-   通过在向量空间中逼近最近邻，高效搜索大型数据集
-   适用于处理海量数据的系统的快速检索  
    **对比学习**
-   通常使用对比学习技术进行训练
-   学习区分相似和不相似（正负样本）的查询-文档对
-   创建一个语义相似的项彼此更接近的向量空间  
    **优势**
-   高效搜索大型数据集
-   适用于实时应用
-   适用于初始的广度优先搜索  
    **局限性**
-   可能忽略查询和文档之间细微的语义关系
-   与交叉编码器相比，处理复杂查询的准确性较低  
    **交叉编码器：精准的相关性评估**  
    交叉编码器将搜索查询和每个文档一同进行比对，如同一个人仔细地逐一比较两份文本。这种方法通常能更准确地评估相关性，使交叉编码器在需要高精度语义匹配的任务中极具价值。  
    

![]((70%20%E5%B0%81%E7%A7%81%E4%BF%A1%20%2083%20%E6%9D%A1%E6%B6%88%E6%81%AF)%20%E8%A7%A3%E8%AF%BB%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8%E5%92%8C%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E4%B8%AD%E7%9A%84%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E4%B8%8E%E8%AF%AD%E4%B9%89%E5%8C%B9%E9%85%8D%20-%20%E7%9F%A5%E4%B9%8E/v2-e2f007931bdcb75117a5786f0ce714c4_1440w.jpg)

  
交叉编码器将两个文本片段（例如，用户查询和文档）同时作为输入。它不分别生成向量表示，而是输出 0 到 1 之间的值，表示输入对的相似度。  
交叉编码器在高精度至关重要的任务中尤其重要，例如在最终文档重排序阶段，或在语义匹配的准确性至关重要时。  
**交叉编码器的关键方面**  
**查询-文档对的联合处理**

-   将查询和文档作为单个输入一同处理
-   直接评估查询和文档之间的关系
-   实现更细致的语义匹配  
    **更准确地捕捉复杂关系**
-   擅长捕捉细微和复杂的语义关系
-   在法律或医学文档检索等领域具有重要价值  
    **高效的文档重排序**
-   通常用于检索的重排序阶段
-   优化双编码器检索到的初始文档集
-   确保语义最相关的文档排名最高  
    **优势**
-   更准确地评估相关性
-   更擅长捕捉复杂关系
-   适用于需要高精度的任务  
    **局限性**
-   对于大规模搜索，计算成本较高
-   不适用于海量文档集合的初始检索  
    **如何选择**  
    双编码器：
-   搜索海量文档集合
-   速度至关重要，例如实时网络搜索
-   执行初始的广度优先搜索  
    交叉编码器：
-   优化较小的搜索结果集
-   准确性比速度更重要
-   处理复杂查询或专业领域（例如，法律或医学搜索）  
    **使用的代码示例**  
    假设有四个句子 A、B、C 和 D，需要比较所有可能的配对：
-   双编码器需要分别编码每个句子，共需编码四次。
-   交叉编码器需要编码所有可能的配对，共需编码六次（AB、AC、AD、BC、BD、CD）。  
    假设有 100,000 个句子，需要比较所有可能的配对：
-   双编码器将编码 100,000 个句子。
-   交叉编码器将编码 4,999,950,000 对（根据组合公式：n! / (r!(n-r)!)，其中 n=100,000 且 r=2）。因此，交叉编码器的扩展性较差，在大规模数据集上计算成本过高。

**使用交叉编码器进行语义相似度检测的实际应用：** 尽管双编码器也可以完成此任务，但交叉编码器在牺牲一定处理速度的情况下能提供更高的准确性。

以下演示将使用微软的预训练模型 MS MARCO，通过两个句子对进行说明。模型输出一个分数，分数越高表示句子之间的语义相似度越高。

```text
 # 安装 sentence_transformers 库
 # pip install sentence_transformers

 fromsentence_transformersimportCrossEncoder

 # 初始化交叉编码器模型
 model=CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-2-v2', max_length=512)

 # 定义要比较的句子对
 sentence_pairs= [
     ('How many people live in Berlin?', 'Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.'),
     ('How many people live in Berlin?', 'Berlin is well known for its museums.')
 ]
 scores=model.predict(sentence_pairs)
 print(scores)

 # 输出: array([ 7.152365 , -6.2870445], dtype=float32)

 # 结果表明，第一对句子的语义相似度远高于第二对。

```

下面的代码片段演示了如何使用双编码器进行语义相似性搜索。模型将查询和语料库编码成嵌入向量，然后执行相似性搜索以找到最相关的段落。结果显示前 k 个匹配项（此处 k=25），每个匹配项包含语料库 ID 和相似度分数：

```text
 fromsentence_transformersimportSentenceTransformer, util

 # 初始化双编码器模型
 # 使用 'multi-qa-MiniLM-L6-cos-v1'，一个用于将问题和段落编码到共享嵌入空间的紧凑模型
 bi_encoder=SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')
 bi_encoder.max_seq_length=256

 # 将所有文本块编码为嵌入
 corpus_embeddings=bi_encoder.encode(chunks, convert_to_tensor=True, show_progress_bar=True)

 # 定义查询并搜索相关段落
 query="what is rlhf?"

 # 编码查询并计算与所有段落的相似度
 query_embedding=bi_encoder.encode(query, convert_to_tensor=True).cuda()

 # 检索前 25 个最相似的段落
 top_k=25
 hits=util.semantic_search(query_embedding, corpus_embeddings, top_k=top_k)[0]

 print(hits)

 # 输出:  
 # [{'corpus_id': 14679, 'score': 0.6097552180290222},
 #  {'corpus_id': 17387, 'score': 0.5659530162811279},
 #  {'corpus_id': 39564, 'score': 0.5590510368347168},
 #  ...]

```

使用高召回率但低精度的双编码器获取最相似的文本块后，可以通过第二阶段使用交叉编码器模型对结果进行重排序，利用其更高的准确性来优化结果。

以下是两阶段方法的实现：

```text
 fromsentence_transformersimportCrossEncoder

 cross_encoder=CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

 # 通过将查询与每个检索到的块配对来准备交叉编码器的输入
 cross_inp= [[query, chunks[hit['corpus_id']]] forhitinhits]

 cross_scores=cross_encoder.predict(cross_inp)

 print(cross_scores)

 # 输出是一个包含交叉编码器分数的数组，分数越高表示查询和文本块之间语义相似度越高。
 # array([ 1.2227577 ,  5.048051  ,  1.2897239 ,  2.205767  ,  4.4136825 ,
 #        1.2272772 ,  2.5638275 ,  0.81847703,  2.35553   ,  5.590804  ,
 #        1.3877895 ,  2.9497519 ,  1.6762824 ,  0.7211323 ,  0.16303705,
 #        1.3640019 ,  2.3106787 ,  1.5849439 ,  2.9696884 , -1.1079378 ,
 #        0.7681126 ,  1.5945492 ,  2.2869687 ,  3.5448399 ,  2.056368  ],
 #      dtype=float32)

```

代码使用交叉编码器模型对双编码器识别的查询-文本块对重新评分。交叉编码器提供更准确的相似度分数，从而实现更精细的排序。这种两阶段方法结合了双编码器在初始检索阶段的高效性和交叉编码器在最终排序阶段的高精度，为语义搜索任务提供了一种均衡的解决方案。

### 总结

双编码器和交叉编码器在现代信息检索中都扮演着至关重要的角色。双编码器提供速度和效率，是进行大规模初始搜索的理想选择。交叉编码器提供精度和深度，非常适合优化结果和处理复杂查询。理解它们的优势和局限性有助于构建能够同时处理广泛和细致信息需求的高效搜索系统。

作者：Marc Puig


---
created: 2025-10-30T07:25:37 (UTC +08:00)
tags: [用交叉编码器(cross-encoder)对检索结果重排序]
source: https://blog.csdn.net/srudfktuffk/article/details/145002396
author: srudfktuffk
---

# 利用Cross Encoder实现高效文档重排序_用交叉编码器(cross-encoder)对检索结果重排序-CSDN博客

> ## Excerpt
> 文章浏览阅读711次，点赞5次，收藏6次。在现代的信息检索系统中，简单的基于向量的检索（Retriever）有时可能无法提供最优的结果排序。为了解决这个问题，我们可以使用Cross Encoder来对初步检索结果进行重新排序（Reranking），以提高检索结果的相关性。Cross Encoder利用语言模型的强大语义理解能力，能够更精确地比较查询与文档之间的相关性。_用交叉编码器(cross-encoder)对检索结果重排序

---
#### 技术背景介绍

在现代的信息检索系统中，简单的基于向量的检索（Retriever）有时可能无法提供最优的结果排序。为了解决这个问题，我们可以使用Cross Encoder来对初步检索结果进行重新排序（Reranking），以提高检索结果的[相关性](https://so.csdn.net/so/search?q=%E7%9B%B8%E5%85%B3%E6%80%A7&spm=1001.2101.3001.7020)。Cross Encoder利用语言模型的强大语义理解能力，能够更精确地比较查询与文档之间的相关性。

#### 核心原理解析

Cross Encoder通过将查询与文档连接起来形成输入对，使用深度学习模型进行评分，从而为每个文档计算一个相关性分数。通过重新排序这些分数可以获得更符合用户意图的结果。

#### 代码实现演示

以下示例展示了如何利用[Langchain](https://langchain.openai.com/)和[Hugging Face](https://huggingface.co/)的Cross Encoder模型来实现检索结果的重排序。

```python
# 安装必要的Python包
!pip install faiss-cpu sentence_transformers

# 导入必要的库
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

# 加载并分割文档
documents = TextLoader("../../how_to/state_of_the_union.txt").load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
texts = text_splitter.split_documents(documents)

# 使用 Hugging Face 的 sentence-transformers 进行嵌入操作
embeddingsModel = HuggingFaceEmbeddings(model_name="sentence-transformers/msmarco-distilbert-dot-v5")
retriever = FAISS.from_documents(texts, embeddingsModel).as_retriever(search_kwargs={"k": 20})

# 基本检索操作
query = "What is the plan for the economy?"
docs = retriever.invoke(query)

# 使用 Cross Encoder 进行结果重排序
model = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-base")
compressor = CrossEncoderReranker(model=model, top_n=3)
compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)

compressed_docs = compression_retriever.invoke("What is the plan for the economy?")

# 输出结果
def pretty_print_docs(docs):
    print(f"\n{'-' * 100}\n".join([f"Document {i+1}:\n\n" + d.page_content for i, d in enumerate(docs)]))

pretty_print_docs(compressed_docs)

```

#### 应用场景分析

这种技术对于需要高精度检索的场景非常有效，比如：

-   法律文档的检索与分析
-   医疗信息系统中症状与诊断的匹配
-   客户服务中的FAQ自动响应系统

#### 实践建议

-   在使用Cross Encoder进行重排序时，请选择与您的应用场景充分匹配的预训练模型。
-   可以通过自定义微调来提升模型在特定领域的表现。
-   注意计算资源的消耗，Cross Encoder的计算需求相对较高，应合理分配资源。

#### 上传Hugging Face模型到SageMaker Endpoint

为了在生产环境中高效部署，我们可以将Hugging Face模型上传至AWS SageMaker Endpoint。这允许我们在云端以高效、可扩展的方式运行为重排序服务。

如果遇到问题欢迎在评论区交流。  
—END—


---
created: 2025-10-30T07:26:09 (UTC +08:00)
tags: [AI大模型]
source: https://zhuanlan.zhihu.com/p/29965983855
author: 关于作者静水流深算法工程师、职场妈妈回答1文章28关注者23关注她发私信
---

# (70 封私信 / 83 条消息) bi-encoder、cross-encoder、bge-reranker 区别 - 知乎

> ## Excerpt
> 1. 核心区别对比表特性Bi-Encoder（双塔模型）Cross-Encoder（交叉编码器）BGE-Reranker架构双独立编码器（Query/Doc分开编码）单编码器（Query+Doc联合编码）改进的Cross-Encoder（如基于Gemma架构）交互方式无交…

---
### **1\. 核心区别对比表**

| 特性 | Bi-Encoder（[双塔模型](https://zhida.zhihu.com/search?content_id=255015500&content_type=Article&match_order=1&q=%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B&zhida_source=entity)） | Cross-Encoder（[交叉编码器](https://zhida.zhihu.com/search?content_id=255015500&content_type=Article&match_order=1&q=%E4%BA%A4%E5%8F%89%E7%BC%96%E7%A0%81%E5%99%A8&zhida_source=entity)） | BGE-Reranker |
| --- | --- | --- | --- |
| 架构 | 双独立编码器（Query/Doc分开编码） | 单编码器（Query+Doc联合编码） | 改进的Cross-Encoder（如基于[Gemma架构](https://zhida.zhihu.com/search?content_id=255015500&content_type=Article&match_order=1&q=Gemma%E6%9E%B6%E6%9E%84&zhida_source=entity)） |
| 交互方式 | 无交互（独立编码后计算相似度） | 全量交互（联合编码捕捉细粒度关系） | 深度交互（可能优化注意力机制） |
| 训练目标 | 对比学习（最大化正样本相似度） | 直接回归/分类（预测相关性分数） | 多目标优化（对比学习+生成任务蒸馏） |
| 应用场景 | 召回阶段（海量文档快速筛选） | 精排阶段（小规模文档精准排序） | 精排或增强召回（支持更大规模/长文本） |
| 计算效率 | 极高（文档可预编码，毫秒级检索） | 低（每次需实时拼接编码，延迟高） | 中等（优化注意力，支持部分预计算） |
| 长文本处理 | 截断或分块编码（可能丢失信息） | 受限（通常截断至128-512 tokens） | 支持更长文本（如4k tokens） |
| 典型模型 | Sentence-BERT、BGE-Base-ZH | [MiniLM](https://zhida.zhihu.com/search?content_id=255015500&content_type=Article&match_order=1&q=MiniLM&zhida_source=entity)、Roberta-For-SequenceClass | [BGE-Reranker-v2-Gemma](https://zhida.zhihu.com/search?content_id=255015500&content_type=Article&match_order=1&q=BGE-Reranker-v2-Gemma&zhida_source=entity)、[BGE-Large](https://zhida.zhihu.com/search?content_id=255015500&content_type=Article&match_order=1&q=BGE-Large&zhida_source=entity) |
| 适用任务 | 召回、粗排、相似度搜索 | 精排、问答对匹配、短文本相关性 | 中文精排、长文档重排序、混合检索 |

### **2\. 架构与交互方式**

### **Bi-Encoder**

-   **双塔结构**：查询和文档分别通过独立的编码器生成向量（如`[CLS]`向量）。
-   **相似度计算**：通过余弦相似度或点积衡量相关性。
-   **优势**：文档向量可离线预计算，适合大规模召回。
-   **劣势**：缺乏细粒度交互，对复杂语义匹配能力较弱。

### **Cross-Encoder**

-   **联合编码**：将查询和文档拼接为`[CLS] Query [SEP] Doc [SEP]`输入模型。
-   **直接预测**：输出层通过回归或分类头生成相关性分数。
-   **优势**：捕捉Query-Doc深度交互，精度高。
-   **劣势**：无法预计算文档向量，计算成本高。

### **BGE-Reranker**

-   **改进架构**：基于生成式模型（如Gemma）改进，可能引入**稀疏注意力**或**LoRA微调**。
-   **混合优化**：结合对比学习和生成任务的知识蒸馏，增强泛化能力。
-   **长文本支持**：通过位置编码优化或分块处理支持更长输入。

### **3\. 训练策略与数据**

| 模型 | 训练数据 | 损失函数 | 关键优化技术 |
| --- | --- | --- | --- |
| Bi-Encoder | 三元组（Query, Pos, Neg） | MultipleNegativesRankingLoss | 困难负采样、批次内负样本增强 |
| Cross-Encoder | 标注文本对（Query, Doc, Score） | MSE/Cross-Entropy | 动态掩码、层间蒸馏 |
| BGE-Reranker | 混合数据（对比+生成任务） | 对比损失+KL散度 | Flash Attention、LoRA、长文本适配 |

### **4\. 应用场景与选型建议**

### **选择Bi-Encoder如果：**

-   需要从百万级文档库中快速召回Top 100候选。
-   资源有限（GPU算力低），需离线预计算文档向量。
-   任务对精度要求适中，但延迟要求严格（如实时搜索）。

### **选择Cross-Encoder如果：**

-   需要对少量（如Top 100）候选文档进行精准排序。
-   查询和文档较短（<512 tokens），允许较高延迟。
-   任务需要捕捉细粒度语义交互（如问答对匹配）。

### **选择BGE-Reranker如果：**

-   需处理中文或长文本（如论文、报告）的精排任务。
-   希望平衡精度与速度，支持千级文档快速重排序。
-   需利用生成式模型的知识增强（如领域自适应）。

### **5\. 性能权衡示例**

| 场景 | Bi-Encoder | Cross-Encoder | BGE-Reranker |
| --- | --- | --- | --- |
| 100万文档召回Top 100 | ✅ 毫秒级 | ❌ 不可行 | ❌ 过慢 |
| 1000文档精排Top 10 | ⚠️ 精度一般 | ✅ 高精度 | ✅ 高精度+较快 |
| 长文本（2000 tokens） | ❌ 截断损失 | ❌ 截断损失 | ✅ 高效处理 |
| 中文优化 | ✅ BGE-中文版 | ⚠️ 依赖多语言模型 | ✅ 原生中文支持 |

### **6\. 总结**

-   **Bi-Encoder**：**效率王者**，适合大规模召回，牺牲精度换速度。
-   **Cross-Encoder**：**精度标杆**，适合小规模精排，牺牲速度换质量。
-   **BGE-Reranker**：**平衡专家**，结合生成式模型优势，在中文、长文本场景表现突出。

实际选型需综合考虑 **文档规模、文本长度、语言场景、计算资源**，通常组合使用（如Bi-Encoder召回 → BGE-Reranker精排）以达到最佳效果。
