---
created: 2025-10-08T19:37:41 (UTC +08:00)
tags: [BM25,scoring algorithm,probabilistic information retrieval,TF-IDF,term frequency,inverse document frequency,document length normalization,final score calculation,advantages of BM25,robustness of BM25,efficiency of BM25,limitations of BM25,semantic understanding,hybrid retrieval models,information retrieval systems]
source: https://www.geeksforgeeks.org/nlp/what-is-bm25-best-matching-25-algorithm/
author: GeeksforGeeks
---

# ä»€ä¹ˆæ˜¯ BM25ï¼ˆæœ€ä½³åŒ¹é… 25ï¼‰ç®—æ³•ï¼Ÿ- æå®¢ä¸ºæå®¢ --- What is BM25 (Best Matching 25) Algorithm? - GeeksforGeeks

> ## Excerpt
> BM25 is a scoring algorithm employed by search engines to evaluate how well a document matches a specific search query. It belongs to the family of probabilistic information retrieval models, which aim to calculate the likelihood that a document is relevant to a user's query based on the statistical properties

---
Last Updated : 23 Jul, 2025  
æœ€åæ›´æ–° ï¼š 23 Julï¼Œ 2025

****BM25 is a scoring algorithm employed by search engines to evaluate how well a document matches a specific search query.**** It belongs to the family of probabilistic information retrieval models, which aim to calculate the likelihood that a document is relevant to a user's query based on the statistical properties of the text.  
****BM25 æ˜¯æœç´¢å¼•æ“ç”¨æ¥è¯„ä¼°æ–‡æ¡£ä¸ç‰¹å®šæœç´¢æŸ¥è¯¢çš„åŒ¹é…ç¨‹åº¦çš„è¯„åˆ†ç®—æ³•ã€‚**** å®ƒå±äºæ¦‚ç‡ä¿¡æ¯æ£€ç´¢æ¨¡å‹å®¶æ—ï¼Œæ—¨åœ¨æ ¹æ®æ–‡æœ¬çš„ç»Ÿè®¡å±æ€§è®¡ç®—æ–‡æ¡£ä¸ç”¨æˆ·æŸ¥è¯¢ç›¸å…³çš„å¯èƒ½æ€§ã€‚

BM25 is an evolution of earlier retrieval models like [****TF-IDF (Term Frequency-Inverse Document Frequency)****](https://www.geeksforgeeks.org/machine-learning/understanding-tf-idf-term-frequency-inverse-document-frequency/), addressing some of its shortcomings while maintaining computational efficiency. It was first introduced as part of the ****Okapi BM25**** system, developed at London City University in the 1980s and 1990s.  
BM25 æ˜¯ [****TF-IDFï¼ˆé¡¹é¢‘ç‡-é€†æ–‡æ¡£é¢‘ç‡ï¼‰****](https://www.geeksforgeeks.org/machine-learning/understanding-tf-idf-term-frequency-inverse-document-frequency/) ç­‰æ—©æœŸæ£€ç´¢æ¨¡å‹çš„æ¼”å˜ ï¼Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶è§£å†³äº†å…¶ä¸€äº›ç¼ºç‚¹ã€‚å®ƒæœ€åˆæ˜¯ä½œä¸º ****Okapi BM25**** ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†å¼•å…¥çš„ï¼Œè¯¥ç³»ç»Ÿäº 1980 å¹´ä»£å’Œ 1990 å¹´ä»£åœ¨ä¼¦æ•¦åŸå¸‚å¤§å­¦å¼€å‘ã€‚

> The "25" in BM25 refers to its development as part of the Okapi project, and it has since become synonymous with state-of-the-art text retrieval.  
> BM25 ä¸­çš„â€œ25â€æŒ‡çš„æ˜¯å®ƒä½œä¸º Okapi é¡¹ç›®çš„ä¸€éƒ¨åˆ†è¿›è¡Œå¼€å‘ï¼Œä»æ­¤æˆä¸ºæœ€å…ˆè¿›çš„æ–‡æœ¬æ£€ç´¢çš„ä»£åè¯ã€‚

## How Does BM25 Work?Â Â BM25 å¦‚ä½•å·¥ä½œï¼Ÿ

BM25 ranks documents based on how well they match a query, considering factors such as term frequency, document length, and inverse document frequency. Here's a breakdown of its key components:  
BM25 æ ¹æ®æ–‡æ¡£ä¸æŸ¥è¯¢çš„åŒ¹é…ç¨‹åº¦å¯¹æ–‡æ¡£è¿›è¡Œæ’åï¼Œå¹¶è€ƒè™‘æœ¯è¯­é¢‘ç‡ã€æ–‡æ¡£é•¿åº¦å’Œåå‘æ–‡æ¡£é¢‘ç‡ç­‰å› ç´ ã€‚ä»¥ä¸‹æ˜¯å…¶å…³é”®ç»„ä»¶çš„ç»†åˆ†ï¼š

### 1\. ****Term Frequency (TF)****  
1\. ****æœ¯è¯­é¢‘ç‡ ï¼ˆTFï¼‰****

****Term frequency**** measures how often a query term appears in a document. Intuitively, a document containing a query term multiple times is more likely to be relevant. However, BM25 introduces a ****saturation effect**** : beyond a certain point, additional occurrences of a term contribute less to the score. This prevents overly long documents from being unfairly favored.  
****æœ¯è¯­é¢‘ç‡è¡¡é‡****æŸ¥è¯¢æœ¯è¯­åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„é¢‘ç‡ã€‚ç›´è§‚åœ°è¯´ï¼Œå¤šæ¬¡åŒ…å«æŸ¥è¯¢è¯çš„æ–‡æ¡£æ›´æœ‰å¯èƒ½æ˜¯ç›¸å…³çš„ã€‚ç„¶è€Œï¼ŒBM25 å¼•å…¥äº†é¥±****å’Œæ•ˆåº”**** ï¼šè¶…è¿‡æŸä¸ªç‚¹ï¼Œä¸€ä¸ªæœ¯è¯­çš„é¢å¤–å‡ºç°å¯¹åˆ†æ•°çš„è´¡çŒ®è¾ƒå°ã€‚è¿™å¯ä»¥é˜²æ­¢è¿‡é•¿çš„æ–‡ä»¶å—åˆ°ä¸å…¬å¹³çš„é’çã€‚

Mathematically, the term frequency component is normalized using the formula:  
ä»æ•°å­¦ä¸Šè®²ï¼Œæœ¯è¯­é¢‘ç‡åˆ†é‡ä½¿ç”¨ä»¥ä¸‹å…¬å¼è¿›è¡Œå½’ä¸€åŒ–ï¼š

$TF(t,d)=\frac{freq(t,d)}{freq(t,d) + k_1 . (1-b+b.\frac{|d|}{\text{avgdl}})}$

where:Â Â å“ªé‡Œï¼š

-   $t$: Query term
-   $d$: Document
-   $freq(t,d)$: Number of times term $t$ appears in document $d$
-   $âˆ£dâˆ£$: Length of document $d$
-   $\text{avgdl}$: Average document length in the corpus
-   $k_1$: Controls the saturation effect (typically set between 1.2 and 2.0)
-   $b$: Controls the influence of document length (typically set to 0.75)

### 2\. ****Inverse Document Frequency (IDF)****  
2\. ****é€†æ–‡æ¡£é¢‘ç‡ ï¼ˆIDFï¼‰****

Inverse document frequency measures the importance of a term across the entire corpus. Rare terms are considered more informative than common ones. For example, the word "the" appears in almost every document and thus carries little value, whereas a rare term like "quantum" is more indicative of relevance.  
é€†æ–‡æ¡£é¢‘ç‡è¡¡é‡ä¸€ä¸ªæœ¯è¯­åœ¨æ•´ä¸ªè¯­æ–™åº“ä¸­çš„é‡è¦æ€§ã€‚ç½•è§æœ¯è¯­è¢«è®¤ä¸ºæ¯”å¸¸è§æœ¯è¯­ä¿¡æ¯é‡æ›´å¤§ã€‚ä¾‹å¦‚ï¼Œâ€œtheâ€ä¸€è¯å‡ ä¹å‡ºç°åœ¨æ¯ä¸ªæ–‡æ¡£ä¸­ï¼Œå› æ­¤ä»·å€¼ä¸å¤§ï¼Œè€Œåƒâ€œé‡å­â€è¿™æ ·çš„ç½•è§æœ¯è¯­æ›´èƒ½è¡¨æ˜ç›¸å…³æ€§ã€‚

The IDF component is calculated as:  
IDF ç»„ä»¶çš„è®¡ç®—å…¬å¼ä¸ºï¼š

$IDF(t)=log(\frac{N-n_t+0.5}{n_t+0.5})$

where:Â Â å“ªé‡Œï¼š

-   $N$: Total number of documents in the corpus  
    $N$ ï¼šè¯­æ–™åº“ä¸­çš„æ–‡æ¡£æ€»æ•°
-   $n_t$: Number of documents containing term $t$  
    ğ‘› ğ‘¡ n t  ï¼šåŒ…å«æœ¯è¯­ $t$ çš„æ–‡æ¡£æ•°

### 3\. ****Document Length Normalization****  
3\. ****æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–****

BM25 accounts for document length by normalizing scores to prevent longer documents from dominating the rankings. This is controlled by the parameter $b$, which adjusts the influence of document length relative to the average document length ($\text{avgdl}$).  
BM25 é€šè¿‡æ ‡å‡†åŒ–åˆ†æ•°æ¥è€ƒè™‘æ–‡æ¡£é•¿åº¦ï¼Œä»¥é˜²æ­¢è¾ƒé•¿çš„æ–‡æ¡£ä¸»å¯¼æ’åã€‚è¿™ç”±å‚æ•° æ§åˆ¶ï¼Œè¯¥å‚æ•° $b$ è°ƒæ•´æ–‡æ¡£é•¿åº¦ç›¸å¯¹äºå¹³å‡æ–‡æ¡£é•¿åº¦ ï¼ˆ ï¼‰ $\text{avgdl}$ çš„å½±å“ ã€‚

### 4\. ****Final Score Calculation****  
4\. ****æœ€ç»ˆåˆ†æ•°è®¡ç®—****

The final BM25 score for a document $d$ with respect to a query $q$ is computed as:  
æ–‡æ¡£ $d$ ä¸æŸ¥è¯¢ $q$ ç›¸å…³çš„æœ€ç»ˆ BM25 åˆ†æ•°è®¡ç®—å¦‚ä¸‹ï¼š

$Score(q,d) = \sum_{t\epsilon q}IDF(t).TF(t,d)$

This sums up the contributions of all query terms $t$ in the document $d$.  
è¿™æ±‡æ€»äº†æ–‡æ¡£ $d$ ä¸­æ‰€æœ‰æŸ¥è¯¢è¯ $t$ çš„è´¡çŒ® ã€‚

## Advantages of BM25Â Â BM25 çš„ä¼˜ç‚¹

1.  ****Robustness**** : BM25 performs consistently well across a wide range of datasets and domains. Its ability to balance term frequency, document length, and term rarity makes it highly reliable.  
    ****é²æ£’æ€§**** ï¼šBM25 åœ¨å¹¿æ³›çš„æ•°æ®é›†å’Œé¢†åŸŸä¸­å§‹ç»ˆè¡¨ç°è‰¯å¥½ã€‚å®ƒå¹³è¡¡æœ¯è¯­é¢‘ç‡ã€æ–‡æ¡£é•¿åº¦å’Œæœ¯è¯­ç¨€æœ‰æ€§çš„èƒ½åŠ›ä½¿å…¶é«˜åº¦å¯é ã€‚
2.  ****Efficiency**** : Despite its probabilistic foundation, BM25 is computationally efficient and can scale to large corpora, making it suitable for real-world applications like web search engines.  
    ****æ•ˆç‡**** ï¼šå°½ç®¡å…·æœ‰æ¦‚ç‡åŸºç¡€ï¼Œä½† BM25 çš„è®¡ç®—æ•ˆç‡å¾ˆé«˜ï¼Œå¯ä»¥æ‰©å±•åˆ°å¤§å‹è¯­æ–™åº“ï¼Œä½¿å…¶é€‚åˆç½‘ç»œæœç´¢å¼•æ“ç­‰å®é™…åº”ç”¨ã€‚
3.  ****Customizability**** : Parameters like $k_1$ and $b$ allow users to fine-tune BM25 for specific tasks or datasets, enhancing its adaptability.  
    ****å¯å®šåˆ¶æ€§**** ï¼šå‚æ•°å¦‚ k 1 k 1  å¹¶ $b$ å…è®¸ç”¨æˆ·é’ˆå¯¹ç‰¹å®šä»»åŠ¡æˆ–æ•°æ®é›†å¯¹ BM25 è¿›è¡Œå¾®è°ƒï¼Œå¢å¼ºå…¶é€‚åº”æ€§ã€‚
4.  ****Interpretability**** : Unlike deep learning-based models, BM25's scoring mechanism is transparent and interpretable, making it easier to debug and understand.  
    å¯****è§£é‡Šæ€§**** ï¼šä¸åŸºäºæ·±åº¦å­¦ä¹ çš„æ¨¡å‹ä¸åŒï¼ŒBM25 çš„è¯„åˆ†æœºåˆ¶æ˜¯é€æ˜ä¸”å¯è§£é‡Šçš„ï¼Œä½¿å…¶æ›´æ˜“äºè°ƒè¯•å’Œç†è§£ã€‚

## Limitations of BM25Â Â BM25 çš„å±€é™æ€§

While BM25 is a powerful algorithm, it is not without limitations:  
è™½ç„¶ BM25 æ˜¯ä¸€ç§å¼ºå¤§çš„ç®—æ³•ï¼Œä½†å®ƒå¹¶éæ²¡æœ‰å±€é™æ€§ï¼š

1.  ****Lack of Semantic Understanding**** : BM25 operates at the lexical level, meaning it matches terms based on exact word forms. It does not account for synonyms, paraphrases, or semantic relationships. For example, "car" and "automobile" would be treated as unrelated terms.  
    ****ç¼ºä¹è¯­ä¹‰ç†è§£**** ï¼šBM25 åœ¨è¯æ±‡çº§åˆ«è¿è¡Œï¼Œè¿™æ„å‘³ç€å®ƒæ ¹æ®ç¡®åˆ‡çš„è¯å½¢åŒ¹é…æœ¯è¯­ã€‚å®ƒä¸è€ƒè™‘åŒä¹‰è¯ã€é‡Šä¹‰æˆ–è¯­ä¹‰å…³ç³»ã€‚ä¾‹å¦‚ï¼Œâ€œæ±½è½¦â€å’Œâ€œæ±½è½¦â€å°†è¢«è§†ä¸ºä¸ç›¸å…³çš„æœ¯è¯­ã€‚
2.  ****Static Scoring**** : BM25 relies solely on statistical properties of the text and does not incorporate external knowledge or context. This can limit its performance on queries requiring deeper understanding or multi-hop reasoning.  
    ****é™æ€è¯„åˆ†**** ï¼šBM25 ä»…ä¾èµ–äºæ–‡æœ¬çš„ç»Ÿè®¡å±æ€§ï¼Œä¸åŒ…å«å¤–éƒ¨çŸ¥è¯†æˆ–ä¸Šä¸‹æ–‡ã€‚è¿™å¯èƒ½ä¼šé™åˆ¶å…¶åœ¨éœ€è¦æ›´æ·±å…¥ç†è§£æˆ–å¤šè·³æ¨ç†çš„æŸ¥è¯¢ä¸Šçš„æ€§èƒ½ã€‚
3.  ****Sensitivity to Corpus Characteristics**** : The effectiveness of BM25 depends on the quality and structure of the corpus. In highly specialized or noisy datasets, its performance may degrade.  
    ****å¯¹è¯­æ–™åº“ç‰¹å¾çš„æ•æ„Ÿæ€§**** ï¼šBM25 çš„æœ‰æ•ˆæ€§å–å†³äºè¯­æ–™åº“çš„è´¨é‡å’Œç»“æ„ã€‚åœ¨é«˜åº¦ä¸“ä¸šåŒ–æˆ–å˜ˆæ‚çš„æ•°æ®é›†ä¸­ï¼Œå…¶æ€§èƒ½å¯èƒ½ä¼šä¸‹é™ã€‚
4.  ****No Support for Dense Representations**** : Unlike modern dense retrieval methods (e.g., ****Dense Passage Retrieval**** ), BM25 does not leverage embeddings or neural networks to capture semantic similarity.  
    ****ä¸æ”¯æŒå¯†é›†è¡¨ç¤º**** ï¼šä¸ç°ä»£å¯†é›†æ£€ç´¢æ–¹æ³•ï¼ˆä¾‹å¦‚å¯†é›†****æ®µè½æ£€ç´¢**** ï¼‰ä¸åŒ ï¼ŒBM25 ä¸åˆ©ç”¨åµŒå…¥æˆ–ç¥ç»ç½‘ç»œæ¥æ•è·è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚

## BM25 in PracticeÂ Â BM25 çš„å®è·µ

BM25 has been widely adopted in both academic research and industry applications. Some notable use cases include:  
BM25 åœ¨å­¦æœ¯ç ”ç©¶å’Œè¡Œä¸šåº”ç”¨ä¸­éƒ½å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚ä¸€äº›å€¼å¾—æ³¨æ„çš„ç”¨ä¾‹åŒ…æ‹¬ï¼š

1.  ****Search Engines**** : BM25 serves as the backbone of many search engines, including open-source platforms like ****Apache Lucene**** and ****Elasticsearch**** .  
    ****æœç´¢å¼•æ“**** ï¼šBM25 æ˜¯è®¸å¤šæœç´¢å¼•æ“çš„æ”¯æŸ±ï¼ŒåŒ…æ‹¬ ****Apache Lucene**** å’Œ ****Elasticsearch**** ç­‰å¼€æºå¹³å° ã€‚
2.  ****Question Answering Systems**** : In open-domain question answering, BM25 is often used as the initial retrieval step to fetch candidate documents before applying more sophisticated ranking or answer extraction techniques.  
    ****é—®ç­”ç³»ç»Ÿ**** ï¼šåœ¨å¼€æ”¾åŸŸé—®ç­”ä¸­ï¼ŒBM25 é€šå¸¸ç”¨ä½œåˆå§‹æ£€ç´¢æ­¥éª¤ï¼Œåœ¨åº”ç”¨æ›´å¤æ‚çš„æ’åæˆ–ç­”æ¡ˆæå–æŠ€æœ¯ä¹‹å‰è·å–å€™é€‰æ–‡æ¡£ã€‚
3.  ****Recommendation Systems**** : BM25 can rank items (e.g., products, articles) based on textual descriptions, improving the relevance of recommendations.  
    ****æ¨èç³»ç»Ÿ**** ï¼šBM25 å¯ä»¥æ ¹æ®æ–‡æœ¬æè¿°å¯¹é¡¹ç›®ï¼ˆä¾‹å¦‚äº§å“ã€æ–‡ç« ï¼‰è¿›è¡Œæ’åï¼Œä»è€Œæé«˜æ¨èçš„ç›¸å…³æ€§ã€‚
4.  ****Legal and Medical Text Retrieval**** : BM25's precision and interpretability make it valuable in domains requiring accurate and explainable results, such as legal case retrieval or medical literature search.  
    ****æ³•å¾‹å’ŒåŒ»å­¦æ–‡æœ¬æ£€ç´¢**** ï¼šBM25 çš„ç²¾ç¡®æ€§å’Œå¯è§£é‡Šæ€§ä½¿å…¶åœ¨éœ€è¦å‡†ç¡®å’Œå¯è§£é‡Šç»“æœçš„é¢†åŸŸä¸­å…·æœ‰ä»·å€¼ï¼Œä¾‹å¦‚æ³•å¾‹æ¡ˆä¾‹æ£€ç´¢æˆ–åŒ»å­¦æ–‡çŒ®æ£€ç´¢ã€‚

## BM25 vs. Modern Approaches  
BM25 ä¸ç°ä»£æ–¹æ³•

With the rise of deep learning and neural networks, newer retrieval methods like ****Dense Passage Retrieval (DPR)**** have gained attention. These methods encode queries and documents into dense vector representations, capturing semantic relationships that BM25 cannot. However, BM25 remains competitive due to its simplicity, efficiency, and interpretability.  
éšç€æ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œçš„å…´èµ·ï¼Œ ****å¯†é›†é€šé“æ£€ç´¢ï¼ˆDPRï¼‰**** ç­‰è¾ƒæ–°çš„æ£€ç´¢æ–¹æ³•å¼•èµ·äº†äººä»¬çš„å…³æ³¨ã€‚è¿™äº›æ–¹æ³•å°†æŸ¥è¯¢å’Œæ–‡æ¡£ç¼–ç ä¸ºå¯†é›†çš„å‘é‡è¡¨ç¤ºï¼Œæ•è· BM25 æ— æ³•æ•è·çš„è¯­ä¹‰å…³ç³»ã€‚ç„¶è€Œï¼ŒBM25 å› å…¶ç®€å•æ€§ã€é«˜æ•ˆæ€§å’Œå¯è§£é‡Šæ€§è€Œä¿æŒç«äº‰åŠ›ã€‚

In practice, hybrid approaches combining BM25 with dense retrieval models are becoming increasingly popular. For example:  
åœ¨å®è·µä¸­ï¼Œå°† BM25 ä¸å¯†é›†æ£€ç´¢æ¨¡å‹ç›¸ç»“åˆçš„æ··åˆæ–¹æ³•æ­£å˜å¾—è¶Šæ¥è¶Šæµè¡Œã€‚ä¾‹å¦‚ï¼š

-   Use BM25 for fast initial retrieval.  
    ä½¿ç”¨ BM25 è¿›è¡Œå¿«é€Ÿåˆå§‹æ£€ç´¢ã€‚
-   Re-rank the top-k results using a cross-encoder or dense retrieval model.  
    ä½¿ç”¨äº¤å‰ç¼–ç å™¨æˆ–å¯†é›†æ£€ç´¢æ¨¡å‹å¯¹å‰ k ä¸ªç»“æœè¿›è¡Œé‡æ–°æ’åã€‚

This hybrid strategy leverages the strengths of both paradigms: BM25's speed and reliability, coupled with the semantic richness of neural models.  
è¿™ç§æ··åˆç­–ç•¥åˆ©ç”¨äº†ä¸¤ç§èŒƒå¼çš„ä¼˜åŠ¿ï¼šBM25 çš„é€Ÿåº¦å’Œå¯é æ€§ï¼Œä»¥åŠç¥ç»æ¨¡å‹çš„è¯­ä¹‰ä¸°å¯Œæ€§ã€‚

As the field of information retrieval evolves, BM25 will likely remain a key playerâ€”either as a standalone solution or as part of hybrid systems that combine the best of traditional and neural approaches. For anyone working in search or text analysis, understanding BM25 is essential, as it provides a solid foundation for building and improving retrieval systems.  
éšç€ä¿¡æ¯æ£€ç´¢é¢†åŸŸçš„å‘å±•ï¼ŒBM25 å¯èƒ½ä»å°†æ˜¯ä¸€ä¸ªå…³é”®å‚ä¸è€…â€”â€”æ— è®ºæ˜¯ä½œä¸ºç‹¬ç«‹è§£å†³æ–¹æ¡ˆè¿˜æ˜¯ä½œä¸ºç»“åˆäº†ä¼ ç»Ÿæ–¹æ³•å’Œç¥ç»æ–¹æ³•ä¼˜ç‚¹çš„æ··åˆç³»ç»Ÿçš„ä¸€éƒ¨åˆ†ã€‚å¯¹äºä»»ä½•ä»äº‹æœç´¢æˆ–æ–‡æœ¬åˆ†æå·¥ä½œçš„äººæ¥è¯´ï¼Œäº†è§£ BM25 éƒ½æ˜¯å¿…ä¸å¯å°‘çš„ï¼Œå› ä¸ºå®ƒä¸ºæ„å»ºå’Œæ”¹è¿›æ£€ç´¢ç³»ç»Ÿæä¾›äº†åšå®çš„åŸºç¡€ã€‚
