# 常用分词器分类、核心特点与适用领域、选型方法论
分词器是将连续文本拆分为有意义词汇（Token）的核心工具，广泛应用于自然语言处理（NLP）的各类场景。不同分词器因设计目标、语言适配性、性能特点不同，适用领域差异显著。以下从**常用分词器分类、核心特点与适用领域、选型方法论**三个维度展开，并附权威学习资料，帮助系统理解和应用。


## 一、常用分词器分类与核心特性（按语言/场景划分）
分词器的核心差异体现在**语言适配（单语/多语）、分词粒度（粗/细）、性能（速度/准确率）、定制化能力**四个维度。以下按“中文分词器”“多语言分词器”“垂直场景专用分词器”分类说明：


### 1. 中文分词器（中文无天然词边界，需特殊处理）
中文分词是NLP的基础难点，需解决“歧义切割”（如“下雨天留客天”）和“未登录词”（如网络新词、专有名词）问题，主流分词器如下：

| 分词器         | 核心原理                | 特点                                  | 适用领域                                  | 工具依赖/开源情况       |
|----------------|-------------------------|---------------------------------------|-------------------------------------------|-------------------------|
| **结巴分词（Jieba）** | 基于前缀词典的字符串匹配（正向/逆向/双向）+ HMM模型（处理未登录词） | - 优点：轻量、速度快（单机每秒百万字符）、支持自定义词典、词性标注<br>- 缺点：多义词歧义处理较弱、对复杂句法结构支持不足 | 中小规模中文NLP场景：如博客/评论情感分析、关键词提取、简单文本检索 | 开源（Python/Java），生态成熟，文档丰富 |
| **HanLP**       | 基于CRF（条件随机场）、深度学习（BERT/Transformer）的混合模型 | - 优点：准确率高（尤其复杂句法）、支持多任务（分词+词性标注+命名实体识别NER）、多语言适配（中/英/日/韩）<br>- 缺点：深度学习模式下速度较慢，资源占用较高 | 高精度需求场景：学术研究、新闻资讯分析、法律/医疗文本结构化（需精准分词） | 开源（Java/Python），支持自定义模型训练 |
| **THULAC（清华分词器）** | 基于CRF模型             | - 优点：学术级准确率、词性标注精细（支持134种词性标签）<br>- 缺点：速度较慢、自定义词典支持较弱 | 学术研究、对词性标注要求高的场景（如语言学分析、古籍文本处理） | 开源（C++/Python），清华NLP实验室维护 |
| **IK Analyzer**  | 基于词典的正向迭代匹配+歧义判断算法 | - 优点：专为搜索引擎优化（支持细粒度/粗粒度分词）、可配置性强（扩展词典/停用词表）、支持Lucene/Elasticsearch集成<br>- 缺点：复杂句子歧义处理一般 | 中文搜索引擎：如电商商品搜索（“华为手机壳”需拆为“华为”“手机壳”）、企业内部文档检索 | 开源（Java），Elasticsearch生态核心分词器 |
| **pkuseg（北大分词器）** | 基于深度学习（双向LSTM） | - 优点：对领域新词（如医疗“免疫球蛋白”、金融“量化宽松”）识别能力强、支持多领域预训练模型（通用/新闻/医疗/金融）<br>- 缺点：推理速度较慢，需GPU加速（大规模场景） | 垂直领域文本处理：医疗电子病历分词、金融研报分析、法律文书结构化 | 开源（Python），北大计算语言所维护 |


### 2. 多语言分词器（适配英、日、韩等多语言）
适用于跨语言NLP场景（如跨境电商、多语言舆情分析），核心解决不同语言的词边界特性（如英文按空格分割，日文有“假名+汉字”混合结构）：

| 分词器         | 支持语言                | 特点                                  | 适用领域                                  | 工具依赖/开源情况       |
|----------------|-------------------------|---------------------------------------|-------------------------------------------|-------------------------|
| **spaCy**       | 英、德、法、西、日、中（需扩展） | - 优点：工业级速度、集成NLP全流程（分词+POS+NER+句法分析）、支持预训练模型微调<br>- 缺点：中文分词需依赖外部插件（如`spacy-jieba`），多语言模型体积较大 | 多语言文本挖掘：跨境电商评论分析、多语言新闻聚类、国际舆情监测 | 开源（Python），工业界主流多语言NLP库 |
| **NLTK（Natural Language Toolkit）** | 英、法、德、葡等（需下载语料） | - 优点：轻量、模块化（可单独调用分词模块`nltk.tokenize`）、适合教学与快速原型<br>- 缺点：多语言支持需手动下载语料，性能低于spaCy | 教学演示、轻量级多语言任务（如英文文本分句、简单词频统计） | 开源（Python），NLP入门必备工具 |
| **ICU Tokenizer** | 支持100+语言（含小语种） | - 优点：基于Unicode标准，适配小语种（如阿拉伯语、斯瓦希里语）、支持规则自定义<br>- 缺点：无语义理解能力，仅基于字符/规则分割（如数字、标点处理） | 多语言内容清洗：国际化产品的文本预处理（如多语言APP的输入文本分割）、小语种数据标注前处理 | 开源（C++/Java/Python），Apache基金会维护 |
| **Hugging Face Tokenizers** | 支持所有主流语言（依赖预训练模型） | - 优点：基于Transformer模型（如BERT、GPT），支持“子词分词”（Subword，如“unhappiness”拆为“un+happiness”）、速度极快（C++内核）<br>- 缺点：需配合具体预训练模型（如中文需`bert-base-chinese`） | 深度学习NLP场景：大语言模型（LLM）预训练/微调、文本分类/生成任务（如ChatGLM输入预处理） | 开源（Python/Rust），LLM时代核心分词工具 |


### 3. 垂直场景专用分词器
针对特定领域的术语、缩写、 slang 优化，解决通用分词器“切错专业词”的问题：

| 分词器/工具         | 专注领域       | 核心优化点                                  | 适用场景                                  |
|----------------------|----------------|---------------------------------------------|-------------------------------------------|
| **医疗领域：MedLDA分词器** | 医疗文本       | 内置医学术语词典（如“冠状动脉粥样硬化”“单克隆抗体”），支持ICD-10编码关联 | 电子病历结构化、医学文献检索、临床数据分析 |
| **金融领域：金融分词器（如jieba金融扩展版）** | 金融文本       | 识别金融专有名词（如“北向资金”“量化交易”“不良贷款率”），支持行情术语（如“涨停/跌停”） | 金融研报关键词提取、股市舆情分析、信贷文本风险识别 |
| **代码领域：Tree-sitter Tokenizer** | 编程语言（Python/Java/C++） | 基于语法树分割代码（如区分“关键字if”“变量名user_id”“函数名get_data”） | 代码分析工具：代码查重、漏洞检测、编程教育AI助手 |
| **搜索引擎：Lucene Standard Tokenizer** | 通用搜索       | 支持多语言基础分词（英文按空格/标点分割，中文需配合IK），优化搜索相关性（如小写转换、去除停用词） | 开源搜索引擎：Solr、Elasticsearch的基础分词组件 |


## 二、分词器选型方法论（4步决策法）
选型的核心是**匹配业务需求与分词器特性**，避免“唯准确率论”（如搜索引擎需速度+粒度可控，而非极致准确率）。以下为4步选型流程：

### 1. 明确核心需求：先回答3个问题
- **语言与场景**：是单语言（如仅中文）还是多语言？是通用文本（新闻/评论）还是垂直领域（医疗/金融）？  
  → 例：中文电商搜索 → 优先IK Analyzer（支持粒度调整）；多语言舆情 → 优先spaCy。
- **性能指标**：每秒需处理多少文本（QPS）？是否接受GPU资源占用？  
  → 例：实时推荐系统（QPS>1000）→ 选Jieba/IK（轻量快）；离线医疗数据处理（无实时要求）→ 选pkuseg/HanLP（高精度）。
- **功能需求**：是否需要联动其他NLP任务（如分词后需词性标注、NER）？是否需要自定义词典（如企业专属术语）？  
  → 例：需同时做词性标注 → 选HanLP/spaCy；需添加行业术语（如“碳中和”）→ 选Jieba（支持自定义词典）。


### 2. 评估关键指标：3个核心维度对比
| 评估维度       | 关键指标说明                                  | 通用场景优先级                          | 垂直场景优先级                          |
|----------------|-----------------------------------------------|---------------------------------------|---------------------------------------|
| **准确率**     | 分词结果与人工标注的匹配度（如“北京大学”是否拆为“北京大学”而非“北京/大学”） | 中（通用文本可接受少量误差）          | 高（医疗/金融需100%准确识别专业词）    |
| **速度**       | 每秒处理字符数（Char/s）或文本条数（Doc/s）    | 高（实时场景如搜索、推荐）            | 中（离线处理可牺牲速度换精度）        |
| **可定制性**   | 是否支持自定义词典、规则调整、模型微调          | 中（通用场景无需频繁定制）            | 高（需添加领域新词、调整分词粒度）    |


### 3. 验证与测试：用业务数据做对比
选型不能依赖官方Benchmark（如HanLP在通用数据集准确率98%），需用**真实业务数据**测试：
- 步骤1：准备100-500条业务文本（如医疗场景的“电子病历片段”），人工标注正确分词结果（作为Ground Truth）。
- 步骤2：用2-3个候选分词器（如pkuseg医疗版 vs HanLP医疗模型）处理数据，计算**准确率（Precision）、召回率（Recall）、F1值**。
- 步骤3：测试性能：在目标硬件（CPU/GPU）上跑批量数据，统计QPS和资源占用（如内存、GPU显存）。


### 4. 考虑生态兼容性
- 若已使用Elasticsearch/Solr做搜索 → 优先选IK Analyzer（原生支持）；
- 若用PyTorch/TensorFlow做深度学习 → 优先选Hugging Face Tokenizers（与模型无缝衔接）；
- 若团队技术栈是Java → 选HanLP/IK Analyzer；是Python → 选Jieba/spaCy。


## 三、权威学习资料与工具库
以下资料覆盖分词器原理、源码解析、实战案例，适合从入门到深入：

### 1. 官方文档（最权威，含API与示例）
- **Jieba**：[GitHub - fxsjy/jieba: 结巴中文分词](https://github.com/fxsjy/jieba)（含Python/Java使用示例，自定义词典教程）
- **HanLP**：[HanLP官网](https://hanlp.com/)（含多语言分词、词性标注实战教程，支持在线演示）
- **Hugging Face Tokenizers**：[Tokenizers Documentation](https://huggingface.co/docs/tokenizers/en/index)（子词分词原理、与LLM配合使用指南）
- **spaCy**：[spaCy Documentation](https://spacy.io/usage)（多语言分词、NLP全流程实战）


### 2. 原理与进阶资料
- 书籍：《自然语言处理入门》（何晗 著）→ 详细讲解中文分词的词典匹配、HMM/CRF模型原理，含Jieba/HanLP源码解析。
- 论文：
  - 《Chinese Word Segmentation with Bi-LSTM-CRF and Neural Features》（pkuseg核心论文，深度学习分词原理）
  - 《Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates》（子词分词核心论文，Hugging Face Tokenizers基础）
- 博客：
  - Elastic官方博客：[Chinese Text Analysis with Elasticsearch](https://www.elastic.co/cn/blog/elasticsearch-and-chinese-text-analysis)（IK Analyzer在搜索中的应用）
  - 谷歌AI博客：[Tokenization: The First Step in NLP](https://ai.googleblog.com/2020/08/tokenization-first-step-in-nlp.html)（分词在LLM中的作用）


### 3. 实战工具与数据集
- 分词测试工具：[中文分词在线测试](https://www.sojson.com/comm/hanlp.html)（支持Jieba/HanLP/IK对比）
-  benchmark数据集：
  - 通用中文分词：MSRA分词数据集（新闻领域）、CTB8（TreeBank中文数据集）
  - 垂直领域：CCKS医疗分词数据集（电子病历）、金融分词数据集（蚂蚁金服公开数据集）


## 总结
分词器选型的核心是“**需求匹配**”：  
- 通用中文、追求速度 → 选Jieba；  
- 中文搜索、需粒度可控 → 选IK Analyzer；  
- 垂直领域（医疗/金融）、需高精度 → 选pkuseg/HanLP；  
- 多语言、需联动NLP全流程 → 选spaCy；  
- LLM预训练/微调 → 选Hugging Face Tokenizers。  

最终需通过**业务数据测试**验证，避免盲目追求“高精度但慢”或“快但不准”的极端选择。


# 不同分词器适用于不同场景。以下是常用分词器的分类、特点及选型建议

在自然语言处理（NLP）中，分词器是将文本拆分为有意义的基本单位（如词语、子词或字符）的工具，不同分词器适用于不同场景。以下是常用分词器的分类、特点及选型建议：


### 一、按语言/场景分类的常用分词器

#### 1. 中文分词器
- **Jieba（结巴分词）**
  - **领域**：通用中文处理（文本挖掘、搜索引擎、情感分析等）。
  - **特点**：支持精确模式、全模式、搜索引擎模式；轻量高效，可自定义词典；适合处理普通中文文本，但对新词、复杂句式的识别能力有限。
  
- **HanLP**
  - **领域**：多语言NLP任务（中文为主，支持英文、日文等）、信息抽取、句法分析。
  - **特点**：基于深度学习和规则融合，支持分词、词性标注、命名实体识别等一体化任务；准确率高，适合复杂场景，但性能消耗略高。
  
- **THULAC（清华分词器）**
  - **领域**：学术研究、高精度要求场景（如古籍处理、专业文献分词）。
  - **特点**：由清华大学开发，分词与词性标注准确率高，支持自定义模型，但灵活性较低，更新较慢。
  
- **IK Analyzer**
  - **领域**：搜索引擎（如Elasticsearch）、全文检索。
  - **特点**：专为索引和检索优化，支持细粒度分词和停用词过滤，适合构建中文搜索引擎。


#### 2. 英文及多语言分词器
- **NLTK（WordPunctTokenizer）**
  - **领域**：英文基础NLP任务（教育、科研、简单文本处理）。
  - **特点**：基于标点和空格分割，简单轻量，但无法处理 contractions（如don't→do not）等复杂情况。
  
- **spaCy Tokenizer**
  - **领域**：工业级NLP应用（文本分类、实体识别、机器翻译）。
  - **特点**：支持多语言（英、法、德等），基于预训练模型，能处理标点、缩写、URL等特殊格式，准确率高，适合生产环境。
  
- **BPE（Byte Pair Encoding）**
  - **领域**：低资源语言处理、大模型训练（如GPT、BERT）。
  - **特点**：将文本拆分为子词（如"unhappiness"→"un-happi-ness"），解决未登录词问题，适合跨语言或词汇量大的场景。
  
- **SentencePiece**
  - **领域**：神经网络机器翻译（NMT）、大模型预处理。
  - **特点**：无监督分词，支持子词和字符级分割，不依赖预处理规则，适合多语言统一处理。


#### 3. 其他场景专用分词器
- **ICU Tokenizer**
  - **领域**：国际化应用（多语言混合文本处理）。
  - **特点**：基于Unicode标准，支持全球语言的分词规则，兼容性强，但精度较低。
  
- ** Jieba-rs（Rust版Jieba）**
  - **领域**：高性能要求场景（实时分词、高并发服务）。
  - **特点**：结巴分词的Rust实现，速度比Python版快10-100倍，适合性能敏感的生产环境。


### 二、分词器核心特点对比
| 分词器       | 优势                                  | 劣势                                  | 典型场景                     |
|--------------|---------------------------------------|---------------------------------------|------------------------------|
| Jieba        | 轻量、速度快、支持自定义词典          | 新词识别弱，复杂句式准确率低          | 通用中文文本处理             |
| spaCy        | 多语言支持、准确率高、集成NLP流水线   | 资源占用较高，小语种支持有限          | 工业级英文/主流语言处理      |
| BPE/SentencePiece | 解决未登录词，适合大模型训练        | 分词结果碎片化，可读性差              | 机器翻译、大模型预处理       |
| HanLP        | 多任务一体化，高精度                  | 配置复杂，性能消耗大                  | 学术研究、复杂NLP任务        |


### 三、选型建议
1. **根据语言选择**：
   - 中文优先考虑 **Jieba（轻量）** 或 **HanLP（高精度）**；
   - 英文/多语言优先考虑 **spaCy（通用）** 或 **SentencePiece（大模型）**。

2. **根据任务需求**：
   - 搜索引擎/检索：选 **IK Analyzer**（中文）或 **Elasticsearch内置分词器**；
   - 大模型训练：选 **BPE** 或 **SentencePiece**（子词级分割）；
   - 实时/高并发场景：选 **Jieba-rs**（Rust）或 **spaCy（优化版）**。

3. **平衡准确率与性能**：
   - 简单任务（如关键词提取）用轻量分词器（Jieba、NLTK）；
   - 复杂任务（如句法分析）用高精度工具（HanLP、spaCy）。

4. **自定义需求**：
   - 需要添加专业术语时，优先选支持自定义词典的分词器（Jieba、HanLP）。


总之，分词器的选型需结合具体语言、任务复杂度、性能要求和自定义需求，必要时可通过对比测试（如准确率、速度、资源占用）确定最优方案。



# 具体例子用法

你可能是想了解“分词器”的使用方法。分词器的使用方式因具体工具和应用场景略有不同，但核心流程通常包括“安装→初始化→加载文本→执行分词→获取结果”几个步骤。以下是不同类型分词器的简单使用示例：


### 1. 中文分词器（以Jieba为例）
**适用场景**：快速处理中文文本（如提取关键词、文本分析）  
**使用步骤**：
```python
# 1. 安装
pip install jieba

# 2. 基本使用
import jieba

text = "我爱自然语言处理"

# 精确模式（默认）：最常用，适合文本分析
result = jieba.cut(text)
print(" ".join(result))  # 输出：我 爱 自然语言处理

# 全模式：列出所有可能的分词结果，适合搜索引擎
result = jieba.cut(text, cut_all=True)
print(" ".join(result))  # 输出：我 爱 自然 自然语言 语言 处理

# 3. 自定义词典（添加专业术语）
jieba.load_userdict("custom_dict.txt")  # 词典格式：术语 词频 词性
```


### 2. 英文分词器（以spaCy为例）
**适用场景**：英文NLP任务（如实体识别、句法分析）  
**使用步骤**：
```python
# 1. 安装
pip install spacy
python -m spacy download en_core_web_sm  # 下载英文预训练模型

# 2. 基本使用
import spacy

nlp = spacy.load("en_core_web_sm")  # 加载模型
text = "I love natural language processing."

doc = nlp(text)  # 处理文本
tokens = [token.text for token in doc]  # 提取分词结果
print(tokens)  # 输出：['I', 'love', 'natural', 'language', 'processing', '.']
```


### 3. 大模型专用分词器（以SentencePiece为例）
**适用场景**：机器翻译、大模型训练（处理子词级分割）  
**使用步骤**：
```python
# 1. 安装
pip install sentencepiece

# 2. 训练分词模型（针对自定义语料）
import sentencepiece as spm
spm.SentencePieceTrainer.train(
    input="corpus.txt",  # 训练语料
    model_prefix="mymodel",  # 模型前缀
    vocab_size=8000  # 词汇表大小
)

# 3. 加载模型并分词
sp = spm.SentencePieceProcessor()
sp.load("mymodel.model")
text = "我爱自然语言处理"
print(sp.encode_as_pieces(text))  # 输出子词结果（如：['▁我', '爱', '▁自然', '语言', '处理']）
```


### 4. 搜索引擎分词器（以IK Analyzer为例）
**适用场景**：Elasticsearch中文检索  
**使用步骤**：
1. 下载IK Analyzer插件并安装到Elasticsearch；
2. 在索引配置中指定分词器：
```json
{
  "settings": {
    "analysis": {
      "analyzer": {
        "ik_smart": {  // 粗粒度分词
          "type": "ik_smart"
        },
        "ik_max_word": {  // 细粒度分词（适合检索）
          "type": "ik_max_word"
        }
      }
    }
  }
}
```


### 核心使用逻辑总结：
1. **简单场景**：直接调用现成工具（如Jieba、spaCy）的API，一行代码完成分词；
2. **专业场景**：需先训练模型（如SentencePiece）或配置插件（如IK Analyzer）；
3. **定制化需求**：通过加载自定义词典、调整分词模式优化结果。

根据具体任务选择工具后，通常参考官方文档的“快速入门”即可完成基础使用。

# HMM CRF
HMM（隐马尔可夫模型）和CRF（条件随机场）是自然语言处理（尤其是序列标注任务，如分词、词性标注、命名实体识别）中两种经典的概率图模型。它们的核心目标是对**序列数据**（如文本中的字符/词语序列）进行建模，预测每个位置的标签（如分词中的“B-词首/I-词中/E-词尾/S-单字”）。


## 一、HMM 模型原理
HMM 是一种**生成式模型**，通过联合概率分布 \( P(X, Y) \) 建模输入序列 \( X \)（如字符序列）和输出序列 \( Y \)（如标签序列）的关系，核心假设是“**马尔可夫性**”：  
- 隐藏状态（标签）\( Y_t \) 只依赖于前一状态 \( Y_{t-1} \)（一阶马尔可夫链）；  
- 观测值（输入）\( X_t \) 只依赖于当前隐藏状态 \( Y_t \)（观测独立性假设）。

### 核心要素：
1. **初始状态概率**：\( \pi_i = P(Y_1 = i) \)（第一个标签为 \( i \) 的概率）；  
2. **状态转移概率**：\( A_{ij} = P(Y_{t} = j \mid Y_{t-1} = i) \)（从标签 \( i \) 转移到 \( j \) 的概率）；  
3. **发射概率**：\( B_{j}(x_t) = P(X_t = x_t \mid Y_t = j) \)（标签为 \( j \) 时生成观测 \( x_t \) 的概率）。

### 优势与局限：
- **优势**：模型简单、训练/推理速度快（可用Viterbi算法高效解码）；  
- **局限**：观测独立性假设过强（实际中输入序列的上下文往往相关，如“北京大学”中“北”和“京”高度关联），且无法直接利用全局特征（如远距离依赖）。


## 二、CRF 模型原理
CRF 是一种**判别式模型**，直接建模条件概率 \( P(Y \mid X) \)，即给定输入序列 \( X \) 时输出标签序列 \( Y \) 的概率。核心是**全局归一化**和**无严格独立性假设**，可纳入任意输入特征（如当前字符、前后n个字符、词性等）。

### 核心思想：
通过**特征函数**捕捉序列中的依赖关系，最终概率由所有特征的加权和经过指数归一化得到：  
\[ P(Y \mid X) = \frac{1}{Z(X)} \exp\left( \sum_{t=1}^T \sum_k \lambda_k f_k(Y_t, Y_{t-1}, X, t) \right) \]  
其中：  
- \( f_k \) 是特征函数（如“若 \( Y_{t-1}=B \) 且 \( Y_t=I \) 且 \( X_t=\text{京} \)，则返回1”）；  
- \( \lambda_k \) 是特征权重（通过训练学习）；  
- \( Z(X) \) 是归一化因子（确保概率和为1）。

### 优势与局限：
- **优势**：可利用全局上下文特征（如“北京大学”中“北”和“大”的关联）、无观测独立性假设、精度通常高于HMM；  
- **局限**：模型复杂度高、训练/推理速度较慢（需动态规划处理全局归一化）。


## 三、HMM 与 CRF 的关系
两者均用于序列标注任务，且存在递进关系：

### 1. 相同点：
- 均基于概率图模型，通过状态转移建模序列依赖；  
- 解码（预测最优标签序列）均可用动态规划算法（HMM用Viterbi，CRF用类似的维特比变种）。

### 2. 核心差异：
| 维度               | HMM                          | CRF                          |
|--------------------|------------------------------|------------------------------|
| 建模方式           | 生成式（\( P(X,Y) \)）        | 判别式（\( P(Y \mid X) \)）   |
| 独立性假设         | 强（观测独立、状态仅依赖前一状态） | 弱（可建模任意特征依赖）      |
| 特征利用           | 仅能利用局部观测特征         | 可利用全局上下文特征         |
| 归一化方式         | 局部归一化（每个位置单独归一） | 全局归一化（整个序列统一归一） |
| 适用场景           | 简单任务、实时性要求高       | 复杂任务、精度要求高         |

### 3. 演进关系：
CRF 可视为 HMM 的“升级版”：  
- HMM 因严格的独立性假设，在复杂场景（如中文分词中的歧义句）表现受限；  
- CRF 放松了假设，允许引入更丰富的特征（如字符的偏旁、词性、上下文窗口），因此在 NLP 序列标注任务中（如分词、NER）性能更优。


## 四、在 NLP 中的典型应用对比
- **HMM**：适用于简单场景或资源受限场景，如早期的中文分词（Jieba的HMM模式）、拼音输入法的联想预测；  
- **CRF**：适用于高精度需求场景，如 HanLP 的分词/词性标注、生物医学命名实体识别（需捕捉长距离依赖）。

简言之，HMM 是“轻量快速但假设严格”，CRF 是“灵活精准但计算复杂”，二者共同构成了现代序列标注模型（如 BiLSTM-CRF）的基础。

