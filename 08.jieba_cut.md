
## 使用参数
在jieba中，`jieba.add_word(wd, tag=tag, freq=300000)` 用于向分词词典中添加自定义词语。这里的 `freq` 参数表示词语的词频，值越大，该词语在分词时被优先识别和切分出来的概率就越高。

jieba 分词主要采用的是 **基于前缀词典的词图扫描算法** 结合 **动态规划** 来实现最优分词路径的选择：
1. 首先通过前缀词典构建一个有向无环图（DAG），记录句子中所有可能的分词组合
2. 然后使用动态规划算法寻找从句子开头到结尾的最优路径（基于词频等因素计算权重）
3. 对于未登录词（词典中没有的词），会使用 **HMM（隐马尔可夫模型）** 进行识别和切分

`freq` 参数影响的是词语在词典中的权重，词频越高，在动态规划计算最优路径时被选中的可能性就越大，从而更容易被单独分出来。

## 底层算法
### 一、jieba中HMM模型处理未登录词的具体例子

HMM（隐马尔可夫模型）主要用于处理词典中没有的"未登录词"（如人名、地名、网络新词等）。jieba中HMM通过"状态转移概率"和"发射概率"来预测汉字的成词可能性。

**例子：分词"李华在学习jieba"**

假设词典中没有"李华"这个词：
1. 首先， jieba会尝试用词典匹配，发现"李"和"华"都在词典中，但"李华"不在
2. 启动HMM模型，分析上下文：
   - 状态集合：B(词首)、M(词中)、E(词尾)、S(单字成词)
   - "李"作为句首字，更可能是B(词首)
   - "华"紧跟"李"后，更可能是E(词尾)
   - 因此HMM会将"李华"预测为一个词

代码验证：
```python
import jieba

# 确保"李华"不在默认词典中
seg_list = jieba.cut("李华在学习jieba")
print("/".join(seg_list))  # 输出：李华/在/学习/jieba
```

## 具体例子计算
### 二、前缀词典+动态规划的具体例子

**例子：分词"南京市长江大桥"**

1. **前缀词典构建**：
   词典中存在"南"、"南京"、"南京市"、"市"、"长江"、"大桥"等词
   构建的前缀词典会记录所有可能的词语及其词频

2. **构建有向无环图(DAG)**：
   句子索引：0:南, 1:京, 2:市, 3:长, 4:江, 5:大, 6:桥
   生成的DAG边包括：
   - 0→0（南）
   - 0→1（南京）
   - 0→2（南京市）
   - 2→2（市）
   - 3→4（长江）
   - 5→6（大桥）等

3. **动态规划寻找最优路径**：
   从后往前计算最大概率路径：
   - 位置6：只能是"桥"，概率P=log(桥的词频)
   - 位置5："大桥"的概率 > "大"+"桥"，选择5→6
   - 位置3："长江"的概率最高，选择3→4
   - 位置0："南京市"的概率 > "南京"+"市" > "南"+"京"+"市"，选择0→2

最终分词结果：南京市/长江大桥

代码验证：
```python
import jieba

seg_list = jieba.cut("南京市长江大桥")
print("/".join(seg_list))  # 输出：南京市/长江大桥
```

### 总结
- **前缀词典+动态规划**：处理已知词，通过词频计算最优分词路径
- **HMM模型**：处理未知词，通过汉字的位置特征（B/M/E/S）预测成词可能性
- 两者结合使jieba既能高效处理常规词汇，又能适应新词不断出现的中文环境

## jieba 所有函数
jieba 是 Python 中常用的中文分词库，支持多种分词模式和自定义词典等功能。下面详细介绍其核心函数、类的用法及背后的核心算法：


### 一、核心函数与用法
1. **分词函数：`jieba.cut()`**
   - 功能：对文本进行分词，返回一个可迭代的生成器
   - 参数：
     - `sentence`：待分词的字符串
     - `cut_all`：是否使用全模式（`True` 为全模式，`False` 为精确模式，默认 `False`）
     - `HMM`：是否使用 HMM 模型处理未登录词（默认 `True`）
   - 示例：
     ```python
     import jieba
     
     text = "我爱自然语言处理"
     # 精确模式（默认）
     print(list(jieba.cut(text)))  # ['我', '爱', '自然语言', '处理']
     
     # 全模式
     print(list(jieba.cut(text, cut_all=True)))  # ['我', '爱', '自然', '自然语言', '语言', '处理']
     ```

2. **分词函数：`jieba.lcut()`**
   - 功能：与 `cut()` 类似，但直接返回列表（无需转换）
   - 示例：
     ```python
     print(jieba.lcut(text))  # ['我', '爱', '自然语言', '处理']
     ```

3. **搜索引擎模式：`jieba.cut_for_search()`**
   - 功能：适合搜索引擎分词，对长词进一步拆分
   - 示例：
     ```python
     print(list(jieba.cut_for_search("小明硕士毕业于中国科学院计算所")))
     # ['小明', '硕士', '毕业', '于', '中国', '科学', '学院', '科学院', '中国科学院', '计算', '计算所']
     ```

4. **添加自定义词典：`jieba.load_userdict()`**
   - 功能：加载自定义词典，提升特定领域词汇的分词准确性
   - 词典格式：每行一词，格式为 `词语 词频 词性`（词频和词性可选）
   - 示例：
     ```python
     jieba.load_userdict("user_dict.txt")  # 加载自定义词典
     ```

5. **动态调整词典：`add_word()` 和 `del_word()`**
   - 功能：临时添加或删除词汇
   - 示例：
     ```python
     jieba.add_word("自然语言处理", freq=3, tag="n")  # 添加新词
     jieba.del_word("自然语言")  # 删除词语
     ```

6. **关键词提取：`jieba.analyse` 模块**
   - 基于 TF-IDF 算法：`extract_tags()`
     ```python
     from jieba.analyse import extract_tags
     print(extract_tags("我爱自然语言处理，自然语言处理很有趣", topK=2))  # ['自然语言', '处理']
     ```
   - 基于 TextRank 算法：`textrank()`
     ```python
     from jieba.analyse import textrank
     print(textrank("我爱自然语言处理，自然语言处理很有趣", topK=2))  # ['处理', '自然语言']
     ```


### 二、核心类：`Tokenizer`
- 功能：自定义分词器，支持多实例隔离（避免全局配置冲突）
- 示例：
  ```python
  from jieba import Tokenizer
  
  # 创建自定义分词器
  tokenizer = Tokenizer(dictionary="custom_dict.txt")
  print(list(tokenizer.cut("自定义分词器示例")))
  ```


### 三、核心算法
1. **基于前缀词典的正向最大匹配（精确模式）**
   - 原理：
     1. 构建前缀词典（收录所有可能的词语前缀）
     2. 从左到右取最大长度的前缀，若匹配词典则切分，否则缩短长度重试
   - 优势：速度快，适合常规文本

2. **全模式分词**
   - 原理：遍历所有可能的词语组合，不考虑歧义（会产生冗余结果）
   - 适用场景：需要穷尽所有可能分词结果的场景

3. **HMM 模型（处理未登录词）**
   - 原理：对于词典中未收录的词（如人名、新词），使用隐马尔可夫模型（HMM）预测分词边界
   - 核心：通过标注 "B（词首）、M（词中）、E（词尾）、S（单字）" 四个状态，训练模型预测最优分词路径

4. **关键词提取算法**
   - **TF-IDF**：通过词频（TF）和逆文档频率（IDF）计算词语重要性
   - **TextRank**：基于图模型，将词语视为节点，通过共现关系构建边，计算节点权重（类似 PageRank）


### 四、总结
- jieba 以 **前缀词典+最大匹配** 为基础，结合 **HMM 模型** 处理未登录词，平衡了分词速度和准确性
- 支持自定义词典和多模式分词，适用于大多数中文处理场景（文本分析、搜索引擎等）
- 关键词提取功能基于经典的 TF-IDF 和 TextRank 算法，可快速获取文本核心信息

使用时需根据场景选择合适的分词模式，对于专业领域文本，建议通过自定义词典优化结果。


## IF-IDF TextRank 关键词提取
下面通过具体例子详细解释 TF-IDF 和 TextRank 算法在关键词提取中的差异和原理：

### 一、TF-IDF 算法（extract_tags()）
TF-IDF 是 "词频-逆文档频率" 的缩写，核心思想是：**一个词的重要性与它在当前文档中出现的频率成正比，与它在所有文档中出现的频率成反比**。

#### 例子解析：
对于句子 `["我爱自然语言处理，自然语言处理很有趣"]`
1. **词频（TF）计算**：
   - "自然语言" 出现 2 次
   - "处理" 出现 2 次 
   - "我" 出现 1 次
   - "爱" 出现 1 次
   - "很" 出现 1 次
   - "有趣" 出现 1 次

2. **逆文档频率（IDF）**：
   假设我们有一个语料库（多个文档的集合），其中：
   - "自然语言" 和 "处理" 是专业术语，在其他文档中很少出现 → IDF 值较高
   - "我"、"爱" 等是常用词，在很多文档中都出现 → IDF 值较低

3. **最终结果**：
   `extract_tags(..., topK=2)` 返回 `['自然语言', '处理']`，因为这两个词的 TF-IDF 综合得分最高：
   - 它们在当前句子中出现频率高（TF 高）
   - 在整个语料库中相对罕见（IDF 高）


### 二、TextRank 算法（textrank()）
TextRank 基于图论，核心思想是：**如果一个词与很多重要的词相邻（共现），那么这个词也很重要**（类似网页排名的 PageRank 算法）。

#### 例子解析：
同样对于句子 `["我爱自然语言处理，自然语言处理很有趣"]`
1. **构建词图**：
   - 分词结果：`['我', '爱', '自然语言', '处理', '自然语言', '处理', '很', '有趣']`
   - 设置窗口大小为 2（相邻的词建立连接）：
     - "自然语言" 与 "处理" 相邻 → 建立连接
     - "处理" 与 "自然语言" 相邻 → 建立连接
     - 其他词的连接较少或权重低

2. **计算词的重要性**：
   - "处理" 和 "自然语言" 相互连接，形成强关联 → 权重较高
   - 其他词（如"我"、"爱"）的连接较少 → 权重较低

3. **最终结果**：
   `textrank(..., topK=2)` 返回 `['处理', '自然语言']`，因为这两个词在词图中相互引用，形成了更强的连接关系，权重得分最高。


### 三、两种算法的核心差异
| 特性 | TF-IDF | TextRank |
|------|--------|----------|
| 依赖 | 需要外部语料库计算 IDF | 仅依赖当前文档 |
| 原理 | 词频统计 + 全局罕见性 | 词与词的连接强度 |
| 适用场景 | 有大规模语料库时 | 单文档关键词提取 |

在上面的例子中，两种算法都提取到了相同的两个关键词，但顺序不同，这正是它们计算逻辑差异的体现。


## Tokenizer 分词器
`Tokenizer` 类是 jieba 中用于创建自定义分词器的核心类，它的主要价值在于实现**多实例隔离**，即不同的分词器可以使用不同的配置（如不同的词典）而不会相互干扰。这在需要同时处理多种分词需求的场景中非常有用。

下面通过具体例子详细解释其用法和作用：


### 场景举例：多领域分词需求
假设我们有两个场景需要处理：
1. 普通中文文本分词
2. 计算机专业术语分词（需要识别"机器学习"、"神经网络"等专业词汇）

如果直接使用 jieba 的全局分词器，添加专业词典后会影响普通文本的分词结果。这时就需要用 `Tokenizer` 创建两个独立的分词器实例。


### 具体实现步骤

#### 1. 准备两个不同的词典
- **普通词典**（默认词典，无需额外准备）
- **专业词典**（`tech_dict.txt`）：
  ```
  机器学习 5 n
  神经网络 5 n
  深度学习 5 n
  自然语言处理 5 n
  ```
  格式说明：`词语 词频 词性`（词频越高，分词时越优先被识别）


#### 2. 创建两个独立的分词器实例
```python
from jieba import Tokenizer

# 实例1：使用默认词典（普通文本分词器）
normal_tokenizer = Tokenizer()  # 不指定dictionary参数，使用默认词典

# 实例2：使用专业词典（技术文本分词器）
tech_tokenizer = Tokenizer(dictionary="tech_dict.txt")  # 加载自定义专业词典
```


#### 3. 对比两个分词器的效果
```python
text = "我在学习机器学习和自然语言处理"

# 普通分词器（未加载专业词典）
print("普通分词器结果：", list(normal_tokenizer.cut(text)))
# 输出：['我', '在', '学习', '机器', '学习', '和', '自然', '语言', '处理']
# （无法识别"机器学习"、"自然语言处理"等专业术语）

# 专业分词器（已加载专业词典）
print("专业分词器结果：", list(tech_tokenizer.cut(text)))
# 输出：['我', '在', '学习', '机器学习', '和', '自然语言处理']
# （成功识别专业术语）
```


### 为什么需要 `Tokenizer` 类？
如果不使用 `Tokenizer`，而是直接用 jieba 的全局函数（如 `jieba.cut()`），添加专业词典后会影响所有分词操作：
```python
import jieba

# 全局加载专业词典
jieba.load_userdict("tech_dict.txt")

# 此时即使处理普通文本，也会使用专业词典的配置
print(list(jieba.cut("机器学习是一门学科")))  # 会识别"机器学习"
```

而 `Tokenizer` 实现了**配置隔离**：
- 每个 `Tokenizer` 实例有自己独立的词典和配置
- 实例之间的设置互不影响
- 可以同时满足多种分词需求


### 常见用途
1. 多领域文本处理（如同时处理医疗、法律、普通文本）
2. 临时测试不同词典的效果
3. 在多线程环境中使用不同的分词配置
4. 需要保存/加载特定分词模型的场景

通过 `Tokenizer` 类，我们可以灵活地管理不同的分词需求，避免全局配置冲突，是处理复杂分词任务的重要工具。



## 结巴分词器的使用
可以简单认为 `Tokenizer` 为 `jieba.add_word` 等操作提供了**隔离环境**，而其底层分词算法与 jieba 全局函数完全一致。

具体来说：

1. **隔离性本质**  
   全局的 `jieba.add_word()` 会影响所有分词操作（因为修改的是全局词典），而 `Tokenizer` 实例的 `add_word()` 只影响当前实例：
   ```python
   from jieba import Tokenizer

   # 实例1
   tokenizer1 = Tokenizer()
   tokenizer1.add_word("自定义词1")

   # 实例2（不受实例1影响）
   tokenizer2 = Tokenizer()
   print("自定义词1" in tokenizer2.cut("这是自定义词1的测试"))  # 输出 False
   ```


2. **底层算法完全一致**  
   `Tokenizer` 实例的 `cut()` 方法与全局 `jieba.cut()` 使用完全相同的分词逻辑：
   - 同样基于**前缀词典+最大匹配**算法
   - 同样支持 HMM 模型处理未登录词
   - 同样支持全模式、精确模式等

   差异仅在于**使用的词典和配置相互隔离**，算法本身没有任何区别。


3. **适用场景总结**  
   - 若只需单一分词配置，用全局函数（`jieba.cut()`、`jieba.add_word()`）更简洁
   - 若需要同时维护多套分词规则（如多领域词典），必须用 `Tokenizer` 实现隔离

可以说，`Tokenizer` 是 jieba 为了解决**多场景配置冲突**而设计的"沙箱机制"，但核心分词能力与底层算法保持一致。
