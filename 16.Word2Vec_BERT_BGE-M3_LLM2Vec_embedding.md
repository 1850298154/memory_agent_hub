# Word2Vec、 BERT、BGE-M3、LLM2Vec，embedding模型选型指南｜最全

本文是 “向量数据库 POC 指南” 的系列文章之二。

在大模型落地过程中，**一个更科学、更符合实际生产的POC 可以帮我们规避生产中的大部分问题。**那如何做好POC？**本系列文章，**将对此做重点拆解。





RAG 要想高效地检索到相关信息，离不开高质量的embedding模型。一个合适的embedding模型，能在兼顾成本的基础上，显著提升检索的准确率、回答的相关性，以及整个系统的性能。

那么该如何选择embedding模型，常见的embedding模型又有哪些优劣势，我们将在本文进行解读。

# **01 什么是embedding，为什么重要**

Embedding 的本质是从非结构化数据到计算机语言的 语义翻译。可以将人类可理解的内容转化为高维向量空间中的数值表示，使计算机能通过向量运算捕捉文本间的语义关系。

在这个向量空间中，语义相似的文本会形成距离相近的向量，而差异显著的文本则相距较远。

### **1.1 三类 Embedding 向量的特性对比**

根据向量的结构与生成方式，embedding 向量可分为三大类，各自适用于不同场景：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/MqgA8Ylgeh4CLOFB8MD8zQOkkvIicxIpJcsnTeZLK6ENGsibexwXDTdjibckzXzDIIr0XCcic1HqQT4YLzVE3XjMKA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=2)

传统稀疏向量以 BM25 为例，通过统计术语频率与文档长度归一化优化关键词匹配，擅长捕捉文本中的显性关键词关联，但无法理解同义词、语义歧义等隐性关系。例如 “人工智能” 与 “AI” 在稀疏向量中会被视为完全不同的术语，导致检索漏检。

稠密向量则通过深度神经网络学习文本语义，例如 BERT 生成的向量能理解 “苹果发布新手机” 与 “iPhone 新品上市” 的语义关联，即使两者没有共同关键词也能被判定为相似。但稠密向量的计算成本较高，且解释性较弱。

“混合类型” 是近年的创新方向，如 BGE-M3 能生成包含稠密向量、稀疏向量，以及多向量表示 —— 即使文本中未出现 “机器学习”，模型也能通过语义推理在向量中赋予该术语一定权重，既保留了关键词匹配的精确性，又增强了语义理解能力。

# **02 评估embedding模型的八大要素**

### **2.1 上下文窗口（Context Window）**

上下文窗口指模型一次能处理的最大 token 数量（1 个单词约对应 1.33 个 token），直接决定模型处理长文本的能力。例如 OpenAI 的 text-embedding-ada-002 支持 8192 tokens，可覆盖大多数科研论文的完整结构（摘要、引言、方法、结论等）；而窗口为 512 tokens 的模型（如 m3e-base）处理长文档时需频繁截断，易丢失关键信息。

选择原则：长文档检索（如论文、法律文书）优先选择 8192 tokens 及以上窗口的模型；短文本场景（如客服对话）可放宽至 2048 tokens。

### **2.2 分词方式（Tokenization Unit）**

分词是将文本拆分为模型可处理的最小单元（token）的过程，直接影响模型对专业术语、罕见词的处理能力：

- 子词分词（BPE）：将单词拆分为子词（如 “unhappiness”→“un”+“happiness”），适合处理生僻词与没有被纳入词汇表的“未登录词”，是 GPT、LLaMA 等大模型的默认分词方式
- WordPiece：基于 BPE 的改进，基于频率的子词分词优化版，BERT 专用，能更高效平衡词汇覆盖与计算成本。
- 词级分词：按完整单词拆分，仅适用于词汇量有限的场景，无法处理复杂术语。

选择原则：专业领域（如医学、法律）优先选择子词分词模型，确保 “心肌梗死”“代位继承” 等专业术语能被正确拆分与理解。此外，现代模型（如 NV-Embed）通过改进分词策略（如引入潜在注意力层）提升术语处理能力，可作为专业领域的参考方案。

### **2.3 向量维度（Dimensionality）**

向量维度即嵌入向量的长度，反映模型承载语义信息的能力：

- 高维度（如 1536 维乃至更多）：能捕捉更细腻的语义差异，但存储与计算成本高，检索速度慢。
- 低维度（如 768 维）：计算高效、存储成本低，但可能丢失部分语义细节。

选择原则：平衡精度与性能，通用场景推荐 768-1536 维；高精度需求（如学术论文检索）可提升至 2000 维以上；资源受限场景可尝试 512 维并通过评估验证效果，一些搜索推荐场景，甚至可以采用更低维度的数据。

### **2.4 词汇量（Vocabulary Size）**

词汇量指模型分词器能识别的独特 token 数量，影响对特定语言、领域术语的覆盖能力。例如中文模型需包含足够的汉字、词语与标点 token，否则会将未识别文本标记为 “[UNK]”，导致语义丢失。

选择原则：多语言场景选择词汇量≥50k 的模型（如 BGE-M3）；专业领域需确保模型包含领域核心术语（如法律模型需覆盖 “诉讼时效”“善意取得” 等）。

### **2.5 训练数据**

模型的 “知识边界” 由训练数据决定：

- 通用数据（网页、书籍、维基百科）训练的模型（如 text-embedding-ada-002）适用于跨领域语义搜索。
- 领域数据（医学论文、法律文书）训练的模型（如 LegalBERT、BioBERT）在特定领域表现更优，但泛化能力较弱。

选择原则：垂直领域优先选择领域专用模型；通用场景需确保训练数据覆盖目标语言与场景（如中文模型需包含足量中文语料）。最新研究表明，混合通用数据与领域数据的模型（如 NV-Embed 的两阶段训练）可同时提升泛化能力与领域性能。

### **2.6 成本**

成本包括经济成本与算力成本：

- API 模型（如OpenAI）：按调用量收费，无需维护基础设施，适合快速验证与中小规模场景。
- 开源模型（BGE、Sentence-BERT）：免费但需自建 GPU/TPU 集群，适合大规模部署与定制化需求。

选择原则：原型验证阶段用 API 模型快速迭代；大规模生产环境优先评估开源模型的总拥有成本（TCO）。

### **2.7 MTEB 分数**

MTEB（Massive Text Embedding Benchmark）是目前最权威的 embedding 模型评测标准，通过语义搜索、文本分类、聚类等任务综合评分，分数越高代表模型通用性越强。但需注意：MTEB 分数高≠特定场景表现好，需结合实际数据验证。

### **2.8 领域专用性**

部分模型针对特定场景优化，例如：

- 法律领域：law-ai/LegalBERT 能精准理解 “抗辩”“管辖权” 等术语的法律含义。
- 生物医学：GanjinZero/biobert-base 擅长处理 “mRNA”“靶向治疗” 等专业表述。
- 多语言场景：BGE-M3 支持 100 + 语言，在中文、英文等多语言检索中表现均衡。
- 代码检索：Qwen3-Embedding 在 MTEB-Code 中得分超 81.0

***\*2.9更多参考角度\****

**多语言对齐性**：对于多语言 Embedding 模型，我们需要评估其不同语言向量空间的对齐程度，例如检查不同语言中语义相同或相近的词在向量空间中的距离是否接近。

**对抗性测试**：构造最小语义变化的输入，测试 Embedding 的稳定性，观察模型在面对微小语义变化时，生成的向量是否会发生合理的变化，还是出现较大波动。

**局部语义一致性（Coherence）：**评估在一个局部里语义相近词的靠近程度。给定一个目标词，用模型从候选样本中召回一定排名的词，让评测者选择认为的入侵词，统计每个模型每道题入侵词被选择的次数来评估。

# **03 常见embedding模型评估**

Embedding 技术的发展历程，本质是对 “语义理解深度” 与 “上下文适应性” 的持续优化。从早期的静态词向量到如今的 LLM 嵌入，每一代技术都解决了前序模型的核心痛点。

### **3.1 Word2Vec：词向量的奠基者（2013）**

作为首个大规模实用化的词嵌入模型，Google 提出的 Word2Vec 开创了 “用向量表示词义” 的范式。其核心思想源于语言学家 Zellig Harris 的 “分布假说”：上下文相似的词，语义也相似。例如通过分析 “Puma” 与 “金钱豹” 的相似上下文（栖息环境、捕猎习性等），模型能将两者映射到向量空间的相近位置。

Word2Vec 包含两种架构：

- CBOW（Continuous Bag of Words）：通过上下文预测中心词（如用 “我 / 要 / 后 / 海” 预测 “去”）。
- Skip-Gram：通过中心词预测上下文（如用 “去” 预测 “我 / 要 / 后 / 海”）。

优势：首次实现用低维向量（通常 300 维）捕捉词的语义关系，支持简单向量运算实现类比推理（如 “国王 - 男人 + 女人 = 女王”）。

局限性：生成的是静态向量，每个词对应唯一向量，无法处理多义词（如 “bank” 在 “河岸” 与 “银行” 场景中向量相同）；且仅能生成词向量，无法直接处理句子或文档级语义。

参考论文：https://arxiv.org/pdf/1301.3781

### **3.2 BERT：双向 Transformer 的革命（2018）**

BERT（Bidirectional Encoder Representations from Transformers）的出现标志着 embedding 技术进入 “深度语义理解” 时代。其核心突破是用双向 Transformer 替代 LSTM，并通过创新的预训练任务学习深层语义。

BERT 的关键技术：

- 双向 Transformer：通过自注意力机制，使每个词能同时关注前后所有词，实现真正的双向上下文理解。
- MLM（Masked Language Model）：随机遮盖 15% 的输入 token，让模型通过上下文预测被遮盖内容，强迫模型学习语义关联。
- NSP（Next Sentence Prediction）：训练模型判断两个句子是否连续，增强对句子间关系的理解。

输入编码：BERT 的输入向量由三部分叠加而成：

- Token Embeddings：词本身的向量表示。
- Segment Embeddings：区分不同句子（如第一句为 0，第二句为 1）。
- Position Embeddings：编码词在序列中的位置，弥补 Transformer 对词序不敏感的缺陷。

优势：生成的上下文向量能捕捉复杂语义关系，支持句子级、文档级 embedding；在问答、语义搜索等任务中刷新 SOTA（State-of-the-Art）。

局限性：上下文窗口早期版本仅 512 tokens，处理长文档需截断；生成的稠密向量解释性较弱，检索时难以定位关键匹配项。

补充说明：部分研究表明 NSP 对性能提升有限，后续模型（如 RoBERTa）取消 NSP 仅保留 MLM。

### **3.3 BGE-M3：稀疏与稠密的融合（2023****）**

```
在BAAI General Embedding-M3中：

- **BAAI** 是 **Beijing Academy of Artificial Intelligence**（北京人工智能研究院）的缩写，它是一家专注于人工智能基础研究和应用的科研机构。

- **M3** 具体含义在公开资料中没有非常明确的统一解释，结合该嵌入模型的定位和特性，推测可能是 **Multimodal, Multilingual, Massive**（多模态、多语言、大规模）的缩写，体现了该模型在处理多种数据类型、支持多语言以及模型规模方面的特点。这一命名方式常见于强调模型能力覆盖广度和规模的人工智能模型中。
```
BGE-M3（BAAI General Embedding-M3）是针对检索任务优化的混合模型，核心创新是同时生成稠密向量，稀疏向量，和多向量，结合他们优势：

- 稠密向量：捕捉深层语义关联，处理同义词、 paraphrase（同义改写）等场景。
- 稀疏向量：通过线性变换与 ReLU 激活，生成包含 “相关术语权重” 的稀疏表示，即使文本中未出现某术语，模型也能通过语义推理赋予其权重（如 “iPhone 新品” 会关联 “苹果公司”“智能手机” 等术语）。
- 多向量：此外，模型还支持每个 token 的嵌入向量计算交互得分，属于稠密向量的精细化应用。

BGE 的训练策略分为三阶段：

1. 预训练：基于大规模无标注数据，用 RetroMAE 策略（编码器掩码编码 + 解码器重建）学习基础语义。
2. 通用微调：在 1 亿对文本上用对比学习优化检索能力。
3. 任务微调：结合指令微调与难负样本采样，增强特定场景性能。

优势：多语言支持（中文表现优异）、多粒度处理（词 / 句 / 文档）、检索精度与效率平衡。BGE-M3 标志着我们在更精确、更高效地筛选和解读大量文本数据方面迈出了重要一步。

### **3.4 LLM-As-Embedding：大模型的嵌入能力（2023-）**

传统观点认为 decoder-only 大模型（如 GPT 系列）不适合生成 embedding，因其因果注意力（仅关注前文）限制语义理解。但近年研究通过架构改造，使 LLM 具备高质量嵌入能力，典型代表包括 LLM2Vec 与 NV-Embed。

#### **LLM2Vec 的改造思路：**

1. 双向注意力转换：将因果注意力掩码替换为全一掩码，使每个 token 能关注整个序列。
2. MNTP 任务：通过掩码下一词预测，让模型适应双向注意力。
3. 无监督对比学习：用 SimCSE 策略拉近距离相似文本向量，提升句子级表征质量。

```
在自然语言处理领域，MNTP 是 Masked Next-Token Prediction 的缩写，即掩码式下一 Token 预测。SimCSE 是 Simple Contrastive Learning of Sentence Embeddings 的缩写，即句子嵌入的简单对比学习。
```
#### **NV-Embed 的创新点：**

1. 潜在注意力层：将 LLM 最后一层隐状态作为 Query，可训练的潜在数组作为 Key/Value，实现更优的序列池化。
2. 双向注意力直接应用：去除因果掩码后，在对比学习中直接训练，简化改造流程。
3. 均值池化优化：对 token 嵌入加权平均，避免 “最后 token 偏差”。

优势：利用 LLM 的强大语义理解能力，在低资源场景下仍能生成高质量嵌入；支持长文本处理（窗口可达 8k-32k tokens）。

# **总结**

结合模型特性与业务需求，不同场景的模型选择建议如下：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/MqgA8Ylgeh4CLOFB8MD8zQOkkvIicxIpJeIpyF3iaCx6bVpIIYLwp7RMlGjtOqgVe0dJyTImicDtkV9ECQhCQn7dw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=3)

实施建议：

1. 优先通过 MTEB 子集（如检索任务）初步筛选模型，缩小候选范围。
2. 用业务真实数据构建测试集，评估召回率、精确率与响应时间。
3. 考虑向量数据库兼容性（如稀疏向量需数据库支持倒排索引）。
4. 长文档场景建议结合分段策略（如滑动窗口）与大窗口模型，平衡语义完整性与效率

对于 RAG 开发者而言，选择 embedding 模型的核心原则是 场景适配：没有放之四海而皆准的最优模型，只有最适合业务需求的选择。

