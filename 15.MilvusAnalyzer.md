# 多少做 RAG 的人，连分词都搞不定？Milvus Analyzer 指南

正文开始前，我们先复习个 RAG 人与向量数据库 er 的噩梦

Milvus 宣称 2.5 版本就已经引入了“全文检索（Full-text Search）”。

结果你搭了个 RAG 后发现：

地名、人名、专有词汇全！都！检！索！不！出！来！

比如，《鲁迅全集》中，能检索到“藤野先生”却检索不到“藤野”；做半导体术语，能搜“EUV”能搜“光刻机”就是搜不到“EUV光刻机”。

那是 embedding 模型选错了？

向量检索阈值设得太高了？

还是 Milvus 垃圾？（绝对不可能！）

或许，最大的可能是你在分词这一步，就把 Analyzer 选错了。

把 武汉市长江大桥 分成了 武汉市长、江大桥

把 霍格沃兹魔法学院 分成了 霍格沃兹魔、法学院

以及这样

![图片](https://mmbiz.qpic.cn/mmbiz_png/MqgA8Ylgeh5wbzXveUKibKVLk3W2YGrEyuKniasMgBmrcHCZB4xlyO9YPQpp5ia0hYLx7IDETicrOop5vNOImdlu8A/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=3)

（给所有广州朋友滑跪道歉）

当然，在分词环节，常见的问题除了过度分词之外，可能还会出现分词不足、语言不匹配等等情况。

而要解决这些问题，实现高效的全文检索，最重要的就是选对合适的 Analyzer。在文本处理中，Analyzer 可以将原始文本转换为结构化、可搜索的格式，它的选择直接关乎最终的查询质量。

那么，Analyzer 是如何工作的？不同场景如何对其选型？我们又该如何将其落地生产实践？

本文将带来重点解读。

# 01 什么是 Milvus Analyzer？

一句话来说，Milvus Analyzer 是 Milvus 提供的文本预处理与分词工具，用来将原始文本拆解为 token，并对其进行标准化和清洗，从而更好地支持全文检索和 text match。

下面这张架构图展示了 Milvus Analyzer 的整体结构：

![图片](https://mmbiz.qpic.cn/mmbiz_png/MqgA8Ylgeh5wbzXveUKibKVLk3W2YGrEy5XP0G5Fu8wVaUo5fgGAib2BDVVO2act58BJkAttLKkZa8kUicoicHgA6Q/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=4)

从图中可以看出，Milvus Analyzer 的整体处理流程可以总结为：原始文本 → Tokenizer → Filter → Tokens。

Analyzer 的核心组件有两个，Tokenizer（分词器）与 Filter（过滤器）。它们共同将输入文本转换为词元（token），并对这些词元进行优化，以便为高效的索引和检索做好准备。

- Tokenizer（分词器）：负责把文本切分成基础的 token，例如按空格切分（Whitespace）、中文分词（Jieba）、多语言分词（ICU）等。
- Filter（过滤器）：对 token 进行特定的处理方法，Milvus 内置了丰富的 filter，例如统一大小写（Lowercase）、去掉标点（Removepunct）、停用词过滤（Stop）、词干提取（Stemmer）、正则匹配（Regex）等。Milvus 支持设置多个 filter 按顺序处理，可以满足复杂的 token 处理需求。

![图片](https://mmbiz.qpic.cn/mmbiz_png/MqgA8Ylgeh5wbzXveUKibKVLk3W2YGrEyEOCJ30icWPtmAuWau46RvtkUrsicic5JUv3UJRvbcmsSMbVh4NGabUw5g/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=5)

## (1) Tokenizer

Tokenizer 是 Milvus Analyzer 的第一步处理工具，它的任务是将一段原始文本切分成更小的 token（词或子词）。不同语言、不同场景需要使用不同的 Tokenizer。Milvus 目前支持以下几类 Tokenizer：

![图片](https://mmbiz.qpic.cn/mmbiz_png/MqgA8Ylgeh5wbzXveUKibKVLk3W2YGrEyBPwMIJkYHum6MysjTTdDvNdEOic0diaDkwqXCwOW1H5nsvrsVeicmK1Dg/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=6)

在 Milvus 中，Tokenizer 是在创建 Collection 的 Schema 时配置的，具体是在定义 VARCHAR 字段时，通过 analyzer_params 指定。也就是说，Tokenizer 并不是一个单独的对象，而是绑定在字段级别的配置里，这样 Milvus 在插入数据时就会自动进行分词和预处理。

```python
# 在字段级别配置 Tokenizer
from pymilvus import DataType

FieldSchema(
    name="text",
    dtype=DataType.VARCHAR,
    max_length=512,
    analyzer_params={
        "tokenizer": "standard",  # 这里配置 Tokenizer
    },
)
```

## (2) Filter

如果说 Tokenizer 是把文本切开的刀，那 Filter 就是刀后面的精修工序。在 Milvus Analyzer 中，Filter 的作用是对切分后的 token 进行进一步的标准化、清洗或改造，让最终的 token 更适合用于检索。

例如，统一大小写、去掉停用词（如 “the”、“and”）、去除标点、词干提取（如 running → run）等，都是典型的 Filter 工作。

Milvus 内置了多种常用的 Filter，可以满足大部分语言处理需求：

![图片](https://mmbiz.qpic.cn/mmbiz_png/MqgA8Ylgeh5wbzXveUKibKVLk3W2YGrEy1ZbpWkMYsBzLln0agMv8sNe7AicGOuz5SMjXvHutASymibeOdnV15ZbQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=7)

使用 Filter 的好处是，你可以根据业务场景灵活组合不同的清洗规则。例如在英文搜索中，常见的组合是 Lowercase + Stop + Stemmer，这样可以保证大小写统一、去掉无意义词汇，并把不同形态的词统一成词干。

在中文搜索中，通常会结合 Cncharonly + Stop，让分词结果更简洁、更精准。在 Milvus 中，Filter 与 Tokenizer 一样，是通过 analyzer_params 配置在 FieldSchema 里的。举个例子：

```python
# 为英文配置常见的过滤器组合
from pymilvus import DataType

FieldSchema(
    name="text",
    dtype=DataType.VARCHAR,
    max_length=512,
    analyzer_params={
        "tokenizer": "standard",
        "filter": [
            "lowercase",
            {
                "type": "stop",  # 指定停用词过滤器
                "stop_words": ["of", "to", "_english_"],  # 自定义 + 英语停用词列表
            },
            {
                "type": "stemmer",  # 词干提取
                "language": "english",
            },
        ],
    },
)
```

# 02 Analyzer 类型

合适的 Analyzer 可以让我们的检索变得更高效与低成本。为了满足不同场景的需求，Milvus 提供了三类 Analyzer：

- 内置（Built-in）的 Standard / English / Chinese 三种 Analyzer
- 基于用户自定义的 Tokenizer 和 Filter 组成的 Custom Analyzer
- 在多语言文档场景中非常实用的 Multi-language Analyzer

## (1) Built-in Analyzer

内置 Analyzer 是 Milvus 自带的标准配置，开箱即用，适合大多数常见场景。它们已经预定义好 Tokenizer 和 Filter 的组合：

![图片](https://mmbiz.qpic.cn/mmbiz_png/MqgA8Ylgeh5wbzXveUKibKVLk3W2YGrEyYWDxNDRaQQVgpk7xsSkwlhG9p7qLFXDsGTicFuxtd4BWaL0icZZBiaQog/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=8)

如果你只是需要最常见的英文或中文搜索，可以直接使用内置 Analyzer，而无需额外配置。

这里需要注意一点，Standard Analyzer 默认是处理英文文档的，如果中文使用 Standard Analyzer，后续就会出现全文搜索没有结果的问题，社区里已经有不少朋友踩坑。

## (2) Multi-language Analyzer

在跨语言的文本库中，单一的分词器往往无法覆盖所有语种。为此，Milvus 提供了 Multi-language Analyzer，它会根据文本的语言自动选择合适的分词器。

不同语言使用的 Tokenizer 对照表：

![图片](https://mmbiz.qpic.cn/mmbiz_png/MqgA8Ylgeh5wbzXveUKibKVLk3W2YGrEyxW1tiapBh4mFRAVNpDibASlVEoaqJOq2D0I435F1SzjaKS8DAxmEYjVg/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=9)

这意味着，如果你的数据集同时包含英文、中文、日文、韩文甚至阿拉伯文，Milvus 可以在同一个字段里进行灵活处理，大大减少了手工预处理的复杂度。

## (3) Custom Analyzer

如果内置或多语言 Analyzer 不能完全满足需求，Milvus 还支持用户自定义 Analyzer。你可以自由组合 Tokenizer 和 Filter，从而形成一个符合业务特点的 Analyzer。

例如：

```python
# 自定义：中文使用 jieba + 仅中文字符 + 中文停用词
from pymilvus import DataType

FieldSchema(
    name="text",
    dtype=DataType.VARCHAR,
    max_length=512,
    analyzer_params={
        "tokenizer": "jieba",
        "filter": ["cncharonly", "stop"],  # 只搜中文并去掉中文停用词
    },
)
```

# 03 代码实践

下面我们通过 Python SDK 演示如何在 Milvus 中使用 Analyzer。我们会分别展示普通 Analyzer 和多语言 Analyzer 的用法。

本文使用的 Milvus 版本为 v2.6.1，Pymilvus 版本为 v2.6.1。

## (1) 普通 Analyzer

假设我们要建立一个英文文本搜索的 Collection，并在插入数据时自动完成分词和预处理。我们选用内置的 English Analyzer（相当于 standard + lowercase + stop + stemmer 的组合）。

```python
from pymilvus import MilvusClient, DataType, Function, FunctionType
import time

client = MilvusClient(uri="http://localhost:19530")

# 构建 Schema
schema = client.create_schema()
schema.add_field(
    field_name="id",
    datatype=DataType.INT64,
    is_primary=True,
    auto_id=True,
)
schema.add_field(
    field_name="text",
    datatype=DataType.VARCHAR,
    max_length=1000,
    enable_analyzer=True,
    analyzer_params={
        "tokenizer": "standard",
        "filter": [
            "lowercase",
            {
                "type": "stop",
                "stop_words": ["of", "to", "_english_"],
            },
            {
                "type": "stemmer",
                "language": "english",
            },
        ],
    },
    enable_match=True,
)
schema.add_field(
    field_name="sparse",
    datatype=DataType.SPARSE_FLOAT_VECTOR,
)

# 定义 BM25 函数
bm25_function = Function(
    name="text_to_vector",
    function_type=FunctionType.BM25,
    input_field_names=["text"],
    output_field_names=["sparse"],
)
schema.add_function(bm25_function)

# 索引
index_params = client.prepare_index_params()
index_params.add_index(
    field_name="sparse",
    index_type="AUTOINDEX",
    metric_type="BM25",
)

COLLECTION_NAME = "english_demo"
if client.has_collection(COLLECTION_NAME):
    client.drop_collection(COLLECTION_NAME)
    print(f"Dropped existing collection: {COLLECTION_NAME}")

client.create_collection(
    collection_name=COLLECTION_NAME,
    schema=schema,
    index_params=index_params,
)
print(f"成功创建集合: {COLLECTION_NAME}")

# 示例数据
sample_texts = [
    "The quick brown fox jumps over the lazy dog",
    "Machine learning algorithms are revolutionizing artificial intelligence",
    "Python programming language is widely used for data science projects",
    "Natural language processing helps computers understand human languages",
    "Deep learning models require large amounts of training data",
    "Search engines use complex algorithms to rank web pages",
    "Text analysis and information retrieval are important NLP tasks",
    "Vector databases enable efficient similarity searches",
    "Stemming reduces words to their root forms for better searching",
    "Stop words like 'the', 'and', 'of' are often filtered out",
]

# 插入数据
print("\n正在插入数据...")
data = [{"text": text} for text in sample_texts]
client.insert(collection_name=COLLECTION_NAME, data=data)
print(f"成功插入 {len(sample_texts)} 条数据")

# 分词器分析
print("\n" + "=" * 60)
print("分词器分析演示")
print("=" * 60)
test_text = "The running dogs are jumping over the lazy cats"
print(f"\n原始文本: '{test_text}'")

analyzer_result = client.run_analyzer(
    texts=test_text,
    collection_name=COLLECTION_NAME,
    field_name="text",
)
print(f"分词结果: {analyzer_result}")
print("\n分析说明:")
print("- lowercase: 将所有字母转换为小写")
print("- stop words: 过滤掉停用词 ['of', 'to'] 和英语常见停用词")
print("- stemmer: 将词汇还原为词干形式 (running -> run, jumping -> jump)")

# 全文检索演示
print("\n" + "=" * 60)
print("全文检索演示")
print("=" * 60)

time.sleep(2)

search_queries = [
    "jump",            # 词干匹配 -> "jumps"
    "algorithm",       # 精确匹配
    "python program",  # 多词查询
    "learn",           # 词干匹配 -> "learning"
]

for i, query in enumerate(search_queries, 1):
    print(f"\n查询 {i}: '{query}'")
    print("-" * 40)
    search_results = client.search(
        collection_name=COLLECTION_NAME,
        data=[query],
        search_params={"metric_type": "BM25"},
        output_fields=["text"],
        limit=3,
    )
    if search_results and len(search_results[0]) > 0:
        for j, result in enumerate(search_results[0], 1):
            score = result["distance"]
            text = result["entity"]["text"]
            print(f"  结果 {j} (相关度: {score:.4f}): {text}")
    else:
        print("  未找到相关结果")

print("\n" + "=" * 60)
print("检索完成！")
print("=" * 60)
```

结果输出：

```text
Dropped existing collection: english_demo
成功创建集合: english_demo
正在插入数据...
成功插入 10 条数据
============================================================
分词器分析演示
============================================================
原始文本: 'The running dogs are jumping over the lazy cats'
分词结果: ['run', 'dog', 'jump', 'over', 'lazi', 'cat']
分析说明:
- lowercase: 将所有字母转换为小写
- stop words: 过滤掉停用词 ['of', 'to'] 和英语常见停用词
- stemmer: 将词汇还原为词干形式 (running -> run, jumping -> jump)
============================================================
全文检索演示
============================================================
查询 1: 'jump'
----------------------------------------
  结果 1 (相关度: 2.0040): The quick brown fox jumps over the lazy dog
查询 2: 'algorithm'
----------------------------------------
  结果 1 (相关度: 1.5819): Machine learning algorithms are revolutionizing artificial intelligence
  结果 2 (相关度: 1.4086): Search engines use complex algorithms to rank web pages
查询 3: 'python program'
----------------------------------------
  结果 1 (相关度: 3.7884): Python programming language is widely used for data science projects
查询 4: 'learn'
----------------------------------------
  结果 1 (相关度: 1.5819): Machine learning algorithms are revolutionizing artificial intelligence
  结果 2 (相关度: 1.4086): Deep learning models require large amounts of training data
============================================================
检索完成！
============================================================
```

## (2) 多语言 Analyzer

如果数据集中同时包含多种语言，比如英文、中文和日文，那么我们就可以启用 Multi-language Analyzer。这样 Milvus 会根据文本语言自动选择合适的分词器。

```python
from pymilvus import MilvusClient, DataType, Function, FunctionType
import time

# 配置连接
client = MilvusClient(uri="http://localhost:19530")
COLLECTION_NAME = "multilingual_demo"

# 删除已存在的集合
if client.has_collection(COLLECTION_NAME):
    client.drop_collection(COLLECTION_NAME)

# 创建 schema
schema = client.create_schema()

# 主键字段
schema.add_field(
    field_name="id",
    datatype=DataType.INT64,
    is_primary=True,
    auto_id=True,
)

# 语言字段
schema.add_field(
    field_name="language",
    datatype=DataType.VARCHAR,
    max_length=50,
)

# 文本字段（多语言分析器）
multi_analyzer_params = {
    "by_field": "language",  # 根据 language 字段选择分析器
    "analyzers": {
        "en": {
            "type": "english",
        },
        "zh": {
            "type": "chinese",
        },
        "jp": {
            "tokenizer": "icu",
            "filter": [
                "lowercase",
                {
                    "type": "stop",
                    "stop_words": ["は", "が", "の", "に", "を", "で", "と"],
                },
            ],
        },
        "default": {
            "tokenizer": "icu",
        },
    },
    "alias": {
        "english": "en",
        "chinese": "zh",
        "japanese": "jp",
        "中文": "zh",
        "英文": "en",
        "日文": "jp",
    },
}

schema.add_field(
    field_name="text",
    datatype=DataType.VARCHAR,
    max_length=2000,
    enable_analyzer=True,
    multi_analyzer_params=multi_analyzer_params,
)

# 稀疏向量字段用于 BM25
schema.add_field(
    field_name="sparse_vector",
    datatype=DataType.SPARSE_FLOAT_VECTOR,
)

# 定义 BM25 函数
bm25_function = Function(
    name="text_bm25",
    function_type=FunctionType.BM25,
    input_field_names=["text"],
    output_field_names=["sparse_vector"],
)
schema.add_function(bm25_function)

# 索引
index_params = client.prepare_index_params()
index_params.add_index(
    field_name="sparse_vector",
    index_type="AUTOINDEX",
    metric_type="BM25",
)

# 创建集合
client.create_collection(
    collection_name=COLLECTION_NAME,
    schema=schema,
    index_params=index_params,
)

# 测试数据
multilingual_data = [
    # 英文
    {"language": "en", "text": "Artificial intelligence is revolutionizing technology industries worldwide"},
    {"language": "en", "text": "Machine learning algorithms process large datasets efficiently"},
    {"language": "en", "text": "Vector databases provide fast similarity search capabilities"},
    # 中文
    {"language": "zh", "text": "人工智能正在改变世界各行各业"},
    {"language": "zh", "text": "机器学习算法能够高效处理大规模数据集"},
    {"language": "zh", "text": "向量数据库提供快速的相似性搜索功能"},
    # 日文
    {"language": "jp", "text": "人工知能は世界中の技術産業に革命をもたらしています"},
    {"language": "jp", "text": "機械学習アルゴリズムは大量のデータセットを効率的に処理します"},
    {"language": "jp", "text": "ベクトルデータベースは高速な類似性検索機能を提供します"},
]

client.insert(collection_name=COLLECTION_NAME, data=multilingual_data)

# 等待 BM25 向量生成
print("等待BM25向量生成...")
client.flush(COLLECTION_NAME)
time.sleep(5)
client.load_collection(COLLECTION_NAME)

# 分词器分析
print("\n分词器分析:")
test_texts = {
    "en": "The running algorithms are processing data efficiently",
    "zh": "这些运行中的算法正在高效地处理数据",
    "jp": "これらの実行中のアルゴリズムは効率的にデータを処理しています",
}
for lang, text in test_texts.items():
    print(f"{lang}: {text}")
    try:
        analyzer_result = client.run_analyzer(
            texts=text,
            collection_name=COLLECTION_NAME,
            field_name="text",
            analyzer_names=[lang],
        )
        print(f"  → {analyzer_result}")
    except Exception as e:
        print(f"  → 分析失败: {e}")

# 多语言检索
print("\n检索测试:")
search_cases = [
    ("zh", "人工智能"),
    ("jp", "機械学習"),
    ("en", "algorithm"),
]
for lang, query in search_cases:
    print(f"\n{lang} '{query}':")
    try:
        search_results = client.search(
            collection_name=COLLECTION_NAME,
            data=[query],
            search_params={"metric_type": "BM25"},
            output_fields=["language", "text"],
            limit=3,
            filter=f'language == "{lang}"',
        )
        if search_results and len(search_results[0]) > 0:
            for result in search_results[0]:
                score = result["distance"]
                text = result["entity"]["text"]
                print(f"  {score:.3f}: {text}")
        else:
            print("  无结果")
    except Exception as e:
        print(f"  错误: {e}")

print("\n完成")
```

结果输出：

```text
等待BM25向量生成...
分词器分析:
en: The running algorithms are processing data efficiently
  → ['run', 'algorithm', 'process', 'data', 'effici']
zh: 这些运行中的算法正在高效地处理数据
  → ['这些', '运行', '中', '的', '算法', '正在', '高效', '地', '处理', '数据']
jp: これらの実行中のアルゴリズムは効率的にデータを処理しています
  → ['これらの', '実行', '中の', 'アルゴリズム', '効率', '的', 'データ', '処理', 'し', 'てい', 'ます']
检索测试:
zh '人工智能':
  3.300: 人工智能正在改变世界各行各业
jp '機械学習':
  3.649: 機械学習アルゴリズムは大量のデータセットを効率的に処理します
en 'algorithm':
  2.096: Machine learning algorithms process large datasets efficiently
完成
```

另外，Milvus 目前也支持使用 language_identifier 分词器进行搜索，它的好处是不用手动告诉系统这段文本是什么语言，Milvus 会自己识别。相应的，语言字段（language）也不是必须的。

社区之前的官宣：Milvus 2.6 引入多语言分析器，全文搜索再升级，助力业务全球化：https://mp.weixin.qq.com/s?__biz=MzUzMDI5OTA5NQ==&mid=2247509815&idx=1&sn=9cb00fe1134e142acec7bca7384ad612&scene=21#wechat_redirect

# pymilvus 可以使用  BM25 做文本的倒排索引吗？
bm25_function = Function(
    name="text_to_vector",            # Descriptive function name
    function_type=FunctionType.BM25,  # Use BM25 algorithm
    input_field_names=["text"],       # Process text from this field
    output_field_names=["sparse"]     # Store vectors in this field
)
schema.add_function(bm25_function)
index_params = client.prepare_index_params()
index_params.add_index(
    field_name="sparse",        # Field to index (our vector field)
    index_type="AUTOINDEX",     # Let Milvus choose optimal index type
    metric_type="BM25"          # Must be BM25 for this feature
)
难道支持 文本的倒排索引？ 还是说这个是向量索引？还是说向量索引是基础？然后支持混合索引（向量索引+倒排索引）？
## 对 text 字段建倒排索引（Lucene 那种）与 BM25 索引的区别
### 难道支持 文本的倒排索引？ 还是说这个是向量索引？还是说向量索引是基础？然后支持混合索引（向量索引+倒排索引）？
- 你这段配置并不是“直接对 text 字段建倒排索引（Lucene 那种）”，而是“文本→稀疏向量（BM25 权重）→对稀疏向量建索引”。本质是稀疏向量索引，但其内部结构等价于按 term 的倒排表（posting lists）来做匹配与打分。
- 因此它属于“稀疏向量的全文检索（BM25）”，不是密集向量索引；也不是自动的“混合（稀疏+稠密）”。
- 若要混合检索，需要额外加一个密集向量字段与索引，然后在查询阶段做 score 融合。

依据与示例：
- 该流程在文档中演示：文本用 Analyzer 分词，然后用 BM25 Function把 text 转为 SPARSE_FLOAT_VECTOR，并用 metric_type=BM25 建索引，完成全文检索能力，见15.MilvusAnalyzer.md。
- BM25 打分项与参数说明参见：11.elasticsearch_explain_tf_idf__bm25.md、12.bm25.md。
- 若想了解 ES 的“倒排+向量”的混合模式原理对比，可看：14.es_mixed_index.md。

如何做“混合（BM25 稀疏 + 密集向量）”的最小改造思路：
1) 保留你现有的 BM25 稀疏索引字段 sparse。  
2) 另加一个密集向量字段（如 emb，HNSW/IVF），插入时生成 embedding。  
3) 查询时分别搜 BM25 和向量，做 score 归一化后加权融合。

````python
# 伪代码要点（在现有 BM25 基础上增加稠密向量混检）
from pymilvus import DataType

# 1) 增加稠密向量字段 + 索引
schema.add_field(
    field_name="emb",
    datatype=DataType.FLOAT_VECTOR,
    dim=768,
)
index_params.add_index(
    field_name="emb",
    index_type="HNSW",
    metric_type="COSINE",
)

# 2) 插入时为 emb 生成向量（略）

# 3) 查询时分别检索并融合
bm25_hits = client.search(
    collection_name=COLLECTION_NAME,
    data=[query_text],
    search_params={"metric_type": "BM25"},
    output_fields=["text"],
    limit=K,
)

dense_hits = client.search(
    collection_name=COLLECTION_NAME,
    data=[encode(query_text)],  # embedding
    search_params={"metric_type": "COSINE"},
    output_fields=["text"],
    limit=K,
)

# 4) 归一化 + 加权融合（如 RRF、Z-score、min-max 等），再排序输出
# ...existing code...
````

总结：
- 你当前代码实现的是“Analyzer 分词 + BM25 稀疏向量索引”的全文检索，功能等价于倒排检索但以稀疏向量形态实现，不是密集向量索引，也不是自动混合。
- 需要混合检索时，额外加密集向量字段与索引，在查询阶段做结果融合即可。


### 你当前代码实现的是“Analyzer 分词 + BM25 稀疏向量索引”的全文检索，功能等价于倒排检索但以稀疏向量形态实现，不是密集向量索引，也不是自动混合。是指用的公式还是bm25对吗？具体例子说明是怎么以稀疏向量形态实现？

- 是的，用的就是 Okapi BM25 打分公式。`FunctionType.BM25` 会把分词后的文本转成一个稀疏向量（term→权重），权重包含 BM25 的 tf 归一化与 idf；`metric_type="BM25"` 在检索时按 BM25 公式打分。
- “稀疏向量形态”指：只存非零项的 term→weight 映射（坐标-值对），**索引内部可视为按 term 的倒排表组织**；查询也被转成同构的稀疏向量，然后基于共享 term 按 BM25 计算得分。

最小化示例（玩具语料，展示形态与打分思路）：
````python
# 文档分词（见运行示例：run_analyzer 输出在 [15.MilvusAnalyzer.md](15.MilvusAnalyzer.md)）
# D1: "The running dogs are jumping ..." → ['run','dog','jump','over','lazi','cat']

# 设语料 N=3，BM25 参数 k1=1.2, b=0.75（示例值）
# 统计得到 idf(jump) ≈ 0.98；对 D1 中 jump 的 tf=1，长度归一化后 BM25 权重：
# w_D1_jump ≈ 0.93（= idf * tf_norm，近似值）

# 于是 D1 的稀疏向量可写成（只列部分 term）：
D1_sparse = {
    "jump": 0.93,   # BM25 权重
    "dog":  0.41,
    "cat":  0.47,
    # ...
}

# 查询 "jump" 也被分词为 ['jump']，形成查询稀疏向量（权重一般含 idf，或按 BM25 的 qtf 归一化）：
Q_sparse = {"jump": 0.98}

# 检索时，Milvus 使用 metric_type="BM25" 按 BM25 公式在共享 term 上累加打分，
# 本质等价于遍历 term="jump" 的倒排表(posting list)，计算每个文档的 BM25 得分并排序。
# 在概念上，你可以把它理解为对这两个稀疏向量在共有坐标处做 BM25 加权求和。
````

与“倒排索引”的关系：
- 存储形态是 SPARSE_FLOAT_VECTOR（只存非零项），但索引内部按 term→posting lists 来加速检索，效果等价于传统倒排检索；只是对外暴露为“稀疏向量字段 + BM25 度量”的接口。
- 不是密集向量索引；也不是自动混合。如果要“BM25 稀疏 + Embedding 稠密”的混合检索，需要再加一个密集向量字段与索引，查询时做分数融合（可参考14.es_mixed_index.md）。BM25 原理详见11.elasticsearch_explain_tf_idf__bm25.md与12.bm25.md。

## 归一化 + 加权融合（如 RRF、Z-score、min-max 等），再排序输出。怎么计算？具体实现


- 用的就是 Okapi BM25 公式；Milvus 把分词后的文本转成 term→weight 的稀疏向量，并按 metric_type="BM25" 打分。
- 混合检索常见做法：分别取 BM25 与稠密向量检索结果，做分数归一化后加权融合或用 RRF 融合。

下面给出最小可用实现：支持 MinMax、Z-Score 与 RRF 三种融合，兼容 MilvusClient.search 的返回结构。

````python
# ...existing code...
from typing import List, Dict, Tuple
import math

def _extract_hits(raw_results) -> List[dict]:
    # raw_results: MilvusClient.search 的返回，形如 [[hit, hit, ...]]
    return raw_results[0] if raw_results else []

def _hit_pk(hit: dict) -> int:
    # 兼容不同返回结构，优先顶层 id，否则从 entity.id 取
    if "id" in hit and hit["id"] is not None:
        return hit["id"]
    ent = hit.get("entity", {})
    return ent.get("id")

def _hit_text(hit: dict) -> str:
    ent = hit.get("entity", {})
    return ent.get("text", "")

def _hit_score(hit: dict, metric_type: str) -> float:
    d = hit["distance"]
    metric = (metric_type or "").upper()
    # 统一为“越大越好”的分数
    if metric in ("BM25", "IP", "COSINE"):
        return float(d)
    elif metric in ("L2", "EUCLIDEAN"):
        return -float(d)  # L2 越小越近，取负号
    else:
        return float(d)

def collect_scores(raw_results, metric_type: str) -> Dict[int, float]:
    scores = {}
    for hit in _extract_hits(raw_results):
        pk = _hit_pk(hit)
        if pk is not None:
            scores[pk] = _hit_score(hit, metric_type)
    return scores

def collect_payload(raw_results) -> Dict[int, dict]:
    payload = {}
    for hit in _extract_hits(raw_results):
        pk = _hit_pk(hit)
        if pk is not None:
            payload[pk] = {
                "text": _hit_text(hit),
                "raw": hit,
            }
    return payload

# ----------------- 归一化 -----------------

def normalize_minmax(scores: Dict[int, float]) -> Dict[int, float]:
    if not scores:
        return {}
    vals = list(scores.values())
    vmin, vmax = min(vals), max(vals)
    if math.isclose(vmin, vmax):
        # 全相等时给 1.0
        return {k: 1.0 for k in scores}
    return {k: (v - vmin) / (vmax - vmin) for k, v in scores.items()}

def normalize_zscore(scores: Dict[int, float]) -> Dict[int, float]:
    if not scores:
        return {}
    vals = list(scores.values())
    mean = sum(vals) / len(vals)
    var = sum((v - mean) ** 2 for v in vals) / max(1, (len(vals) - 1))
    std = math.sqrt(var)
    if std == 0:
        return {k: 0.0 for k in scores}
    return {k: (v - mean) / std for k, v in scores.items()}

# ----------------- RRF 融合 -----------------

def rrf_fusion(rank_a: Dict[int, int], rank_b: Dict[int, int], k: int = 60) -> Dict[int, float]:
    # rank 字典: doc_id -> 从1开始的名次
    all_ids = set(rank_a) | set(rank_b)
    fused = {}
    for did in all_ids:
        ra = rank_a.get(did, 10**9)  # 缺席给极差名次
        rb = rank_b.get(did, 10**9)
        fused[did] = 1.0 / (k + ra) + 1.0 / (k + rb)
    return fused

def ranks_from_scores(scores: Dict[int, float]) -> Dict[int, int]:
    # 分数高者名次小（1为最好）
    sorted_ids = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)
    return {did: i + 1 for i, (did, _) in enumerate(sorted_ids)}

# ----------------- 融合策略 -----------------

def fuse_weighted_minmax(bm25: Dict[int, float], dense: Dict[int, float], alpha: float = 0.6) -> Dict[int, float]:
    nb = normalize_minmax(bm25)
    nd = normalize_minmax(dense)
    all_ids = set(nb) | set(nd)
    return {i: alpha * nb.get(i, 0.0) + (1 - alpha) * nd.get(i, 0.0) for i in all_ids}

def fuse_weighted_zscore(bm25: Dict[int, float], dense: Dict[int, float], alpha: float = 0.6) -> Dict[int, float]:
    zb = normalize_zscore(bm25)
    zd = normalize_zscore(dense)
    all_ids = set(zb) | set(zd)
    return {i: alpha * zb.get(i, 0.0) + (1 - alpha) * zd.get(i, 0.0) for i in all_ids}

# ----------------- 示例：从 Milvus 拉取并融合 -----------------

def hybrid_search_and_fuse(client, collection: str, query_text: str, query_emb: List[float], k: int = 10,
                           alpha: float = 0.6, method: str = "minmax") -> List[Tuple[int, float, dict]]:
    # 1) 分别检索 BM25 稀疏与稠密向量
    bm25_raw = client.search(
        collection_name=collection,
        data=[query_text],
        search_params={"metric_type": "BM25"},
        output_fields=["id", "text"],
        limit=k,
    )
    dense_raw = client.search(
        collection_name=collection,
        data=[query_emb],
        search_params={"metric_type": "COSINE"},
        output_fields=["id", "text"],
        limit=k,
    )

    bm25_scores = collect_scores(bm25_raw, "BM25")
    dense_scores = collect_scores(dense_raw, "COSINE")
    payload = collect_payload(bm25_raw) | collect_payload(dense_raw)

    # 2) 融合
    if method == "rrf":
        r1 = ranks_from_scores(bm25_scores)
        r2 = ranks_from_scores(dense_scores)
        fused = rrf_fusion(r1, r2, k=60)
    elif method == "zscore":
        fused = fuse_weighted_zscore(bm25_scores, dense_scores, alpha=alpha)
    else:
        fused = fuse_weighted_minmax(bm25_scores, dense_scores, alpha=alpha)

    # 3) 排序与回填文本
    sorted_items = sorted(fused.items(), key=lambda kv: kv[1], reverse=True)[:k]
    return [(did, score, payload.get(did, {})) for did, score in sorted_items]

# ----------------- 用法示例 -----------------

# 准备查询：BM25 用原始文本；稠密向量需要先编码
# query_emb = encode(query_text)  # 替换为你的向量编码函数

# results = hybrid_search_and_fuse(client, COLLECTION_NAME, query_text, query_emb,
#                                  k=10, alpha=0.6, method="minmax")
# for did, s, pl in results:
#     print(did, f"{s:.4f}", pl.get("text", ""))
# ...existing code...
````

要点：
- 融合前需在两次 search 的 output_fields 中带上主键 id 以便对齐。
- MinMax/Z-Score 先各自归一化，再按 alpha 做加权和。
- RRF 不看分数，只看名次，鲁棒于分数尺度不一致的情况。

# 稀疏向量BM25与ES原生BM25的详细对比手册
以下是稀疏向量BM25与ES原生BM25的详细对比手册：

### 一、原理对比
- **ES原生BM25**：ES原生BM25基于倒排索引，通过遍历倒排链，实时计算查询词与文档的相关性得分。其评分公式为：
\[Score(D, Q)=\sum\left(IDF(q_i) * \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D)+k_1 \cdot (1 - b + b \cdot \frac{|D|}{\text{avgdl}})}\right)\]
其中，\(D\)表示文档，\(Q\)表示查询，\(q_i\)是查询中的词项，\(f(q_i, D)\)是词项\(q_i\)在文档\(D\)中的词频，\(|D|\)是文档\(D\)的长度，\(\text{avgdl}\)是文档集合的平均文档长度，\(k_1\)和\(b\)是可调参数，通常\(k_1\)取1.2到2.0之间的值，\(b\)取0.0到0.75之间的值。
- **稀疏向量BM25**：稀疏向量BM25先对文档分词后，计算每个词的BM25权重，以“词ID→BM25权重”的键值对形式构建稀疏向量并预先存储。查询时，构建查询稀疏向量，通过向量点积运算计算查询向量与文档向量的相似度，等价于原生BM25的得分公式计算结果。

### 二、存储对比
- **ES原生BM25**：存储倒排链，即“词→文档列表”，同时存储词频、文档长度等统计信息，用于实时计算相关性得分。
- **稀疏向量BM25**：除了可保留倒排链（可选）外，主要存储文档稀疏向量，即每个文档对应的“词→BM25权重”的键值对集合。

### 三、计算时机对比
- **ES原生BM25**：查询时，遍历倒排链，根据倒排链中的统计信息实时代入BM25公式计算每个文档与查询的相关性得分。
- **稀疏向量BM25**：预处理时计算每个词的BM25权重并构建稀疏向量，查询时直接通过向量点积计算相似度得分，无需实时代入公式计算。

### 四、具体例子对比
假设有文档集合\(\{D_1, D_2, D_3\}\)，查询\(Q = \)“人工智能 汽车”，分词后为\(\{“人工智能”, “汽车”\}\)。
- **ES原生BM25**：
    - 假设倒排索引中“人工智能”对应的文档列表为\(\{D_1, D_2\}\)，词频分别为\(f(“人工智能”, D_1)=3\)，\(f(“人工智能”, D_2)=2\)；“汽车”对应的文档列表为\(\{D_1, D_3\}\)，词频分别为\(f(“汽车”, D_1)=2\)，\(f(“汽车”, D_3)=1\)。
    - 已知文档集合的平均长度\(\text{avgdl}=10\)，\(k_1 = 1.5\)，\(b = 0.75\)，文档总数\(N = 3\)，包含“人工智能”的文档数\(df(“人工智能”)=2\)，包含“汽车”的文档数\(df(“汽车”)=2\)。
    - 计算\(IDF(“人工智能”)=\log(\frac{N}{df(“人工智能”)})=\log(\frac{3}{2})\approx0.176\)，\(IDF(“汽车”)=\log(\frac{3}{2})\approx0.176\)。
    - 计算\(Score(D_1, Q)\)：
\[
\begin{align*}
Score(D_1, Q)&=IDF(“人工智能”) * \frac{f(“人工智能”, D_1) \cdot (k_1 + 1)}{f(“人工智能”, D_1)+k_1 \cdot (1 - b + b \cdot \frac{|D_1|}{\text{avgdl}})}+IDF(“汽车”) * \frac{f(“汽车”, D_1) \cdot (k_1 + 1)}{f(“汽车”, D_1)+k_1 \cdot (1 - b + b \cdot \frac{|D_1|}{\text{avgdl}})}\\
&=0.176 * \frac{3 \cdot (1.5 + 1)}{3+1.5 \cdot (1 - 0.75 + 0.75 \cdot \frac{|D_1|}{10})}+0.176 * \frac{2 \cdot (1.5 + 1)}{2+1.5 \cdot (1 - 0.75 + 0.75 \cdot \frac{|D_1|}{10})}
\end{align*}
\]
假设\(|D_1| = 10\)，代入计算可得\(Score(D_1, Q)\approx0.247\)。
同理可计算\(Score(D_2, Q)\)和\(Score(D_3, Q)\)。
- **稀疏向量BM25**：
    - 预处理时，计算得到\(D_1\)的稀疏向量为\(\{“人工智能”: 0.247, “汽车”: 0.165\}\)（假设通过BM25公式计算得到），\(D_2\)的稀疏向量为\(\{“人工智能”: 0.198\}\)，\(D_3\)的稀疏向量为\(\{“汽车”: 0.083\}\)。
    - 查询时，构建查询稀疏向量为\(\{“人工智能”: 0.176, “汽车”: 0.176\}\)。
    - 计算\(D_1\)与查询的相似度：\(0.247×0.176 + 0.165×0.176\approx0.072\)。
    - 同理可计算\(D_2\)和\(D_3\)与查询的相似度。

### 五、性能与应用场景对比
- **性能**：ES原生BM25在查询时需要实时计算，当文档数量和查询复杂度增加时，计算量较大，性能可能下降。稀疏向量BM25预处理时计算权重，查询时只需向量点积，计算效率高，尤其适用于大规模文档集合和高频查询场景。
- **应用场景**：ES原生BM25适用于对关键词精确匹配要求高的场景，如代码搜索、特定术语搜索等。稀疏向量BM25除了能处理关键词精确匹配外，还能通过向量相似度计算，在一定程度上处理语义相关的查询，应用场景更广泛。


# 结构
首先纠正一个表述，你说的“道旁缩影”实际是“倒排索引”（Inverted Index），这是全文检索的核心数据结构。你的观察非常准确，倒排索引远不止存储关键词，而是一套包含多维度信息的复杂存储体系。

核心结论：倒排索引的核心结构是“**词典（Term Dictionary）+ 倒排表（Posting List）** ”，ES及其他搜索引擎会在此基础上扩展存储词频、位置、文档元数据等信息，并通过分段、压缩等方式优化存储，不同系统的存储细节会因架构设计差异而不同。

---

### 一、倒排索引的通用核心数据结构
无论ES还是其他系统（如Lucene、Solr），倒排索引的基础结构一致，主要包含以下两部分，各部分存储的信息如下：

#### 1. 词典（Term Dictionary）：关键词的“目录”
词典是所有去重后关键词（Term）的集合，用于快速定位关键词对应的倒排表，主要存储两类信息：
- **关键词本身**：如“人工智能”“汽车”等分词后的Term。
- **指向倒排表的指针**：记录该Term对应的倒排表在磁盘/内存中的存储位置（如偏移量、块编号），确保能快速找到倒排表。

#### 2. 倒排表（Posting List）：关键词的“文档清单”
倒排表是该Term出现过的所有文档的集合，是倒排索引的核心，**你提到的文档ID、词位置、词频等信息都存储在这里**，具体包含：
- **文档ID（Doc ID）**：该Term所在文档的唯一标识，通常是整数（如1、2、3），方便快速关联文档。
- **词频（Term Frequency, TF）**：该Term在当前文档中的出现次数，用于BM25、TF-IDF等评分公式计算。
- **词位置（Position）**：该Term在文档中的具体位置（如第5个词、第12个词），用于支持“短语查询”（如“人工智能技术”需两个词连续出现）和“邻近查询”（如“人工智能”与“汽车”间隔不超过3个词）。
- **偏移量（Offset）**：该Term在文档原始文本中的字符起始和结束位置（如从第10个字符到第14个字符），用于查询结果中的“关键词高亮”功能。

**举个具体例子**：  
假设文档1（Doc ID=1）内容为“人工智能在汽车行业的应用”，分词后“人工智能”的倒排表条目如下：
```
Term：人工智能
Posting List：[
  {
    Doc ID: 1,
    TF: 1,
    Position: [0],  // 该词是文档的第1个词（位置从0开始计数）
    Offset: (0, 4)  // 原始文本中从第0个字符到第4个字符（“人工智能”共4个字符）
  },
  {
    Doc ID: 3,  // 其他包含“人工智能”的文档
    TF: 1,
    Position: [2],
    Offset: (8, 12)
  }
]
```

---

### 二、Elasticsearch（ES）的倒排索引存储细节
ES基于Lucene实现，其倒排索引存储在“**分段（Segment）** ”中（ES的索引由多个Segment组成，Segment是不可变的文件集合），具体存储逻辑如下：

#### 1. 核心信息的存储位置
| 存储内容          | 存储位置                  | 作用说明                                                                 |
|-------------------|---------------------------|--------------------------------------------------------------------------|
| 词典（Term Dict） | Segment的`.tim`文件       | 存储所有Term及指向倒排表的指针，会通过“FST（有限状态转换器）”压缩，减少内存占用。 |
| 倒排表（Posting List） | Segment的`.doc`文件    | 存储Doc ID、TF、Position、Offset，会用“差值编码”“位压缩”优化存储（如Doc ID按递增排序，存储差值而非原始值）。 |
| 文档元数据        | Segment的`.fnm`和`.fdx/.fdt`文件 | `.fnm`存字段信息（如字段名、类型）；`.fdx`是文档元数据的索引，`.fdt`存具体元数据（如文档原始内容、字段值）。 |
| 词频统计（DF/IDF） | 词典（`.tim`）或单独统计文件 | DF（该Term出现的文档数）会存在词典中，IDF不直接存储，查询时通过“总文档数/DF”实时计算（ES中总文档数存在`.seg`文件）。 |

#### 2. ES的特殊存储优化
- **分段不可变**：每个Segment生成后不再修改，新增文档会写入新Segment，删除文档仅在“删除位图（Delete Bitmap）”中标记（`.del`文件），不实际删除倒排表数据，提升写入性能。
- **内存缓存**：词典（FST结构）和高频Term的倒排表会加载到内存，低频Term的倒排表从磁盘读取，平衡内存占用和查询速度。
- **字段级隔离**：每个字段（如“标题”“内容”）会单独建立倒排索引，存储在不同的`.tim`和`.doc`文件中，避免查询时加载无关字段数据。

---

### 三、其他系统的倒排索引存储差异
除ES外，常见的搜索引擎（如Lucene、Solr、Elasticsearch）或数据库（如PostgreSQL的全文检索），倒排索引存储逻辑核心一致，但细节因场景优化不同而有差异：

| 系统        | 核心存储文件/结构                | 关键差异点                                                                 |
|-------------|----------------------------------|----------------------------------------------------------------------------|
| **Lucene**  | 与ES一致（`.tim`/`.doc`/`.fdt`等） | ES的底层依赖，存储逻辑完全一致，无额外封装，适合自定义开发。                 |
| **Solr**    | 基于Lucene，文件结构相同         | 会额外存储“字段类型配置”“同义词表”等元数据到`core.properties`文件，倒排表存储无本质差异。 |
| **PostgreSQL** | 存储在“文本搜索字典”和“倒排索引表” | 倒排表以数据库表的形式存储（而非独立文件），Doc ID关联数据库行ID，支持与关系数据联动查询，但性能弱于ES/Lucene。 |
| **Apache Lucene** | 原生分段存储                   | 无分布式能力，倒排表仅在单节点存储，适合单机场景，ES通过分布式架构将Lucene Segment分散到不同节点。 |

---

### 四、你关心的“参数存储”问题
你提到的“词频投放”“参数存放”，本质是倒排索引相关的“统计参数”和“配置参数”，不同参数的存储位置不同：

| 参数类型          | 具体参数示例                | 存储位置                                                                 |
|-------------------|-----------------------------|--------------------------------------------------------------------------|
| **评分参数**      | BM25的k1、b值，IDF的总文档数 | k1、b是查询时传入的配置（或ES索引的“相似度配置”，存于`mapping.json`）；总文档数存于Segment的`.seg`文件。 |
| **分词参数**      | 分词器类型（如IK、Jieba）、停用词表 | 存于ES的“索引映射（Mapping）”中（`.mapping`文件），分词时按配置加载停用词、自定义词典。 |
| **存储配置参数**  | 分段大小阈值、压缩算法       | ES的`elasticsearch.yml`配置文件（全局）或索引创建时的“settings”（如`index.merge.policy.segment_size`）。 |

---

要不要我帮你整理一份**倒排索引核心结构与存储位置的可视化表格**？表格会按“结构类型-ES存储-其他系统存储-关键作用”分类，更直观地对比不同系统的存储差异，方便你快速查阅。

